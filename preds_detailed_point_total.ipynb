{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# March Machine Learning Mania\n",
    "In this notebook, we:\n",
    "- Try different models/features and assess their performance.\n",
    "- Simulate multiple 2024 March Madness brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gc import collect\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, StackingRegressor, StackingClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score, log_loss, accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# display 100 rows and 100 columns\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "\n",
    "# global random seed\n",
    "SEED = 0\n",
    "\n",
    "# set numpy seed\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root dirs\n",
    "root = 'data/'\n",
    "mroot = 'data/mens/'\n",
    "wroot = 'data/womens/'\n",
    "\n",
    "# load in features compact\n",
    "features = pd.read_csv(root + 'processed/features_detailed_3.csv')\n",
    "\n",
    "# team names\n",
    "# teams = pd.concat([pd.read_csv(mroot + 'MTeams.csv'), pd.read_csv(wroot + 'WTeams.csv')], ignore_index=True)\n",
    "\n",
    "# create a map for team names\n",
    "# team_map = teams.set_index('TeamID')['TeamName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearranging the Data\n",
    "When we input a new row into our model (for the 2024 bracket), the winner could be the first or the second team in the row that we input into our model. Currently, all rows have the winning team on the left, and this will cause our model to learn this locational information. We need to rearrange the order of winners and losers, to where it is essentially random to prevent the model from learning this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>A_TeamID</th>\n",
       "      <th>A_Score</th>\n",
       "      <th>B_TeamID</th>\n",
       "      <th>B_Score</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>A_PlayIn</th>\n",
       "      <th>B_PlayIn</th>\n",
       "      <th>A_SeedNum</th>\n",
       "      <th>B_SeedNum</th>\n",
       "      <th>A_num_games</th>\n",
       "      <th>A_win_ratio</th>\n",
       "      <th>A_avg_Score_for</th>\n",
       "      <th>A_avg_Score_against</th>\n",
       "      <th>A_std_Score_for</th>\n",
       "      <th>A_std_Score_against</th>\n",
       "      <th>A_avg_FGPct2_for</th>\n",
       "      <th>A_avg_FGPct2_against</th>\n",
       "      <th>A_std_FGPct2_for</th>\n",
       "      <th>A_std_FGPct2_against</th>\n",
       "      <th>A_avg_FGPct3_for</th>\n",
       "      <th>A_avg_FGPct3_against</th>\n",
       "      <th>A_std_FGPct3_for</th>\n",
       "      <th>A_std_FGPct3_against</th>\n",
       "      <th>A_avg_FTPct_for</th>\n",
       "      <th>A_avg_FTPct_against</th>\n",
       "      <th>A_std_FTPct_for</th>\n",
       "      <th>A_std_FTPct_against</th>\n",
       "      <th>A_avg_Ast_for</th>\n",
       "      <th>A_avg_Ast_against</th>\n",
       "      <th>A_std_Ast_for</th>\n",
       "      <th>A_std_Ast_against</th>\n",
       "      <th>A_avg_OR_for</th>\n",
       "      <th>A_avg_OR_against</th>\n",
       "      <th>A_std_OR_for</th>\n",
       "      <th>A_std_OR_against</th>\n",
       "      <th>A_avg_DR_for</th>\n",
       "      <th>A_avg_DR_against</th>\n",
       "      <th>A_std_DR_for</th>\n",
       "      <th>A_std_DR_against</th>\n",
       "      <th>A_avg_Stl_for</th>\n",
       "      <th>A_avg_Stl_against</th>\n",
       "      <th>A_std_Stl_for</th>\n",
       "      <th>A_std_Stl_against</th>\n",
       "      <th>A_avg_Blk_for</th>\n",
       "      <th>A_avg_Blk_against</th>\n",
       "      <th>A_std_Blk_for</th>\n",
       "      <th>A_std_Blk_against</th>\n",
       "      <th>A_avg_TO_for</th>\n",
       "      <th>A_avg_TO_against</th>\n",
       "      <th>A_std_TO_for</th>\n",
       "      <th>A_std_TO_against</th>\n",
       "      <th>A_avg_PF_for</th>\n",
       "      <th>A_avg_PF_against</th>\n",
       "      <th>A_std_PF_for</th>\n",
       "      <th>A_std_PF_against</th>\n",
       "      <th>A_avg_diff</th>\n",
       "      <th>A_std_diff</th>\n",
       "      <th>A_home_win_ratio</th>\n",
       "      <th>A_away_win_ratio</th>\n",
       "      <th>A_neutral_win_ratio</th>\n",
       "      <th>A_home_missing</th>\n",
       "      <th>A_away_missing</th>\n",
       "      <th>A_neutral_missing</th>\n",
       "      <th>A_close_game_missing</th>\n",
       "      <th>A_close_game_ratio</th>\n",
       "      <th>A_close_win_ratio</th>\n",
       "      <th>A_recent_win_ratio</th>\n",
       "      <th>A_recent_avg_Score_for</th>\n",
       "      <th>A_recent_avg_Score_against</th>\n",
       "      <th>A_recent_std_Score_for</th>\n",
       "      <th>A_recent_std_Score_against</th>\n",
       "      <th>A_recent_avg_FGPct2_for</th>\n",
       "      <th>A_recent_avg_FGPct2_against</th>\n",
       "      <th>A_recent_std_FGPct2_for</th>\n",
       "      <th>A_recent_std_FGPct2_against</th>\n",
       "      <th>A_recent_avg_FGPct3_for</th>\n",
       "      <th>A_recent_avg_FGPct3_against</th>\n",
       "      <th>A_recent_std_FGPct3_for</th>\n",
       "      <th>A_recent_std_FGPct3_against</th>\n",
       "      <th>A_recent_avg_FTPct_for</th>\n",
       "      <th>A_recent_avg_FTPct_against</th>\n",
       "      <th>A_recent_std_FTPct_for</th>\n",
       "      <th>A_recent_std_FTPct_against</th>\n",
       "      <th>A_recent_avg_Ast_for</th>\n",
       "      <th>A_recent_avg_Ast_against</th>\n",
       "      <th>A_recent_std_Ast_for</th>\n",
       "      <th>A_recent_std_Ast_against</th>\n",
       "      <th>A_recent_avg_OR_for</th>\n",
       "      <th>A_recent_avg_OR_against</th>\n",
       "      <th>A_recent_std_OR_for</th>\n",
       "      <th>A_recent_std_OR_against</th>\n",
       "      <th>A_recent_avg_DR_for</th>\n",
       "      <th>A_recent_avg_DR_against</th>\n",
       "      <th>A_recent_std_DR_for</th>\n",
       "      <th>A_recent_std_DR_against</th>\n",
       "      <th>A_recent_avg_Stl_for</th>\n",
       "      <th>A_recent_avg_Stl_against</th>\n",
       "      <th>A_recent_std_Stl_for</th>\n",
       "      <th>A_recent_std_Stl_against</th>\n",
       "      <th>A_recent_avg_Blk_for</th>\n",
       "      <th>A_recent_avg_Blk_against</th>\n",
       "      <th>A_recent_std_Blk_for</th>\n",
       "      <th>A_recent_std_Blk_against</th>\n",
       "      <th>A_recent_avg_TO_for</th>\n",
       "      <th>A_recent_avg_TO_against</th>\n",
       "      <th>A_recent_std_TO_for</th>\n",
       "      <th>A_recent_std_TO_against</th>\n",
       "      <th>A_recent_avg_PF_for</th>\n",
       "      <th>A_recent_avg_PF_against</th>\n",
       "      <th>A_recent_std_PF_for</th>\n",
       "      <th>A_recent_std_PF_against</th>\n",
       "      <th>A_recent_avg_diff</th>\n",
       "      <th>A_conf_champs</th>\n",
       "      <th>B_num_games</th>\n",
       "      <th>B_win_ratio</th>\n",
       "      <th>B_avg_Score_for</th>\n",
       "      <th>B_avg_Score_against</th>\n",
       "      <th>B_std_Score_for</th>\n",
       "      <th>B_std_Score_against</th>\n",
       "      <th>B_avg_FGPct2_for</th>\n",
       "      <th>B_avg_FGPct2_against</th>\n",
       "      <th>B_std_FGPct2_for</th>\n",
       "      <th>B_std_FGPct2_against</th>\n",
       "      <th>B_avg_FGPct3_for</th>\n",
       "      <th>B_avg_FGPct3_against</th>\n",
       "      <th>B_std_FGPct3_for</th>\n",
       "      <th>B_std_FGPct3_against</th>\n",
       "      <th>B_avg_FTPct_for</th>\n",
       "      <th>B_avg_FTPct_against</th>\n",
       "      <th>B_std_FTPct_for</th>\n",
       "      <th>B_std_FTPct_against</th>\n",
       "      <th>B_avg_Ast_for</th>\n",
       "      <th>B_avg_Ast_against</th>\n",
       "      <th>B_std_Ast_for</th>\n",
       "      <th>B_std_Ast_against</th>\n",
       "      <th>B_avg_OR_for</th>\n",
       "      <th>B_avg_OR_against</th>\n",
       "      <th>B_std_OR_for</th>\n",
       "      <th>B_std_OR_against</th>\n",
       "      <th>B_avg_DR_for</th>\n",
       "      <th>B_avg_DR_against</th>\n",
       "      <th>B_std_DR_for</th>\n",
       "      <th>B_std_DR_against</th>\n",
       "      <th>B_avg_Stl_for</th>\n",
       "      <th>B_avg_Stl_against</th>\n",
       "      <th>B_std_Stl_for</th>\n",
       "      <th>B_std_Stl_against</th>\n",
       "      <th>B_avg_Blk_for</th>\n",
       "      <th>B_avg_Blk_against</th>\n",
       "      <th>B_std_Blk_for</th>\n",
       "      <th>B_std_Blk_against</th>\n",
       "      <th>B_avg_TO_for</th>\n",
       "      <th>B_avg_TO_against</th>\n",
       "      <th>B_std_TO_for</th>\n",
       "      <th>B_std_TO_against</th>\n",
       "      <th>B_avg_PF_for</th>\n",
       "      <th>B_avg_PF_against</th>\n",
       "      <th>B_std_PF_for</th>\n",
       "      <th>B_std_PF_against</th>\n",
       "      <th>B_avg_diff</th>\n",
       "      <th>B_std_diff</th>\n",
       "      <th>B_home_win_ratio</th>\n",
       "      <th>B_away_win_ratio</th>\n",
       "      <th>B_neutral_win_ratio</th>\n",
       "      <th>B_home_missing</th>\n",
       "      <th>B_away_missing</th>\n",
       "      <th>B_neutral_missing</th>\n",
       "      <th>B_close_game_missing</th>\n",
       "      <th>B_close_game_ratio</th>\n",
       "      <th>B_close_win_ratio</th>\n",
       "      <th>B_recent_win_ratio</th>\n",
       "      <th>B_recent_avg_Score_for</th>\n",
       "      <th>B_recent_avg_Score_against</th>\n",
       "      <th>B_recent_std_Score_for</th>\n",
       "      <th>B_recent_std_Score_against</th>\n",
       "      <th>B_recent_avg_FGPct2_for</th>\n",
       "      <th>B_recent_avg_FGPct2_against</th>\n",
       "      <th>B_recent_std_FGPct2_for</th>\n",
       "      <th>B_recent_std_FGPct2_against</th>\n",
       "      <th>B_recent_avg_FGPct3_for</th>\n",
       "      <th>B_recent_avg_FGPct3_against</th>\n",
       "      <th>B_recent_std_FGPct3_for</th>\n",
       "      <th>B_recent_std_FGPct3_against</th>\n",
       "      <th>B_recent_avg_FTPct_for</th>\n",
       "      <th>B_recent_avg_FTPct_against</th>\n",
       "      <th>B_recent_std_FTPct_for</th>\n",
       "      <th>B_recent_std_FTPct_against</th>\n",
       "      <th>B_recent_avg_Ast_for</th>\n",
       "      <th>B_recent_avg_Ast_against</th>\n",
       "      <th>B_recent_std_Ast_for</th>\n",
       "      <th>B_recent_std_Ast_against</th>\n",
       "      <th>B_recent_avg_OR_for</th>\n",
       "      <th>B_recent_avg_OR_against</th>\n",
       "      <th>B_recent_std_OR_for</th>\n",
       "      <th>B_recent_std_OR_against</th>\n",
       "      <th>B_recent_avg_DR_for</th>\n",
       "      <th>B_recent_avg_DR_against</th>\n",
       "      <th>B_recent_std_DR_for</th>\n",
       "      <th>B_recent_std_DR_against</th>\n",
       "      <th>B_recent_avg_Stl_for</th>\n",
       "      <th>B_recent_avg_Stl_against</th>\n",
       "      <th>B_recent_std_Stl_for</th>\n",
       "      <th>B_recent_std_Stl_against</th>\n",
       "      <th>B_recent_avg_Blk_for</th>\n",
       "      <th>B_recent_avg_Blk_against</th>\n",
       "      <th>B_recent_std_Blk_for</th>\n",
       "      <th>B_recent_std_Blk_against</th>\n",
       "      <th>B_recent_avg_TO_for</th>\n",
       "      <th>B_recent_avg_TO_against</th>\n",
       "      <th>B_recent_std_TO_for</th>\n",
       "      <th>B_recent_std_TO_against</th>\n",
       "      <th>B_recent_avg_PF_for</th>\n",
       "      <th>B_recent_avg_PF_against</th>\n",
       "      <th>B_recent_std_PF_for</th>\n",
       "      <th>B_recent_std_PF_against</th>\n",
       "      <th>B_recent_avg_diff</th>\n",
       "      <th>B_conf_champs</th>\n",
       "      <th>round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>1112</td>\n",
       "      <td>80</td>\n",
       "      <td>1436</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>85.214286</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>10.222087</td>\n",
       "      <td>8.945104</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.069324</td>\n",
       "      <td>0.068456</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.098794</td>\n",
       "      <td>0.075495</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>0.152864</td>\n",
       "      <td>17.642857</td>\n",
       "      <td>15.464286</td>\n",
       "      <td>4.233511</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>15.178571</td>\n",
       "      <td>13.107143</td>\n",
       "      <td>3.362078</td>\n",
       "      <td>4.064949</td>\n",
       "      <td>27.642857</td>\n",
       "      <td>23.285714</td>\n",
       "      <td>4.076282</td>\n",
       "      <td>3.365175</td>\n",
       "      <td>8.464286</td>\n",
       "      <td>5.964286</td>\n",
       "      <td>3.730505</td>\n",
       "      <td>3.067204</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>2.017778</td>\n",
       "      <td>1.463850</td>\n",
       "      <td>16.857143</td>\n",
       "      <td>14.785714</td>\n",
       "      <td>5.163690</td>\n",
       "      <td>4.527693</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>22.071429</td>\n",
       "      <td>2.905250</td>\n",
       "      <td>4.435718</td>\n",
       "      <td>14.964286</td>\n",
       "      <td>12.522491</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>88.5</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>67.793103</td>\n",
       "      <td>63.137931</td>\n",
       "      <td>10.750924</td>\n",
       "      <td>9.368980</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.081048</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.117352</td>\n",
       "      <td>0.102801</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.110197</td>\n",
       "      <td>0.145330</td>\n",
       "      <td>14.206897</td>\n",
       "      <td>13.275862</td>\n",
       "      <td>4.446791</td>\n",
       "      <td>3.760553</td>\n",
       "      <td>12.965517</td>\n",
       "      <td>9.586207</td>\n",
       "      <td>3.598105</td>\n",
       "      <td>3.715969</td>\n",
       "      <td>25.724138</td>\n",
       "      <td>21.862069</td>\n",
       "      <td>3.196622</td>\n",
       "      <td>5.948373</td>\n",
       "      <td>6.862069</td>\n",
       "      <td>7.103448</td>\n",
       "      <td>3.546084</td>\n",
       "      <td>2.848674</td>\n",
       "      <td>2.965517</td>\n",
       "      <td>3.655172</td>\n",
       "      <td>1.736469</td>\n",
       "      <td>2.668821</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>4.303612</td>\n",
       "      <td>3.456905</td>\n",
       "      <td>15.896552</td>\n",
       "      <td>17.931034</td>\n",
       "      <td>4.026731</td>\n",
       "      <td>4.089516</td>\n",
       "      <td>4.655172</td>\n",
       "      <td>12.530245</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.5</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033566</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.115476</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.134647</td>\n",
       "      <td>0.017081</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>1113</td>\n",
       "      <td>84</td>\n",
       "      <td>1272</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>75.965517</td>\n",
       "      <td>69.172414</td>\n",
       "      <td>10.407596</td>\n",
       "      <td>10.385999</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.079974</td>\n",
       "      <td>0.076055</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.127522</td>\n",
       "      <td>0.122928</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.118812</td>\n",
       "      <td>0.123120</td>\n",
       "      <td>15.551724</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>3.490526</td>\n",
       "      <td>3.299825</td>\n",
       "      <td>13.689655</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.950465</td>\n",
       "      <td>3.869106</td>\n",
       "      <td>23.310345</td>\n",
       "      <td>20.551724</td>\n",
       "      <td>3.855293</td>\n",
       "      <td>4.345663</td>\n",
       "      <td>5.206897</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.064380</td>\n",
       "      <td>2.365746</td>\n",
       "      <td>4.241379</td>\n",
       "      <td>3.931034</td>\n",
       "      <td>2.609236</td>\n",
       "      <td>2.166685</td>\n",
       "      <td>15.517241</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.994857</td>\n",
       "      <td>3.459004</td>\n",
       "      <td>19.413793</td>\n",
       "      <td>22.862069</td>\n",
       "      <td>3.058457</td>\n",
       "      <td>4.978860</td>\n",
       "      <td>6.793103</td>\n",
       "      <td>17.095164</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>73.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>74.517241</td>\n",
       "      <td>65.827586</td>\n",
       "      <td>8.793165</td>\n",
       "      <td>8.783927</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.098708</td>\n",
       "      <td>0.073709</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.125127</td>\n",
       "      <td>0.100276</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.154544</td>\n",
       "      <td>0.125517</td>\n",
       "      <td>16.620690</td>\n",
       "      <td>13.310345</td>\n",
       "      <td>4.400484</td>\n",
       "      <td>2.838826</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>12.344828</td>\n",
       "      <td>5.090097</td>\n",
       "      <td>4.189029</td>\n",
       "      <td>25.965517</td>\n",
       "      <td>23.586207</td>\n",
       "      <td>3.666676</td>\n",
       "      <td>3.778590</td>\n",
       "      <td>7.379310</td>\n",
       "      <td>7.275862</td>\n",
       "      <td>3.181648</td>\n",
       "      <td>2.854024</td>\n",
       "      <td>5.068966</td>\n",
       "      <td>3.172414</td>\n",
       "      <td>3.231506</td>\n",
       "      <td>2.248650</td>\n",
       "      <td>15.068966</td>\n",
       "      <td>13.793103</td>\n",
       "      <td>4.750780</td>\n",
       "      <td>4.018531</td>\n",
       "      <td>18.758621</td>\n",
       "      <td>19.931034</td>\n",
       "      <td>4.067456</td>\n",
       "      <td>6.079979</td>\n",
       "      <td>8.689655</td>\n",
       "      <td>10.953570</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.5</td>\n",
       "      <td>68.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>1141</td>\n",
       "      <td>79</td>\n",
       "      <td>1166</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>79.344828</td>\n",
       "      <td>73.241379</td>\n",
       "      <td>10.563408</td>\n",
       "      <td>10.130156</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.088209</td>\n",
       "      <td>0.089039</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.121457</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.081641</td>\n",
       "      <td>0.092883</td>\n",
       "      <td>15.620690</td>\n",
       "      <td>11.793103</td>\n",
       "      <td>4.475359</td>\n",
       "      <td>3.226069</td>\n",
       "      <td>10.586207</td>\n",
       "      <td>12.241379</td>\n",
       "      <td>3.533716</td>\n",
       "      <td>4.440128</td>\n",
       "      <td>23.275862</td>\n",
       "      <td>18.896552</td>\n",
       "      <td>4.229913</td>\n",
       "      <td>4.047291</td>\n",
       "      <td>7.103448</td>\n",
       "      <td>8.448276</td>\n",
       "      <td>2.955999</td>\n",
       "      <td>2.924655</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.482759</td>\n",
       "      <td>2.337241</td>\n",
       "      <td>1.976508</td>\n",
       "      <td>16.068966</td>\n",
       "      <td>18.241379</td>\n",
       "      <td>4.503360</td>\n",
       "      <td>3.274294</td>\n",
       "      <td>20.965517</td>\n",
       "      <td>21.689655</td>\n",
       "      <td>4.483687</td>\n",
       "      <td>3.832463</td>\n",
       "      <td>6.103448</td>\n",
       "      <td>13.216876</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.5</td>\n",
       "      <td>69.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.053419</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>15.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>79.242424</td>\n",
       "      <td>64.333333</td>\n",
       "      <td>11.670098</td>\n",
       "      <td>9.280190</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>0.077470</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.102340</td>\n",
       "      <td>0.143098</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.093757</td>\n",
       "      <td>0.119023</td>\n",
       "      <td>16.818182</td>\n",
       "      <td>12.363636</td>\n",
       "      <td>4.677785</td>\n",
       "      <td>3.239145</td>\n",
       "      <td>10.878788</td>\n",
       "      <td>11.060606</td>\n",
       "      <td>2.766268</td>\n",
       "      <td>3.807981</td>\n",
       "      <td>23.181818</td>\n",
       "      <td>21.363636</td>\n",
       "      <td>3.083962</td>\n",
       "      <td>4.377316</td>\n",
       "      <td>8.393939</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>2.717322</td>\n",
       "      <td>2.261255</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>2.575758</td>\n",
       "      <td>2.129806</td>\n",
       "      <td>2.337967</td>\n",
       "      <td>17.060606</td>\n",
       "      <td>13.363636</td>\n",
       "      <td>4.477215</td>\n",
       "      <td>3.364572</td>\n",
       "      <td>17.272727</td>\n",
       "      <td>19.393939</td>\n",
       "      <td>3.119320</td>\n",
       "      <td>3.295894</td>\n",
       "      <td>14.909091</td>\n",
       "      <td>14.876080</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.079917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052036</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.064912</td>\n",
       "      <td>0.173295</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>1143</td>\n",
       "      <td>76</td>\n",
       "      <td>1301</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>74.482759</td>\n",
       "      <td>69.758621</td>\n",
       "      <td>7.410284</td>\n",
       "      <td>8.129585</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.066102</td>\n",
       "      <td>0.089574</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.116170</td>\n",
       "      <td>0.082790</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.144146</td>\n",
       "      <td>0.132386</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.034483</td>\n",
       "      <td>4.085418</td>\n",
       "      <td>3.232227</td>\n",
       "      <td>11.241379</td>\n",
       "      <td>11.172414</td>\n",
       "      <td>3.753816</td>\n",
       "      <td>4.921752</td>\n",
       "      <td>24.379310</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.986534</td>\n",
       "      <td>3.605346</td>\n",
       "      <td>6.551724</td>\n",
       "      <td>5.931034</td>\n",
       "      <td>2.634837</td>\n",
       "      <td>3.420454</td>\n",
       "      <td>2.793103</td>\n",
       "      <td>2.517241</td>\n",
       "      <td>1.687830</td>\n",
       "      <td>1.534497</td>\n",
       "      <td>14.931034</td>\n",
       "      <td>14.172414</td>\n",
       "      <td>4.008919</td>\n",
       "      <td>4.512981</td>\n",
       "      <td>17.103448</td>\n",
       "      <td>19.103448</td>\n",
       "      <td>3.519124</td>\n",
       "      <td>3.777298</td>\n",
       "      <td>4.724138</td>\n",
       "      <td>10.577059</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>65.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>72.400000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>10.864126</td>\n",
       "      <td>9.610211</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.102842</td>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.094883</td>\n",
       "      <td>0.133451</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.097546</td>\n",
       "      <td>0.116456</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>12.566667</td>\n",
       "      <td>4.392345</td>\n",
       "      <td>3.178106</td>\n",
       "      <td>9.733333</td>\n",
       "      <td>10.533333</td>\n",
       "      <td>3.778445</td>\n",
       "      <td>4.403653</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>21.433333</td>\n",
       "      <td>4.038474</td>\n",
       "      <td>4.164167</td>\n",
       "      <td>7.766667</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>3.555092</td>\n",
       "      <td>2.476665</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>2.455885</td>\n",
       "      <td>1.627033</td>\n",
       "      <td>14.633333</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>4.663107</td>\n",
       "      <td>3.768289</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>19.333333</td>\n",
       "      <td>3.699352</td>\n",
       "      <td>4.291807</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>16.106577</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>83.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>1163</td>\n",
       "      <td>58</td>\n",
       "      <td>1140</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>80.033333</td>\n",
       "      <td>71.400000</td>\n",
       "      <td>12.145095</td>\n",
       "      <td>9.258780</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.104143</td>\n",
       "      <td>0.077361</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.117574</td>\n",
       "      <td>0.097358</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.112417</td>\n",
       "      <td>0.120166</td>\n",
       "      <td>15.633333</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>5.453745</td>\n",
       "      <td>4.197221</td>\n",
       "      <td>14.766667</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>4.419087</td>\n",
       "      <td>5.058491</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>4.454960</td>\n",
       "      <td>4.295928</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>2.516280</td>\n",
       "      <td>3.478505</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>3.407100</td>\n",
       "      <td>2.890213</td>\n",
       "      <td>13.366667</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>3.510935</td>\n",
       "      <td>3.670150</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>19.266667</td>\n",
       "      <td>3.465785</td>\n",
       "      <td>4.543310</td>\n",
       "      <td>8.633333</td>\n",
       "      <td>16.742518</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.5</td>\n",
       "      <td>68.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>72.451613</td>\n",
       "      <td>63.677419</td>\n",
       "      <td>9.726895</td>\n",
       "      <td>9.792998</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.082135</td>\n",
       "      <td>0.071911</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.134514</td>\n",
       "      <td>0.118374</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.083710</td>\n",
       "      <td>0.111363</td>\n",
       "      <td>13.419355</td>\n",
       "      <td>10.774194</td>\n",
       "      <td>4.050684</td>\n",
       "      <td>3.285277</td>\n",
       "      <td>10.870968</td>\n",
       "      <td>10.419355</td>\n",
       "      <td>3.656719</td>\n",
       "      <td>2.887646</td>\n",
       "      <td>24.419355</td>\n",
       "      <td>20.193548</td>\n",
       "      <td>4.574295</td>\n",
       "      <td>2.836636</td>\n",
       "      <td>6.935484</td>\n",
       "      <td>5.483871</td>\n",
       "      <td>2.689488</td>\n",
       "      <td>2.421538</td>\n",
       "      <td>2.516129</td>\n",
       "      <td>2.225806</td>\n",
       "      <td>1.685220</td>\n",
       "      <td>1.641778</td>\n",
       "      <td>14.483871</td>\n",
       "      <td>13.741935</td>\n",
       "      <td>4.073065</td>\n",
       "      <td>3.984154</td>\n",
       "      <td>21.419355</td>\n",
       "      <td>21.387097</td>\n",
       "      <td>3.365527</td>\n",
       "      <td>3.595603</td>\n",
       "      <td>8.774194</td>\n",
       "      <td>12.752390</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.5</td>\n",
       "      <td>75.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  A_TeamID  A_Score  B_TeamID  B_Score  NumOT  A_PlayIn  B_PlayIn  \\\n",
       "0    2003      1112       80      1436       51      0         0         0   \n",
       "1    2003      1113       84      1272       71      0         0         0   \n",
       "2    2003      1141       79      1166       73      0         0         0   \n",
       "3    2003      1143       76      1301       74      1         0         0   \n",
       "4    2003      1163       58      1140       53      0         0         0   \n",
       "\n",
       "   A_SeedNum  B_SeedNum  A_num_games  A_win_ratio  A_avg_Score_for  \\\n",
       "0          1         16           28     0.892857        85.214286   \n",
       "1         10          7           29     0.620690        75.965517   \n",
       "2         11          6           29     0.793103        79.344828   \n",
       "3          8          9           29     0.724138        74.482759   \n",
       "4          5         12           30     0.700000        80.033333   \n",
       "\n",
       "   A_avg_Score_against  A_std_Score_for  A_std_Score_against  \\\n",
       "0            70.250000        10.222087             8.945104   \n",
       "1            69.172414        10.407596            10.385999   \n",
       "2            73.241379        10.563408            10.130156   \n",
       "3            69.758621         7.410284             8.129585   \n",
       "4            71.400000        12.145095             9.258780   \n",
       "\n",
       "   A_avg_FGPct2_for  A_avg_FGPct2_against  A_std_FGPct2_for  \\\n",
       "0          0.464286              0.428571          0.069324   \n",
       "1          0.482759              0.482759          0.079974   \n",
       "2          0.517241              0.448276          0.088209   \n",
       "3          0.448276              0.482759          0.066102   \n",
       "4          0.466667              0.400000          0.104143   \n",
       "\n",
       "   A_std_FGPct2_against  A_avg_FGPct3_for  A_avg_FGPct3_against  \\\n",
       "0              0.068456          0.321429              0.285714   \n",
       "1              0.076055          0.275862              0.310345   \n",
       "2              0.089039          0.344828              0.344828   \n",
       "3              0.089574          0.344828              0.310345   \n",
       "4              0.077361          0.366667              0.300000   \n",
       "\n",
       "   A_std_FGPct3_for  A_std_FGPct3_against  A_avg_FTPct_for  \\\n",
       "0          0.098794              0.075495         0.678571   \n",
       "1          0.127522              0.122928         0.655172   \n",
       "2          0.121457              0.144578         0.724138   \n",
       "3          0.116170              0.082790         0.655172   \n",
       "4          0.117574              0.097358         0.633333   \n",
       "\n",
       "   A_avg_FTPct_against  A_std_FTPct_for  A_std_FTPct_against  A_avg_Ast_for  \\\n",
       "0             0.571429         0.106410             0.152864      17.642857   \n",
       "1             0.655172         0.118812             0.123120      15.551724   \n",
       "2             0.655172         0.081641             0.092883      15.620690   \n",
       "3             0.620690         0.144146             0.132386      16.000000   \n",
       "4             0.666667         0.112417             0.120166      15.633333   \n",
       "\n",
       "   A_avg_Ast_against  A_std_Ast_for  A_std_Ast_against  A_avg_OR_for  \\\n",
       "0          15.464286       4.233511           3.250000     15.178571   \n",
       "1          14.068966       3.490526           3.299825     13.689655   \n",
       "2          11.793103       4.475359           3.226069     10.586207   \n",
       "3          16.034483       4.085418           3.232227     11.241379   \n",
       "4          13.600000       5.453745           4.197221     14.766667   \n",
       "\n",
       "   A_avg_OR_against  A_std_OR_for  A_std_OR_against  A_avg_DR_for  \\\n",
       "0         13.107143      3.362078          4.064949     27.642857   \n",
       "1         11.000000      4.950465          3.869106     23.310345   \n",
       "2         12.241379      3.533716          4.440128     23.275862   \n",
       "3         11.172414      3.753816          4.921752     24.379310   \n",
       "4         15.333333      4.419087          5.058491     27.900000   \n",
       "\n",
       "   A_avg_DR_against  A_std_DR_for  A_std_DR_against  A_avg_Stl_for  \\\n",
       "0         23.285714      4.076282          3.365175       8.464286   \n",
       "1         20.551724      3.855293          4.345663       5.206897   \n",
       "2         18.896552      4.229913          4.047291       7.103448   \n",
       "3         23.000000      4.986534          3.605346       6.551724   \n",
       "4         21.500000      4.454960          4.295928       5.933333   \n",
       "\n",
       "   A_avg_Stl_against  A_std_Stl_for  A_std_Stl_against  A_avg_Blk_for  \\\n",
       "0           5.964286       3.730505           3.067204       4.214286   \n",
       "1           6.000000       2.064380           2.365746       4.241379   \n",
       "2           8.448276       2.955999           2.924655       4.000000   \n",
       "3           5.931034       2.634837           3.420454       2.793103   \n",
       "4           7.800000       2.516280           3.478505       7.733333   \n",
       "\n",
       "   A_avg_Blk_against  A_std_Blk_for  A_std_Blk_against  A_avg_TO_for  \\\n",
       "0           2.392857       2.017778           1.463850     16.857143   \n",
       "1           3.931034       2.609236           2.166685     15.517241   \n",
       "2           2.482759       2.337241           1.976508     16.068966   \n",
       "3           2.517241       1.687830           1.534497     14.931034   \n",
       "4           4.133333       3.407100           2.890213     13.366667   \n",
       "\n",
       "   A_avg_TO_against  A_std_TO_for  A_std_TO_against  A_avg_PF_for  \\\n",
       "0         14.785714      5.163690          4.527693     17.750000   \n",
       "1         14.000000      2.994857          3.459004     19.413793   \n",
       "2         18.241379      4.503360          3.274294     20.965517   \n",
       "3         14.172414      4.008919          4.512981     17.103448   \n",
       "4         15.800000      3.510935          3.670150     18.400000   \n",
       "\n",
       "   A_avg_PF_against  A_std_PF_for  A_std_PF_against  A_avg_diff  A_std_diff  \\\n",
       "0         22.071429      2.905250          4.435718   14.964286   12.522491   \n",
       "1         22.862069      3.058457          4.978860    6.793103   17.095164   \n",
       "2         21.689655      4.483687          3.832463    6.103448   13.216876   \n",
       "3         19.103448      3.519124          3.777298    4.724138   10.577059   \n",
       "4         19.266667      3.465785          4.543310    8.633333   16.742518   \n",
       "\n",
       "   A_home_win_ratio  A_away_win_ratio  A_neutral_win_ratio  A_home_missing  \\\n",
       "0          0.933333          0.916667             0.000000               0   \n",
       "1          0.800000          0.400000             0.500000               0   \n",
       "2          0.916667          0.642857             1.000000               0   \n",
       "3          0.928571          0.636364             0.250000               0   \n",
       "4          0.875000          0.454545             0.666667               0   \n",
       "\n",
       "   A_away_missing  A_neutral_missing  A_close_game_missing  \\\n",
       "0               0                  0                     0   \n",
       "1               0                  0                     0   \n",
       "2               0                  0                     0   \n",
       "3               0                  0                     0   \n",
       "4               0                  0                     0   \n",
       "\n",
       "   A_close_game_ratio  A_close_win_ratio  A_recent_win_ratio  \\\n",
       "0            0.178571           0.600000                 0.5   \n",
       "1            0.379310           0.272727                 0.5   \n",
       "2            0.379310           0.545455                 1.0   \n",
       "3            0.344828           0.200000                 0.5   \n",
       "4            0.300000           0.111111                 0.5   \n",
       "\n",
       "   A_recent_avg_Score_for  A_recent_avg_Score_against  A_recent_std_Score_for  \\\n",
       "0                    88.5                        88.0                     0.0   \n",
       "1                    78.0                        73.5                     0.0   \n",
       "2                    85.5                        69.5                     8.5   \n",
       "3                    65.5                        62.5                     0.0   \n",
       "4                    68.0                        70.5                     0.0   \n",
       "\n",
       "   A_recent_std_Score_against  A_recent_avg_FGPct2_for  \\\n",
       "0                         0.0                      0.0   \n",
       "1                         0.0                      0.0   \n",
       "2                         2.5                      0.5   \n",
       "3                         0.0                      0.0   \n",
       "4                         0.0                      0.0   \n",
       "\n",
       "   A_recent_avg_FGPct2_against  A_recent_std_FGPct2_for  \\\n",
       "0                          0.0                 0.000000   \n",
       "1                          0.0                 0.000000   \n",
       "2                          0.0                 0.006747   \n",
       "3                          0.0                 0.000000   \n",
       "4                          0.0                 0.000000   \n",
       "\n",
       "   A_recent_std_FGPct2_against  A_recent_avg_FGPct3_for  \\\n",
       "0                     0.000000                      0.0   \n",
       "1                     0.000000                      0.0   \n",
       "2                     0.040816                      0.5   \n",
       "3                     0.000000                      0.0   \n",
       "4                     0.000000                      0.0   \n",
       "\n",
       "   A_recent_avg_FGPct3_against  A_recent_std_FGPct3_for  \\\n",
       "0                          0.0                 0.000000   \n",
       "1                          0.0                 0.000000   \n",
       "2                          0.0                 0.122807   \n",
       "3                          0.0                 0.000000   \n",
       "4                          0.0                 0.000000   \n",
       "\n",
       "   A_recent_std_FGPct3_against  A_recent_avg_FTPct_for  \\\n",
       "0                     0.000000                     0.0   \n",
       "1                     0.000000                     0.0   \n",
       "2                     0.053419                     0.5   \n",
       "3                     0.000000                     0.0   \n",
       "4                     0.000000                     0.0   \n",
       "\n",
       "   A_recent_avg_FTPct_against  A_recent_std_FTPct_for  \\\n",
       "0                         0.0                0.000000   \n",
       "1                         0.0                0.000000   \n",
       "2                         0.5                0.015152   \n",
       "3                         0.0                0.000000   \n",
       "4                         0.0                0.000000   \n",
       "\n",
       "   A_recent_std_FTPct_against  A_recent_avg_Ast_for  A_recent_avg_Ast_against  \\\n",
       "0                    0.000000                  18.5                      14.5   \n",
       "1                    0.000000                  16.0                      14.5   \n",
       "2                    0.034615                  15.5                       9.0   \n",
       "3                    0.000000                  14.5                      11.5   \n",
       "4                    0.000000                  10.0                      12.5   \n",
       "\n",
       "   A_recent_std_Ast_for  A_recent_std_Ast_against  A_recent_avg_OR_for  \\\n",
       "0                   0.0                       0.0                 17.5   \n",
       "1                   0.0                       0.0                 14.5   \n",
       "2                   0.5                       2.0                  9.5   \n",
       "3                   0.0                       0.0                 15.5   \n",
       "4                   0.0                       0.0                 16.5   \n",
       "\n",
       "   A_recent_avg_OR_against  A_recent_std_OR_for  A_recent_std_OR_against  \\\n",
       "0                     12.5                  0.0                      0.0   \n",
       "1                      6.5                  0.0                      0.0   \n",
       "2                     13.0                  1.5                      1.0   \n",
       "3                     16.5                  0.0                      0.0   \n",
       "4                     22.5                  0.0                      0.0   \n",
       "\n",
       "   A_recent_avg_DR_for  A_recent_avg_DR_against  A_recent_std_DR_for  \\\n",
       "0                 23.5                     29.5                  0.0   \n",
       "1                 23.5                     17.5                  0.0   \n",
       "2                 25.0                     14.5                  4.0   \n",
       "3                 30.0                     23.5                  0.0   \n",
       "4                 25.0                     20.0                  0.0   \n",
       "\n",
       "   A_recent_std_DR_against  A_recent_avg_Stl_for  A_recent_avg_Stl_against  \\\n",
       "0                      0.0                  11.5                       6.5   \n",
       "1                      0.0                   5.0                       7.0   \n",
       "2                      2.5                   7.0                      12.0   \n",
       "3                      0.0                   3.5                       9.0   \n",
       "4                      0.0                   4.5                       7.5   \n",
       "\n",
       "   A_recent_std_Stl_for  A_recent_std_Stl_against  A_recent_avg_Blk_for  \\\n",
       "0                   0.0                       0.0                   3.5   \n",
       "1                   0.0                       0.0                   1.5   \n",
       "2                   3.0                       2.0                   3.0   \n",
       "3                   0.0                       0.0                   4.5   \n",
       "4                   0.0                       0.0                   7.0   \n",
       "\n",
       "   A_recent_avg_Blk_against  A_recent_std_Blk_for  A_recent_std_Blk_against  \\\n",
       "0                       3.5                   0.0                       0.0   \n",
       "1                       2.0                   0.0                       0.0   \n",
       "2                       2.5                   1.0                       1.5   \n",
       "3                       4.0                   0.0                       0.0   \n",
       "4                       7.5                   0.0                       0.0   \n",
       "\n",
       "   A_recent_avg_TO_for  A_recent_avg_TO_against  A_recent_std_TO_for  \\\n",
       "0                 18.0                     12.0                  0.0   \n",
       "1                 15.0                     15.0                  0.0   \n",
       "2                 11.5                     16.0                  4.5   \n",
       "3                  8.5                     15.0                  0.0   \n",
       "4                 12.0                     18.0                  0.0   \n",
       "\n",
       "   A_recent_std_TO_against  A_recent_avg_PF_for  A_recent_avg_PF_against  \\\n",
       "0                      0.0                 20.0                     24.0   \n",
       "1                      0.0                 19.5                     19.5   \n",
       "2                      1.0                 16.0                     25.5   \n",
       "3                      0.0                 15.5                     17.0   \n",
       "4                      0.0                 23.0                     16.5   \n",
       "\n",
       "   A_recent_std_PF_for  A_recent_std_PF_against  A_recent_avg_diff  \\\n",
       "0                  0.0                      0.0                0.5   \n",
       "1                  0.0                      0.0                4.5   \n",
       "2                  1.0                      0.5               16.0   \n",
       "3                  0.0                      0.0                3.0   \n",
       "4                  0.0                      0.0               -2.5   \n",
       "\n",
       "   A_conf_champs  B_num_games  B_win_ratio  B_avg_Score_for  \\\n",
       "0            0.0           29     0.655172        67.793103   \n",
       "1            0.0           29     0.793103        74.517241   \n",
       "2            1.0           33     0.878788        79.242424   \n",
       "3            0.0           30     0.600000        72.400000   \n",
       "4            0.0           31     0.741935        72.451613   \n",
       "\n",
       "   B_avg_Score_against  B_std_Score_for  B_std_Score_against  \\\n",
       "0            63.137931        10.750924             9.368980   \n",
       "1            65.827586         8.793165             8.783927   \n",
       "2            64.333333        11.670098             9.280190   \n",
       "3            68.000000        10.864126             9.610211   \n",
       "4            63.677419         9.726895             9.792998   \n",
       "\n",
       "   B_avg_FGPct2_for  B_avg_FGPct2_against  B_std_FGPct2_for  \\\n",
       "0          0.448276              0.448276          0.087979   \n",
       "1          0.448276              0.379310          0.098708   \n",
       "2          0.515152              0.454545          0.078507   \n",
       "3          0.500000              0.466667          0.102842   \n",
       "4          0.483871              0.419355          0.082135   \n",
       "\n",
       "   B_std_FGPct2_against  B_avg_FGPct3_for  B_avg_FGPct3_against  \\\n",
       "0              0.081048          0.344828              0.310345   \n",
       "1              0.073709          0.310345              0.310345   \n",
       "2              0.077470          0.363636              0.303030   \n",
       "3              0.087979          0.333333              0.300000   \n",
       "4              0.071911          0.354839              0.258065   \n",
       "\n",
       "   B_std_FGPct3_for  B_std_FGPct3_against  B_avg_FTPct_for  \\\n",
       "0          0.117352              0.102801         0.620690   \n",
       "1          0.125127              0.100276         0.586207   \n",
       "2          0.102340              0.143098         0.636364   \n",
       "3          0.094883              0.133451         0.766667   \n",
       "4          0.134514              0.118374         0.709677   \n",
       "\n",
       "   B_avg_FTPct_against  B_std_FTPct_for  B_std_FTPct_against  B_avg_Ast_for  \\\n",
       "0             0.655172         0.110197             0.145330      14.206897   \n",
       "1             0.620690         0.154544             0.125517      16.620690   \n",
       "2             0.666667         0.093757             0.119023      16.818182   \n",
       "3             0.666667         0.097546             0.116456      14.666667   \n",
       "4             0.645161         0.083710             0.111363      13.419355   \n",
       "\n",
       "   B_avg_Ast_against  B_std_Ast_for  B_std_Ast_against  B_avg_OR_for  \\\n",
       "0          13.275862       4.446791           3.760553     12.965517   \n",
       "1          13.310345       4.400484           2.838826     14.068966   \n",
       "2          12.363636       4.677785           3.239145     10.878788   \n",
       "3          12.566667       4.392345           3.178106      9.733333   \n",
       "4          10.774194       4.050684           3.285277     10.870968   \n",
       "\n",
       "   B_avg_OR_against  B_std_OR_for  B_std_OR_against  B_avg_DR_for  \\\n",
       "0          9.586207      3.598105          3.715969     25.724138   \n",
       "1         12.344828      5.090097          4.189029     25.965517   \n",
       "2         11.060606      2.766268          3.807981     23.181818   \n",
       "3         10.533333      3.778445          4.403653     22.033333   \n",
       "4         10.419355      3.656719          2.887646     24.419355   \n",
       "\n",
       "   B_avg_DR_against  B_std_DR_for  B_std_DR_against  B_avg_Stl_for  \\\n",
       "0         21.862069      3.196622          5.948373       6.862069   \n",
       "1         23.586207      3.666676          3.778590       7.379310   \n",
       "2         21.363636      3.083962          4.377316       8.393939   \n",
       "3         21.433333      4.038474          4.164167       7.766667   \n",
       "4         20.193548      4.574295          2.836636       6.935484   \n",
       "\n",
       "   B_avg_Stl_against  B_std_Stl_for  B_std_Stl_against  B_avg_Blk_for  \\\n",
       "0           7.103448       3.546084           2.848674       2.965517   \n",
       "1           7.275862       3.181648           2.854024       5.068966   \n",
       "2           6.333333       2.717322           2.261255       4.454545   \n",
       "3           7.433333       3.555092           2.476665       3.066667   \n",
       "4           5.483871       2.689488           2.421538       2.516129   \n",
       "\n",
       "   B_avg_Blk_against  B_std_Blk_for  B_std_Blk_against  B_avg_TO_for  \\\n",
       "0           3.655172       1.736469           2.668821     13.000000   \n",
       "1           3.172414       3.231506           2.248650     15.068966   \n",
       "2           2.575758       2.129806           2.337967     17.060606   \n",
       "3           2.833333       2.455885           1.627033     14.633333   \n",
       "4           2.225806       1.685220           1.641778     14.483871   \n",
       "\n",
       "   B_avg_TO_against  B_std_TO_for  B_std_TO_against  B_avg_PF_for  \\\n",
       "0         14.068966      4.303612          3.456905     15.896552   \n",
       "1         13.793103      4.750780          4.018531     18.758621   \n",
       "2         13.363636      4.477215          3.364572     17.272727   \n",
       "3         14.200000      4.663107          3.768289     18.666667   \n",
       "4         13.741935      4.073065          3.984154     21.419355   \n",
       "\n",
       "   B_avg_PF_against  B_std_PF_for  B_std_PF_against  B_avg_diff  B_std_diff  \\\n",
       "0         17.931034      4.026731          4.089516    4.655172   12.530245   \n",
       "1         19.931034      4.067456          6.079979    8.689655   10.953570   \n",
       "2         19.393939      3.119320          3.295894   14.909091   14.876080   \n",
       "3         19.333333      3.699352          4.291807    4.400000   16.106577   \n",
       "4         21.387097      3.365527          3.595603    8.774194   12.752390   \n",
       "\n",
       "   B_home_win_ratio  B_away_win_ratio  B_neutral_win_ratio  B_home_missing  \\\n",
       "0          0.900000          0.500000             0.600000               0   \n",
       "1          0.875000          0.636364             1.000000               0   \n",
       "2          1.000000          0.636364             1.000000               0   \n",
       "3          0.812500          0.300000             0.500000               0   \n",
       "4          0.928571          0.545455             0.666667               0   \n",
       "\n",
       "   B_away_missing  B_neutral_missing  B_close_game_missing  \\\n",
       "0               0                  0                     0   \n",
       "1               0                  0                     0   \n",
       "2               0                  0                     0   \n",
       "3               0                  0                     0   \n",
       "4               0                  0                     0   \n",
       "\n",
       "   B_close_game_ratio  B_close_win_ratio  B_recent_win_ratio  \\\n",
       "0            0.448276           0.230769                 1.0   \n",
       "1            0.241379           0.285714                 0.5   \n",
       "2            0.212121           0.571429                 1.0   \n",
       "3            0.466667           0.142857                 0.5   \n",
       "4            0.290323           0.111111                 0.5   \n",
       "\n",
       "   B_recent_avg_Score_for  B_recent_avg_Score_against  B_recent_std_Score_for  \\\n",
       "0                    61.5                        53.0                     5.5   \n",
       "1                    68.5                        67.0                     0.0   \n",
       "2                    75.0                        62.5                     5.0   \n",
       "3                    82.0                        83.5                     0.0   \n",
       "4                    75.5                        71.0                     0.0   \n",
       "\n",
       "   B_recent_std_Score_against  B_recent_avg_FGPct2_for  \\\n",
       "0                         2.0                      0.0   \n",
       "1                         0.0                      0.0   \n",
       "2                         6.5                      0.5   \n",
       "3                         0.0                      0.0   \n",
       "4                         0.0                      0.0   \n",
       "\n",
       "   B_recent_avg_FGPct2_against  B_recent_std_FGPct2_for  \\\n",
       "0                          0.0                 0.033566   \n",
       "1                          0.0                 0.000000   \n",
       "2                          0.5                 0.022528   \n",
       "3                          0.0                 0.000000   \n",
       "4                          0.0                 0.000000   \n",
       "\n",
       "   B_recent_std_FGPct2_against  B_recent_avg_FGPct3_for  \\\n",
       "0                     0.007612                      0.0   \n",
       "1                     0.000000                      0.0   \n",
       "2                     0.079917                      0.0   \n",
       "3                     0.000000                      0.0   \n",
       "4                     0.000000                      0.0   \n",
       "\n",
       "   B_recent_avg_FGPct3_against  B_recent_std_FGPct3_for  \\\n",
       "0                          0.0                 0.047619   \n",
       "1                          0.0                 0.000000   \n",
       "2                          0.0                 0.052036   \n",
       "3                          0.0                 0.000000   \n",
       "4                          0.0                 0.000000   \n",
       "\n",
       "   B_recent_std_FGPct3_against  B_recent_avg_FTPct_for  \\\n",
       "0                     0.115476                     0.5   \n",
       "1                     0.000000                     0.0   \n",
       "2                     0.107843                     0.5   \n",
       "3                     0.000000                     0.0   \n",
       "4                     0.000000                     0.0   \n",
       "\n",
       "   B_recent_avg_FTPct_against  B_recent_std_FTPct_for  \\\n",
       "0                         0.5                0.134647   \n",
       "1                         0.0                0.000000   \n",
       "2                         0.5                0.064912   \n",
       "3                         0.0                0.000000   \n",
       "4                         0.0                0.000000   \n",
       "\n",
       "   B_recent_std_FTPct_against  B_recent_avg_Ast_for  B_recent_avg_Ast_against  \\\n",
       "0                    0.017081                  10.0                       9.5   \n",
       "1                    0.000000                  11.0                      10.5   \n",
       "2                    0.173295                  17.0                      13.5   \n",
       "3                    0.000000                  15.0                      12.0   \n",
       "4                    0.000000                  12.5                      18.5   \n",
       "\n",
       "   B_recent_std_Ast_for  B_recent_std_Ast_against  B_recent_avg_OR_for  \\\n",
       "0                   1.0                       3.5                  9.0   \n",
       "1                   0.0                       0.0                 15.0   \n",
       "2                   1.0                       0.5                 11.0   \n",
       "3                   0.0                       0.0                  9.0   \n",
       "4                   0.0                       0.0                 11.0   \n",
       "\n",
       "   B_recent_avg_OR_against  B_recent_std_OR_for  B_recent_std_OR_against  \\\n",
       "0                     11.5                  2.0                      1.5   \n",
       "1                     16.5                  0.0                      0.0   \n",
       "2                     10.0                  2.0                      3.0   \n",
       "3                     11.5                  0.0                      0.0   \n",
       "4                      8.0                  0.0                      0.0   \n",
       "\n",
       "   B_recent_avg_DR_for  B_recent_avg_DR_against  B_recent_std_DR_for  \\\n",
       "0                 28.0                     19.5                  2.0   \n",
       "1                 25.0                     24.5                  0.0   \n",
       "2                 26.5                     19.5                  3.5   \n",
       "3                 25.5                     23.5                  0.0   \n",
       "4                 23.0                     22.0                  0.0   \n",
       "\n",
       "   B_recent_std_DR_against  B_recent_avg_Stl_for  B_recent_avg_Stl_against  \\\n",
       "0                      2.5                   7.5                       7.5   \n",
       "1                      0.0                   3.0                       7.5   \n",
       "2                      1.5                   5.5                       6.5   \n",
       "3                      0.0                   3.5                       6.0   \n",
       "4                      0.0                   8.0                       1.5   \n",
       "\n",
       "   B_recent_std_Stl_for  B_recent_std_Stl_against  B_recent_avg_Blk_for  \\\n",
       "0                   3.5                       1.5                   2.5   \n",
       "1                   0.0                       0.0                   4.5   \n",
       "2                   0.5                       1.5                   6.0   \n",
       "3                   0.0                       0.0                   2.5   \n",
       "4                   0.0                       0.0                   4.0   \n",
       "\n",
       "   B_recent_avg_Blk_against  B_recent_std_Blk_for  B_recent_std_Blk_against  \\\n",
       "0                       3.0                   0.5                       1.0   \n",
       "1                       2.0                   0.0                       0.0   \n",
       "2                       3.0                   1.0                       3.0   \n",
       "3                       4.0                   0.0                       0.0   \n",
       "4                       3.0                   0.0                       0.0   \n",
       "\n",
       "   B_recent_avg_TO_for  B_recent_avg_TO_against  B_recent_std_TO_for  \\\n",
       "0                 11.5                     15.0                  3.5   \n",
       "1                 10.5                     13.0                  0.0   \n",
       "2                 13.0                     14.5                  1.0   \n",
       "3                  7.0                     14.0                  0.0   \n",
       "4                 13.5                      8.0                  0.0   \n",
       "\n",
       "   B_recent_std_TO_against  B_recent_avg_PF_for  B_recent_avg_PF_against  \\\n",
       "0                      2.0                 17.5                     19.0   \n",
       "1                      0.0                 26.5                     24.5   \n",
       "2                      2.5                 18.0                     17.5   \n",
       "3                      0.0                 24.5                     23.0   \n",
       "4                      0.0                 21.5                     22.0   \n",
       "\n",
       "   B_recent_std_PF_for  B_recent_std_PF_against  B_recent_avg_diff  \\\n",
       "0                  2.5                      3.0                8.5   \n",
       "1                  0.0                      0.0                1.5   \n",
       "2                  2.0                      1.5               12.5   \n",
       "3                  0.0                      0.0               -1.5   \n",
       "4                  0.0                      0.0                4.5   \n",
       "\n",
       "   B_conf_champs  round  \n",
       "0            1.0      1  \n",
       "1            0.0      1  \n",
       "2            1.0      1  \n",
       "3            0.0      1  \n",
       "4            0.0      1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename W, L to A, B\n",
    "features.columns = [x.replace('W', 'A_', 1) if x[0] == 'W' else x for x in features.columns]\n",
    "features.columns = [x.replace('L', 'B_', 1) if x[0] == 'L' else x for x in features.columns]\n",
    "\n",
    "# check\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_PlayIn</th>\n",
       "      <th>A_Score</th>\n",
       "      <th>A_SeedNum</th>\n",
       "      <th>A_TeamID</th>\n",
       "      <th>A_avg_Ast_against</th>\n",
       "      <th>A_avg_Ast_for</th>\n",
       "      <th>A_avg_Blk_against</th>\n",
       "      <th>A_avg_Blk_for</th>\n",
       "      <th>A_avg_DR_against</th>\n",
       "      <th>A_avg_DR_for</th>\n",
       "      <th>A_avg_FGPct2_against</th>\n",
       "      <th>A_avg_FGPct2_for</th>\n",
       "      <th>A_avg_FGPct3_against</th>\n",
       "      <th>A_avg_FGPct3_for</th>\n",
       "      <th>A_avg_FTPct_against</th>\n",
       "      <th>A_avg_FTPct_for</th>\n",
       "      <th>A_avg_OR_against</th>\n",
       "      <th>A_avg_OR_for</th>\n",
       "      <th>A_avg_PF_against</th>\n",
       "      <th>A_avg_PF_for</th>\n",
       "      <th>A_avg_Score_against</th>\n",
       "      <th>A_avg_Score_for</th>\n",
       "      <th>A_avg_Stl_against</th>\n",
       "      <th>A_avg_Stl_for</th>\n",
       "      <th>A_avg_TO_against</th>\n",
       "      <th>A_avg_TO_for</th>\n",
       "      <th>A_avg_diff</th>\n",
       "      <th>A_away_missing</th>\n",
       "      <th>A_away_win_ratio</th>\n",
       "      <th>A_close_game_missing</th>\n",
       "      <th>A_close_game_ratio</th>\n",
       "      <th>A_close_win_ratio</th>\n",
       "      <th>A_conf_champs</th>\n",
       "      <th>A_home_missing</th>\n",
       "      <th>A_home_win_ratio</th>\n",
       "      <th>A_neutral_missing</th>\n",
       "      <th>A_neutral_win_ratio</th>\n",
       "      <th>A_num_games</th>\n",
       "      <th>A_recent_avg_Ast_against</th>\n",
       "      <th>A_recent_avg_Ast_for</th>\n",
       "      <th>A_recent_avg_Blk_against</th>\n",
       "      <th>A_recent_avg_Blk_for</th>\n",
       "      <th>A_recent_avg_DR_against</th>\n",
       "      <th>A_recent_avg_DR_for</th>\n",
       "      <th>A_recent_avg_FGPct2_against</th>\n",
       "      <th>A_recent_avg_FGPct2_for</th>\n",
       "      <th>A_recent_avg_FGPct3_against</th>\n",
       "      <th>A_recent_avg_FGPct3_for</th>\n",
       "      <th>A_recent_avg_FTPct_against</th>\n",
       "      <th>A_recent_avg_FTPct_for</th>\n",
       "      <th>A_recent_avg_OR_against</th>\n",
       "      <th>A_recent_avg_OR_for</th>\n",
       "      <th>A_recent_avg_PF_against</th>\n",
       "      <th>A_recent_avg_PF_for</th>\n",
       "      <th>A_recent_avg_Score_against</th>\n",
       "      <th>A_recent_avg_Score_for</th>\n",
       "      <th>A_recent_avg_Stl_against</th>\n",
       "      <th>A_recent_avg_Stl_for</th>\n",
       "      <th>A_recent_avg_TO_against</th>\n",
       "      <th>A_recent_avg_TO_for</th>\n",
       "      <th>A_recent_avg_diff</th>\n",
       "      <th>A_recent_std_Ast_against</th>\n",
       "      <th>A_recent_std_Ast_for</th>\n",
       "      <th>A_recent_std_Blk_against</th>\n",
       "      <th>A_recent_std_Blk_for</th>\n",
       "      <th>A_recent_std_DR_against</th>\n",
       "      <th>A_recent_std_DR_for</th>\n",
       "      <th>A_recent_std_FGPct2_against</th>\n",
       "      <th>A_recent_std_FGPct2_for</th>\n",
       "      <th>A_recent_std_FGPct3_against</th>\n",
       "      <th>A_recent_std_FGPct3_for</th>\n",
       "      <th>A_recent_std_FTPct_against</th>\n",
       "      <th>A_recent_std_FTPct_for</th>\n",
       "      <th>A_recent_std_OR_against</th>\n",
       "      <th>A_recent_std_OR_for</th>\n",
       "      <th>A_recent_std_PF_against</th>\n",
       "      <th>A_recent_std_PF_for</th>\n",
       "      <th>A_recent_std_Score_against</th>\n",
       "      <th>A_recent_std_Score_for</th>\n",
       "      <th>A_recent_std_Stl_against</th>\n",
       "      <th>A_recent_std_Stl_for</th>\n",
       "      <th>A_recent_std_TO_against</th>\n",
       "      <th>A_recent_std_TO_for</th>\n",
       "      <th>A_recent_win_ratio</th>\n",
       "      <th>A_std_Ast_against</th>\n",
       "      <th>A_std_Ast_for</th>\n",
       "      <th>A_std_Blk_against</th>\n",
       "      <th>A_std_Blk_for</th>\n",
       "      <th>A_std_DR_against</th>\n",
       "      <th>A_std_DR_for</th>\n",
       "      <th>A_std_FGPct2_against</th>\n",
       "      <th>A_std_FGPct2_for</th>\n",
       "      <th>A_std_FGPct3_against</th>\n",
       "      <th>A_std_FGPct3_for</th>\n",
       "      <th>A_std_FTPct_against</th>\n",
       "      <th>A_std_FTPct_for</th>\n",
       "      <th>A_std_OR_against</th>\n",
       "      <th>A_std_OR_for</th>\n",
       "      <th>A_std_PF_against</th>\n",
       "      <th>A_std_PF_for</th>\n",
       "      <th>A_std_Score_against</th>\n",
       "      <th>A_std_Score_for</th>\n",
       "      <th>A_std_Stl_against</th>\n",
       "      <th>A_std_Stl_for</th>\n",
       "      <th>A_std_TO_against</th>\n",
       "      <th>A_std_TO_for</th>\n",
       "      <th>A_std_diff</th>\n",
       "      <th>A_win_ratio</th>\n",
       "      <th>B_PlayIn</th>\n",
       "      <th>B_Score</th>\n",
       "      <th>B_SeedNum</th>\n",
       "      <th>B_TeamID</th>\n",
       "      <th>B_avg_Ast_against</th>\n",
       "      <th>B_avg_Ast_for</th>\n",
       "      <th>B_avg_Blk_against</th>\n",
       "      <th>B_avg_Blk_for</th>\n",
       "      <th>B_avg_DR_against</th>\n",
       "      <th>B_avg_DR_for</th>\n",
       "      <th>B_avg_FGPct2_against</th>\n",
       "      <th>B_avg_FGPct2_for</th>\n",
       "      <th>B_avg_FGPct3_against</th>\n",
       "      <th>B_avg_FGPct3_for</th>\n",
       "      <th>B_avg_FTPct_against</th>\n",
       "      <th>B_avg_FTPct_for</th>\n",
       "      <th>B_avg_OR_against</th>\n",
       "      <th>B_avg_OR_for</th>\n",
       "      <th>B_avg_PF_against</th>\n",
       "      <th>B_avg_PF_for</th>\n",
       "      <th>B_avg_Score_against</th>\n",
       "      <th>B_avg_Score_for</th>\n",
       "      <th>B_avg_Stl_against</th>\n",
       "      <th>B_avg_Stl_for</th>\n",
       "      <th>B_avg_TO_against</th>\n",
       "      <th>B_avg_TO_for</th>\n",
       "      <th>B_avg_diff</th>\n",
       "      <th>B_away_missing</th>\n",
       "      <th>B_away_win_ratio</th>\n",
       "      <th>B_close_game_missing</th>\n",
       "      <th>B_close_game_ratio</th>\n",
       "      <th>B_close_win_ratio</th>\n",
       "      <th>B_conf_champs</th>\n",
       "      <th>B_home_missing</th>\n",
       "      <th>B_home_win_ratio</th>\n",
       "      <th>B_neutral_missing</th>\n",
       "      <th>B_neutral_win_ratio</th>\n",
       "      <th>B_num_games</th>\n",
       "      <th>B_recent_avg_Ast_against</th>\n",
       "      <th>B_recent_avg_Ast_for</th>\n",
       "      <th>B_recent_avg_Blk_against</th>\n",
       "      <th>B_recent_avg_Blk_for</th>\n",
       "      <th>B_recent_avg_DR_against</th>\n",
       "      <th>B_recent_avg_DR_for</th>\n",
       "      <th>B_recent_avg_FGPct2_against</th>\n",
       "      <th>B_recent_avg_FGPct2_for</th>\n",
       "      <th>B_recent_avg_FGPct3_against</th>\n",
       "      <th>B_recent_avg_FGPct3_for</th>\n",
       "      <th>B_recent_avg_FTPct_against</th>\n",
       "      <th>B_recent_avg_FTPct_for</th>\n",
       "      <th>B_recent_avg_OR_against</th>\n",
       "      <th>B_recent_avg_OR_for</th>\n",
       "      <th>B_recent_avg_PF_against</th>\n",
       "      <th>B_recent_avg_PF_for</th>\n",
       "      <th>B_recent_avg_Score_against</th>\n",
       "      <th>B_recent_avg_Score_for</th>\n",
       "      <th>B_recent_avg_Stl_against</th>\n",
       "      <th>B_recent_avg_Stl_for</th>\n",
       "      <th>B_recent_avg_TO_against</th>\n",
       "      <th>B_recent_avg_TO_for</th>\n",
       "      <th>B_recent_avg_diff</th>\n",
       "      <th>B_recent_std_Ast_against</th>\n",
       "      <th>B_recent_std_Ast_for</th>\n",
       "      <th>B_recent_std_Blk_against</th>\n",
       "      <th>B_recent_std_Blk_for</th>\n",
       "      <th>B_recent_std_DR_against</th>\n",
       "      <th>B_recent_std_DR_for</th>\n",
       "      <th>B_recent_std_FGPct2_against</th>\n",
       "      <th>B_recent_std_FGPct2_for</th>\n",
       "      <th>B_recent_std_FGPct3_against</th>\n",
       "      <th>B_recent_std_FGPct3_for</th>\n",
       "      <th>B_recent_std_FTPct_against</th>\n",
       "      <th>B_recent_std_FTPct_for</th>\n",
       "      <th>B_recent_std_OR_against</th>\n",
       "      <th>B_recent_std_OR_for</th>\n",
       "      <th>B_recent_std_PF_against</th>\n",
       "      <th>B_recent_std_PF_for</th>\n",
       "      <th>B_recent_std_Score_against</th>\n",
       "      <th>B_recent_std_Score_for</th>\n",
       "      <th>B_recent_std_Stl_against</th>\n",
       "      <th>B_recent_std_Stl_for</th>\n",
       "      <th>B_recent_std_TO_against</th>\n",
       "      <th>B_recent_std_TO_for</th>\n",
       "      <th>B_recent_win_ratio</th>\n",
       "      <th>B_std_Ast_against</th>\n",
       "      <th>B_std_Ast_for</th>\n",
       "      <th>B_std_Blk_against</th>\n",
       "      <th>B_std_Blk_for</th>\n",
       "      <th>B_std_DR_against</th>\n",
       "      <th>B_std_DR_for</th>\n",
       "      <th>B_std_FGPct2_against</th>\n",
       "      <th>B_std_FGPct2_for</th>\n",
       "      <th>B_std_FGPct3_against</th>\n",
       "      <th>B_std_FGPct3_for</th>\n",
       "      <th>B_std_FTPct_against</th>\n",
       "      <th>B_std_FTPct_for</th>\n",
       "      <th>B_std_OR_against</th>\n",
       "      <th>B_std_OR_for</th>\n",
       "      <th>B_std_PF_against</th>\n",
       "      <th>B_std_PF_for</th>\n",
       "      <th>B_std_Score_against</th>\n",
       "      <th>B_std_Score_for</th>\n",
       "      <th>B_std_Stl_against</th>\n",
       "      <th>B_std_Stl_for</th>\n",
       "      <th>B_std_TO_against</th>\n",
       "      <th>B_std_TO_for</th>\n",
       "      <th>B_std_diff</th>\n",
       "      <th>B_win_ratio</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>Season</th>\n",
       "      <th>round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1112</td>\n",
       "      <td>15.464286</td>\n",
       "      <td>17.642857</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>23.285714</td>\n",
       "      <td>27.642857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>13.107143</td>\n",
       "      <td>15.178571</td>\n",
       "      <td>22.071429</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>85.214286</td>\n",
       "      <td>5.964286</td>\n",
       "      <td>8.464286</td>\n",
       "      <td>14.785714</td>\n",
       "      <td>16.857143</td>\n",
       "      <td>14.964286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28</td>\n",
       "      <td>14.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>4.233511</td>\n",
       "      <td>1.463850</td>\n",
       "      <td>2.017778</td>\n",
       "      <td>3.365175</td>\n",
       "      <td>4.076282</td>\n",
       "      <td>0.068456</td>\n",
       "      <td>0.069324</td>\n",
       "      <td>0.075495</td>\n",
       "      <td>0.098794</td>\n",
       "      <td>0.152864</td>\n",
       "      <td>0.106410</td>\n",
       "      <td>4.064949</td>\n",
       "      <td>3.362078</td>\n",
       "      <td>4.435718</td>\n",
       "      <td>2.905250</td>\n",
       "      <td>8.945104</td>\n",
       "      <td>10.222087</td>\n",
       "      <td>3.067204</td>\n",
       "      <td>3.730505</td>\n",
       "      <td>4.527693</td>\n",
       "      <td>5.163690</td>\n",
       "      <td>12.522491</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>1436</td>\n",
       "      <td>13.275862</td>\n",
       "      <td>14.206897</td>\n",
       "      <td>3.655172</td>\n",
       "      <td>2.965517</td>\n",
       "      <td>21.862069</td>\n",
       "      <td>25.724138</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>9.586207</td>\n",
       "      <td>12.965517</td>\n",
       "      <td>17.931034</td>\n",
       "      <td>15.896552</td>\n",
       "      <td>63.137931</td>\n",
       "      <td>67.793103</td>\n",
       "      <td>7.103448</td>\n",
       "      <td>6.862069</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.655172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>29</td>\n",
       "      <td>9.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>53.0</td>\n",
       "      <td>61.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.033566</td>\n",
       "      <td>0.115476</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.017081</td>\n",
       "      <td>0.134647</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.760553</td>\n",
       "      <td>4.446791</td>\n",
       "      <td>2.668821</td>\n",
       "      <td>1.736469</td>\n",
       "      <td>5.948373</td>\n",
       "      <td>3.196622</td>\n",
       "      <td>0.081048</td>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.102801</td>\n",
       "      <td>0.117352</td>\n",
       "      <td>0.145330</td>\n",
       "      <td>0.110197</td>\n",
       "      <td>3.715969</td>\n",
       "      <td>3.598105</td>\n",
       "      <td>4.089516</td>\n",
       "      <td>4.026731</td>\n",
       "      <td>9.368980</td>\n",
       "      <td>10.750924</td>\n",
       "      <td>2.848674</td>\n",
       "      <td>3.546084</td>\n",
       "      <td>3.456905</td>\n",
       "      <td>4.303612</td>\n",
       "      <td>12.530245</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>7</td>\n",
       "      <td>1272</td>\n",
       "      <td>13.310345</td>\n",
       "      <td>16.620690</td>\n",
       "      <td>3.172414</td>\n",
       "      <td>5.068966</td>\n",
       "      <td>23.586207</td>\n",
       "      <td>25.965517</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>12.344828</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>19.931034</td>\n",
       "      <td>18.758621</td>\n",
       "      <td>65.827586</td>\n",
       "      <td>74.517241</td>\n",
       "      <td>7.275862</td>\n",
       "      <td>7.379310</td>\n",
       "      <td>13.793103</td>\n",
       "      <td>15.068966</td>\n",
       "      <td>8.689655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>24.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.838826</td>\n",
       "      <td>4.400484</td>\n",
       "      <td>2.248650</td>\n",
       "      <td>3.231506</td>\n",
       "      <td>3.778590</td>\n",
       "      <td>3.666676</td>\n",
       "      <td>0.073709</td>\n",
       "      <td>0.098708</td>\n",
       "      <td>0.100276</td>\n",
       "      <td>0.125127</td>\n",
       "      <td>0.125517</td>\n",
       "      <td>0.154544</td>\n",
       "      <td>4.189029</td>\n",
       "      <td>5.090097</td>\n",
       "      <td>6.079979</td>\n",
       "      <td>4.067456</td>\n",
       "      <td>8.783927</td>\n",
       "      <td>8.793165</td>\n",
       "      <td>2.854024</td>\n",
       "      <td>3.181648</td>\n",
       "      <td>4.018531</td>\n",
       "      <td>4.750780</td>\n",
       "      <td>10.953570</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>1113</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>15.551724</td>\n",
       "      <td>3.931034</td>\n",
       "      <td>4.241379</td>\n",
       "      <td>20.551724</td>\n",
       "      <td>23.310345</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.689655</td>\n",
       "      <td>22.862069</td>\n",
       "      <td>19.413793</td>\n",
       "      <td>69.172414</td>\n",
       "      <td>75.965517</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.206897</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>15.517241</td>\n",
       "      <td>6.793103</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>29</td>\n",
       "      <td>14.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.5</td>\n",
       "      <td>73.5</td>\n",
       "      <td>78.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.299825</td>\n",
       "      <td>3.490526</td>\n",
       "      <td>2.166685</td>\n",
       "      <td>2.609236</td>\n",
       "      <td>4.345663</td>\n",
       "      <td>3.855293</td>\n",
       "      <td>0.076055</td>\n",
       "      <td>0.079974</td>\n",
       "      <td>0.122928</td>\n",
       "      <td>0.127522</td>\n",
       "      <td>0.123120</td>\n",
       "      <td>0.118812</td>\n",
       "      <td>3.869106</td>\n",
       "      <td>4.950465</td>\n",
       "      <td>4.978860</td>\n",
       "      <td>3.058457</td>\n",
       "      <td>10.385999</td>\n",
       "      <td>10.407596</td>\n",
       "      <td>2.365746</td>\n",
       "      <td>2.064380</td>\n",
       "      <td>3.459004</td>\n",
       "      <td>2.994857</td>\n",
       "      <td>17.095164</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>1166</td>\n",
       "      <td>12.363636</td>\n",
       "      <td>16.818182</td>\n",
       "      <td>2.575758</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>21.363636</td>\n",
       "      <td>23.181818</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>11.060606</td>\n",
       "      <td>10.878788</td>\n",
       "      <td>19.393939</td>\n",
       "      <td>17.272727</td>\n",
       "      <td>64.333333</td>\n",
       "      <td>79.242424</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>8.393939</td>\n",
       "      <td>13.363636</td>\n",
       "      <td>17.060606</td>\n",
       "      <td>14.909091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.079917</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.052036</td>\n",
       "      <td>0.173295</td>\n",
       "      <td>0.064912</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.239145</td>\n",
       "      <td>4.677785</td>\n",
       "      <td>2.337967</td>\n",
       "      <td>2.129806</td>\n",
       "      <td>4.377316</td>\n",
       "      <td>3.083962</td>\n",
       "      <td>0.077470</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>0.143098</td>\n",
       "      <td>0.102340</td>\n",
       "      <td>0.119023</td>\n",
       "      <td>0.093757</td>\n",
       "      <td>3.807981</td>\n",
       "      <td>2.766268</td>\n",
       "      <td>3.295894</td>\n",
       "      <td>3.119320</td>\n",
       "      <td>9.280190</td>\n",
       "      <td>11.670098</td>\n",
       "      <td>2.261255</td>\n",
       "      <td>2.717322</td>\n",
       "      <td>3.364572</td>\n",
       "      <td>4.477215</td>\n",
       "      <td>14.876080</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>11</td>\n",
       "      <td>1141</td>\n",
       "      <td>11.793103</td>\n",
       "      <td>15.620690</td>\n",
       "      <td>2.482759</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>18.896552</td>\n",
       "      <td>23.275862</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>12.241379</td>\n",
       "      <td>10.586207</td>\n",
       "      <td>21.689655</td>\n",
       "      <td>20.965517</td>\n",
       "      <td>73.241379</td>\n",
       "      <td>79.344828</td>\n",
       "      <td>8.448276</td>\n",
       "      <td>7.103448</td>\n",
       "      <td>18.241379</td>\n",
       "      <td>16.068966</td>\n",
       "      <td>6.103448</td>\n",
       "      <td>0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>85.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.053419</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.034615</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.226069</td>\n",
       "      <td>4.475359</td>\n",
       "      <td>1.976508</td>\n",
       "      <td>2.337241</td>\n",
       "      <td>4.047291</td>\n",
       "      <td>4.229913</td>\n",
       "      <td>0.089039</td>\n",
       "      <td>0.088209</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.121457</td>\n",
       "      <td>0.092883</td>\n",
       "      <td>0.081641</td>\n",
       "      <td>4.440128</td>\n",
       "      <td>3.533716</td>\n",
       "      <td>3.832463</td>\n",
       "      <td>4.483687</td>\n",
       "      <td>10.130156</td>\n",
       "      <td>10.563408</td>\n",
       "      <td>2.924655</td>\n",
       "      <td>2.955999</td>\n",
       "      <td>3.274294</td>\n",
       "      <td>4.503360</td>\n",
       "      <td>13.216876</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>8</td>\n",
       "      <td>1143</td>\n",
       "      <td>16.034483</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.517241</td>\n",
       "      <td>2.793103</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>24.379310</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>11.172414</td>\n",
       "      <td>11.241379</td>\n",
       "      <td>19.103448</td>\n",
       "      <td>17.103448</td>\n",
       "      <td>69.758621</td>\n",
       "      <td>74.482759</td>\n",
       "      <td>5.931034</td>\n",
       "      <td>6.551724</td>\n",
       "      <td>14.172414</td>\n",
       "      <td>14.931034</td>\n",
       "      <td>4.724138</td>\n",
       "      <td>0</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>29</td>\n",
       "      <td>11.5</td>\n",
       "      <td>14.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>62.5</td>\n",
       "      <td>65.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.232227</td>\n",
       "      <td>4.085418</td>\n",
       "      <td>1.534497</td>\n",
       "      <td>1.687830</td>\n",
       "      <td>3.605346</td>\n",
       "      <td>4.986534</td>\n",
       "      <td>0.089574</td>\n",
       "      <td>0.066102</td>\n",
       "      <td>0.082790</td>\n",
       "      <td>0.116170</td>\n",
       "      <td>0.132386</td>\n",
       "      <td>0.144146</td>\n",
       "      <td>4.921752</td>\n",
       "      <td>3.753816</td>\n",
       "      <td>3.777298</td>\n",
       "      <td>3.519124</td>\n",
       "      <td>8.129585</td>\n",
       "      <td>7.410284</td>\n",
       "      <td>3.420454</td>\n",
       "      <td>2.634837</td>\n",
       "      <td>4.512981</td>\n",
       "      <td>4.008919</td>\n",
       "      <td>10.577059</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>9</td>\n",
       "      <td>1301</td>\n",
       "      <td>12.566667</td>\n",
       "      <td>14.666667</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>21.433333</td>\n",
       "      <td>22.033333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>10.533333</td>\n",
       "      <td>9.733333</td>\n",
       "      <td>19.333333</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>72.400000</td>\n",
       "      <td>7.433333</td>\n",
       "      <td>7.766667</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.633333</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>30</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>23.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>24.5</td>\n",
       "      <td>83.5</td>\n",
       "      <td>82.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.178106</td>\n",
       "      <td>4.392345</td>\n",
       "      <td>1.627033</td>\n",
       "      <td>2.455885</td>\n",
       "      <td>4.164167</td>\n",
       "      <td>4.038474</td>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.102842</td>\n",
       "      <td>0.133451</td>\n",
       "      <td>0.094883</td>\n",
       "      <td>0.116456</td>\n",
       "      <td>0.097546</td>\n",
       "      <td>4.403653</td>\n",
       "      <td>3.778445</td>\n",
       "      <td>4.291807</td>\n",
       "      <td>3.699352</td>\n",
       "      <td>9.610211</td>\n",
       "      <td>10.864126</td>\n",
       "      <td>2.476665</td>\n",
       "      <td>3.555092</td>\n",
       "      <td>3.768289</td>\n",
       "      <td>4.663107</td>\n",
       "      <td>16.106577</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>1140</td>\n",
       "      <td>10.774194</td>\n",
       "      <td>13.419355</td>\n",
       "      <td>2.225806</td>\n",
       "      <td>2.516129</td>\n",
       "      <td>20.193548</td>\n",
       "      <td>24.419355</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>10.419355</td>\n",
       "      <td>10.870968</td>\n",
       "      <td>21.387097</td>\n",
       "      <td>21.419355</td>\n",
       "      <td>63.677419</td>\n",
       "      <td>72.451613</td>\n",
       "      <td>5.483871</td>\n",
       "      <td>6.935484</td>\n",
       "      <td>13.741935</td>\n",
       "      <td>14.483871</td>\n",
       "      <td>8.774194</td>\n",
       "      <td>0</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>31</td>\n",
       "      <td>18.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>71.0</td>\n",
       "      <td>75.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.285277</td>\n",
       "      <td>4.050684</td>\n",
       "      <td>1.641778</td>\n",
       "      <td>1.685220</td>\n",
       "      <td>2.836636</td>\n",
       "      <td>4.574295</td>\n",
       "      <td>0.071911</td>\n",
       "      <td>0.082135</td>\n",
       "      <td>0.118374</td>\n",
       "      <td>0.134514</td>\n",
       "      <td>0.111363</td>\n",
       "      <td>0.083710</td>\n",
       "      <td>2.887646</td>\n",
       "      <td>3.656719</td>\n",
       "      <td>3.595603</td>\n",
       "      <td>3.365527</td>\n",
       "      <td>9.792998</td>\n",
       "      <td>9.726895</td>\n",
       "      <td>2.421538</td>\n",
       "      <td>2.689488</td>\n",
       "      <td>3.984154</td>\n",
       "      <td>4.073065</td>\n",
       "      <td>12.752390</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>1163</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>15.633333</td>\n",
       "      <td>4.133333</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>14.766667</td>\n",
       "      <td>19.266667</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>71.400000</td>\n",
       "      <td>80.033333</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>5.933333</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>13.366667</td>\n",
       "      <td>8.633333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>30</td>\n",
       "      <td>12.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>16.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.197221</td>\n",
       "      <td>5.453745</td>\n",
       "      <td>2.890213</td>\n",
       "      <td>3.407100</td>\n",
       "      <td>4.295928</td>\n",
       "      <td>4.454960</td>\n",
       "      <td>0.077361</td>\n",
       "      <td>0.104143</td>\n",
       "      <td>0.097358</td>\n",
       "      <td>0.117574</td>\n",
       "      <td>0.120166</td>\n",
       "      <td>0.112417</td>\n",
       "      <td>5.058491</td>\n",
       "      <td>4.419087</td>\n",
       "      <td>4.543310</td>\n",
       "      <td>3.465785</td>\n",
       "      <td>9.258780</td>\n",
       "      <td>12.145095</td>\n",
       "      <td>3.478505</td>\n",
       "      <td>2.516280</td>\n",
       "      <td>3.670150</td>\n",
       "      <td>3.510935</td>\n",
       "      <td>16.742518</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_PlayIn  A_Score  A_SeedNum  A_TeamID  A_avg_Ast_against  A_avg_Ast_for  \\\n",
       "0         0       80          1      1112          15.464286      17.642857   \n",
       "1         0       71          7      1272          13.310345      16.620690   \n",
       "2         0       73          6      1166          12.363636      16.818182   \n",
       "3         0       76          8      1143          16.034483      16.000000   \n",
       "4         0       53         12      1140          10.774194      13.419355   \n",
       "\n",
       "   A_avg_Blk_against  A_avg_Blk_for  A_avg_DR_against  A_avg_DR_for  \\\n",
       "0           2.392857       4.214286         23.285714     27.642857   \n",
       "1           3.172414       5.068966         23.586207     25.965517   \n",
       "2           2.575758       4.454545         21.363636     23.181818   \n",
       "3           2.517241       2.793103         23.000000     24.379310   \n",
       "4           2.225806       2.516129         20.193548     24.419355   \n",
       "\n",
       "   A_avg_FGPct2_against  A_avg_FGPct2_for  A_avg_FGPct3_against  \\\n",
       "0              0.428571          0.464286              0.285714   \n",
       "1              0.379310          0.448276              0.310345   \n",
       "2              0.454545          0.515152              0.303030   \n",
       "3              0.482759          0.448276              0.310345   \n",
       "4              0.419355          0.483871              0.258065   \n",
       "\n",
       "   A_avg_FGPct3_for  A_avg_FTPct_against  A_avg_FTPct_for  A_avg_OR_against  \\\n",
       "0          0.321429             0.571429         0.678571         13.107143   \n",
       "1          0.310345             0.620690         0.586207         12.344828   \n",
       "2          0.363636             0.666667         0.636364         11.060606   \n",
       "3          0.344828             0.620690         0.655172         11.172414   \n",
       "4          0.354839             0.645161         0.709677         10.419355   \n",
       "\n",
       "   A_avg_OR_for  A_avg_PF_against  A_avg_PF_for  A_avg_Score_against  \\\n",
       "0     15.178571         22.071429     17.750000            70.250000   \n",
       "1     14.068966         19.931034     18.758621            65.827586   \n",
       "2     10.878788         19.393939     17.272727            64.333333   \n",
       "3     11.241379         19.103448     17.103448            69.758621   \n",
       "4     10.870968         21.387097     21.419355            63.677419   \n",
       "\n",
       "   A_avg_Score_for  A_avg_Stl_against  A_avg_Stl_for  A_avg_TO_against  \\\n",
       "0        85.214286           5.964286       8.464286         14.785714   \n",
       "1        74.517241           7.275862       7.379310         13.793103   \n",
       "2        79.242424           6.333333       8.393939         13.363636   \n",
       "3        74.482759           5.931034       6.551724         14.172414   \n",
       "4        72.451613           5.483871       6.935484         13.741935   \n",
       "\n",
       "   A_avg_TO_for  A_avg_diff  A_away_missing  A_away_win_ratio  \\\n",
       "0     16.857143   14.964286               0          0.916667   \n",
       "1     15.068966    8.689655               0          0.636364   \n",
       "2     17.060606   14.909091               0          0.636364   \n",
       "3     14.931034    4.724138               0          0.636364   \n",
       "4     14.483871    8.774194               0          0.545455   \n",
       "\n",
       "   A_close_game_missing  A_close_game_ratio  A_close_win_ratio  A_conf_champs  \\\n",
       "0                     0            0.178571           0.600000            0.0   \n",
       "1                     0            0.241379           0.285714            0.0   \n",
       "2                     0            0.212121           0.571429            1.0   \n",
       "3                     0            0.344828           0.200000            0.0   \n",
       "4                     0            0.290323           0.111111            0.0   \n",
       "\n",
       "   A_home_missing  A_home_win_ratio  A_neutral_missing  A_neutral_win_ratio  \\\n",
       "0               0          0.933333                  0             0.000000   \n",
       "1               0          0.875000                  0             1.000000   \n",
       "2               0          1.000000                  0             1.000000   \n",
       "3               0          0.928571                  0             0.250000   \n",
       "4               0          0.928571                  0             0.666667   \n",
       "\n",
       "   A_num_games  A_recent_avg_Ast_against  A_recent_avg_Ast_for  \\\n",
       "0           28                      14.5                  18.5   \n",
       "1           29                      10.5                  11.0   \n",
       "2           33                      13.5                  17.0   \n",
       "3           29                      11.5                  14.5   \n",
       "4           31                      18.5                  12.5   \n",
       "\n",
       "   A_recent_avg_Blk_against  A_recent_avg_Blk_for  A_recent_avg_DR_against  \\\n",
       "0                       3.5                   3.5                     29.5   \n",
       "1                       2.0                   4.5                     24.5   \n",
       "2                       3.0                   6.0                     19.5   \n",
       "3                       4.0                   4.5                     23.5   \n",
       "4                       3.0                   4.0                     22.0   \n",
       "\n",
       "   A_recent_avg_DR_for  A_recent_avg_FGPct2_against  A_recent_avg_FGPct2_for  \\\n",
       "0                 23.5                          0.0                      0.0   \n",
       "1                 25.0                          0.0                      0.0   \n",
       "2                 26.5                          0.5                      0.5   \n",
       "3                 30.0                          0.0                      0.0   \n",
       "4                 23.0                          0.0                      0.0   \n",
       "\n",
       "   A_recent_avg_FGPct3_against  A_recent_avg_FGPct3_for  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          0.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "\n",
       "   A_recent_avg_FTPct_against  A_recent_avg_FTPct_for  \\\n",
       "0                         0.0                     0.0   \n",
       "1                         0.0                     0.0   \n",
       "2                         0.5                     0.5   \n",
       "3                         0.0                     0.0   \n",
       "4                         0.0                     0.0   \n",
       "\n",
       "   A_recent_avg_OR_against  A_recent_avg_OR_for  A_recent_avg_PF_against  \\\n",
       "0                     12.5                 17.5                     24.0   \n",
       "1                     16.5                 15.0                     24.5   \n",
       "2                     10.0                 11.0                     17.5   \n",
       "3                     16.5                 15.5                     17.0   \n",
       "4                      8.0                 11.0                     22.0   \n",
       "\n",
       "   A_recent_avg_PF_for  A_recent_avg_Score_against  A_recent_avg_Score_for  \\\n",
       "0                 20.0                        88.0                    88.5   \n",
       "1                 26.5                        67.0                    68.5   \n",
       "2                 18.0                        62.5                    75.0   \n",
       "3                 15.5                        62.5                    65.5   \n",
       "4                 21.5                        71.0                    75.5   \n",
       "\n",
       "   A_recent_avg_Stl_against  A_recent_avg_Stl_for  A_recent_avg_TO_against  \\\n",
       "0                       6.5                  11.5                     12.0   \n",
       "1                       7.5                   3.0                     13.0   \n",
       "2                       6.5                   5.5                     14.5   \n",
       "3                       9.0                   3.5                     15.0   \n",
       "4                       1.5                   8.0                      8.0   \n",
       "\n",
       "   A_recent_avg_TO_for  A_recent_avg_diff  A_recent_std_Ast_against  \\\n",
       "0                 18.0                0.5                       0.0   \n",
       "1                 10.5                1.5                       0.0   \n",
       "2                 13.0               12.5                       0.5   \n",
       "3                  8.5                3.0                       0.0   \n",
       "4                 13.5                4.5                       0.0   \n",
       "\n",
       "   A_recent_std_Ast_for  A_recent_std_Blk_against  A_recent_std_Blk_for  \\\n",
       "0                   0.0                       0.0                   0.0   \n",
       "1                   0.0                       0.0                   0.0   \n",
       "2                   1.0                       3.0                   1.0   \n",
       "3                   0.0                       0.0                   0.0   \n",
       "4                   0.0                       0.0                   0.0   \n",
       "\n",
       "   A_recent_std_DR_against  A_recent_std_DR_for  A_recent_std_FGPct2_against  \\\n",
       "0                      0.0                  0.0                     0.000000   \n",
       "1                      0.0                  0.0                     0.000000   \n",
       "2                      1.5                  3.5                     0.079917   \n",
       "3                      0.0                  0.0                     0.000000   \n",
       "4                      0.0                  0.0                     0.000000   \n",
       "\n",
       "   A_recent_std_FGPct2_for  A_recent_std_FGPct3_against  \\\n",
       "0                 0.000000                     0.000000   \n",
       "1                 0.000000                     0.000000   \n",
       "2                 0.022528                     0.107843   \n",
       "3                 0.000000                     0.000000   \n",
       "4                 0.000000                     0.000000   \n",
       "\n",
       "   A_recent_std_FGPct3_for  A_recent_std_FTPct_against  \\\n",
       "0                 0.000000                    0.000000   \n",
       "1                 0.000000                    0.000000   \n",
       "2                 0.052036                    0.173295   \n",
       "3                 0.000000                    0.000000   \n",
       "4                 0.000000                    0.000000   \n",
       "\n",
       "   A_recent_std_FTPct_for  A_recent_std_OR_against  A_recent_std_OR_for  \\\n",
       "0                0.000000                      0.0                  0.0   \n",
       "1                0.000000                      0.0                  0.0   \n",
       "2                0.064912                      3.0                  2.0   \n",
       "3                0.000000                      0.0                  0.0   \n",
       "4                0.000000                      0.0                  0.0   \n",
       "\n",
       "   A_recent_std_PF_against  A_recent_std_PF_for  A_recent_std_Score_against  \\\n",
       "0                      0.0                  0.0                         0.0   \n",
       "1                      0.0                  0.0                         0.0   \n",
       "2                      1.5                  2.0                         6.5   \n",
       "3                      0.0                  0.0                         0.0   \n",
       "4                      0.0                  0.0                         0.0   \n",
       "\n",
       "   A_recent_std_Score_for  A_recent_std_Stl_against  A_recent_std_Stl_for  \\\n",
       "0                     0.0                       0.0                   0.0   \n",
       "1                     0.0                       0.0                   0.0   \n",
       "2                     5.0                       1.5                   0.5   \n",
       "3                     0.0                       0.0                   0.0   \n",
       "4                     0.0                       0.0                   0.0   \n",
       "\n",
       "   A_recent_std_TO_against  A_recent_std_TO_for  A_recent_win_ratio  \\\n",
       "0                      0.0                  0.0                 0.5   \n",
       "1                      0.0                  0.0                 0.5   \n",
       "2                      2.5                  1.0                 1.0   \n",
       "3                      0.0                  0.0                 0.5   \n",
       "4                      0.0                  0.0                 0.5   \n",
       "\n",
       "   A_std_Ast_against  A_std_Ast_for  A_std_Blk_against  A_std_Blk_for  \\\n",
       "0           3.250000       4.233511           1.463850       2.017778   \n",
       "1           2.838826       4.400484           2.248650       3.231506   \n",
       "2           3.239145       4.677785           2.337967       2.129806   \n",
       "3           3.232227       4.085418           1.534497       1.687830   \n",
       "4           3.285277       4.050684           1.641778       1.685220   \n",
       "\n",
       "   A_std_DR_against  A_std_DR_for  A_std_FGPct2_against  A_std_FGPct2_for  \\\n",
       "0          3.365175      4.076282              0.068456          0.069324   \n",
       "1          3.778590      3.666676              0.073709          0.098708   \n",
       "2          4.377316      3.083962              0.077470          0.078507   \n",
       "3          3.605346      4.986534              0.089574          0.066102   \n",
       "4          2.836636      4.574295              0.071911          0.082135   \n",
       "\n",
       "   A_std_FGPct3_against  A_std_FGPct3_for  A_std_FTPct_against  \\\n",
       "0              0.075495          0.098794             0.152864   \n",
       "1              0.100276          0.125127             0.125517   \n",
       "2              0.143098          0.102340             0.119023   \n",
       "3              0.082790          0.116170             0.132386   \n",
       "4              0.118374          0.134514             0.111363   \n",
       "\n",
       "   A_std_FTPct_for  A_std_OR_against  A_std_OR_for  A_std_PF_against  \\\n",
       "0         0.106410          4.064949      3.362078          4.435718   \n",
       "1         0.154544          4.189029      5.090097          6.079979   \n",
       "2         0.093757          3.807981      2.766268          3.295894   \n",
       "3         0.144146          4.921752      3.753816          3.777298   \n",
       "4         0.083710          2.887646      3.656719          3.595603   \n",
       "\n",
       "   A_std_PF_for  A_std_Score_against  A_std_Score_for  A_std_Stl_against  \\\n",
       "0      2.905250             8.945104        10.222087           3.067204   \n",
       "1      4.067456             8.783927         8.793165           2.854024   \n",
       "2      3.119320             9.280190        11.670098           2.261255   \n",
       "3      3.519124             8.129585         7.410284           3.420454   \n",
       "4      3.365527             9.792998         9.726895           2.421538   \n",
       "\n",
       "   A_std_Stl_for  A_std_TO_against  A_std_TO_for  A_std_diff  A_win_ratio  \\\n",
       "0       3.730505          4.527693      5.163690   12.522491     0.892857   \n",
       "1       3.181648          4.018531      4.750780   10.953570     0.793103   \n",
       "2       2.717322          3.364572      4.477215   14.876080     0.878788   \n",
       "3       2.634837          4.512981      4.008919   10.577059     0.724138   \n",
       "4       2.689488          3.984154      4.073065   12.752390     0.741935   \n",
       "\n",
       "   B_PlayIn  B_Score  B_SeedNum  B_TeamID  B_avg_Ast_against  B_avg_Ast_for  \\\n",
       "0         0       51         16      1436          13.275862      14.206897   \n",
       "1         0       84         10      1113          14.068966      15.551724   \n",
       "2         0       79         11      1141          11.793103      15.620690   \n",
       "3         0       74          9      1301          12.566667      14.666667   \n",
       "4         0       58          5      1163          13.600000      15.633333   \n",
       "\n",
       "   B_avg_Blk_against  B_avg_Blk_for  B_avg_DR_against  B_avg_DR_for  \\\n",
       "0           3.655172       2.965517         21.862069     25.724138   \n",
       "1           3.931034       4.241379         20.551724     23.310345   \n",
       "2           2.482759       4.000000         18.896552     23.275862   \n",
       "3           2.833333       3.066667         21.433333     22.033333   \n",
       "4           4.133333       7.733333         21.500000     27.900000   \n",
       "\n",
       "   B_avg_FGPct2_against  B_avg_FGPct2_for  B_avg_FGPct3_against  \\\n",
       "0              0.448276          0.448276              0.310345   \n",
       "1              0.482759          0.482759              0.310345   \n",
       "2              0.448276          0.517241              0.344828   \n",
       "3              0.466667          0.500000              0.300000   \n",
       "4              0.400000          0.466667              0.300000   \n",
       "\n",
       "   B_avg_FGPct3_for  B_avg_FTPct_against  B_avg_FTPct_for  B_avg_OR_against  \\\n",
       "0          0.344828             0.655172         0.620690          9.586207   \n",
       "1          0.275862             0.655172         0.655172         11.000000   \n",
       "2          0.344828             0.655172         0.724138         12.241379   \n",
       "3          0.333333             0.666667         0.766667         10.533333   \n",
       "4          0.366667             0.666667         0.633333         15.333333   \n",
       "\n",
       "   B_avg_OR_for  B_avg_PF_against  B_avg_PF_for  B_avg_Score_against  \\\n",
       "0     12.965517         17.931034     15.896552            63.137931   \n",
       "1     13.689655         22.862069     19.413793            69.172414   \n",
       "2     10.586207         21.689655     20.965517            73.241379   \n",
       "3      9.733333         19.333333     18.666667            68.000000   \n",
       "4     14.766667         19.266667     18.400000            71.400000   \n",
       "\n",
       "   B_avg_Score_for  B_avg_Stl_against  B_avg_Stl_for  B_avg_TO_against  \\\n",
       "0        67.793103           7.103448       6.862069         14.068966   \n",
       "1        75.965517           6.000000       5.206897         14.000000   \n",
       "2        79.344828           8.448276       7.103448         18.241379   \n",
       "3        72.400000           7.433333       7.766667         14.200000   \n",
       "4        80.033333           7.800000       5.933333         15.800000   \n",
       "\n",
       "   B_avg_TO_for  B_avg_diff  B_away_missing  B_away_win_ratio  \\\n",
       "0     13.000000    4.655172               0          0.500000   \n",
       "1     15.517241    6.793103               0          0.400000   \n",
       "2     16.068966    6.103448               0          0.642857   \n",
       "3     14.633333    4.400000               0          0.300000   \n",
       "4     13.366667    8.633333               0          0.454545   \n",
       "\n",
       "   B_close_game_missing  B_close_game_ratio  B_close_win_ratio  B_conf_champs  \\\n",
       "0                     0            0.448276           0.230769            1.0   \n",
       "1                     0            0.379310           0.272727            0.0   \n",
       "2                     0            0.379310           0.545455            1.0   \n",
       "3                     0            0.466667           0.142857            0.0   \n",
       "4                     0            0.300000           0.111111            0.0   \n",
       "\n",
       "   B_home_missing  B_home_win_ratio  B_neutral_missing  B_neutral_win_ratio  \\\n",
       "0               0          0.900000                  0             0.600000   \n",
       "1               0          0.800000                  0             0.500000   \n",
       "2               0          0.916667                  0             1.000000   \n",
       "3               0          0.812500                  0             0.500000   \n",
       "4               0          0.875000                  0             0.666667   \n",
       "\n",
       "   B_num_games  B_recent_avg_Ast_against  B_recent_avg_Ast_for  \\\n",
       "0           29                       9.5                  10.0   \n",
       "1           29                      14.5                  16.0   \n",
       "2           29                       9.0                  15.5   \n",
       "3           30                      12.0                  15.0   \n",
       "4           30                      12.5                  10.0   \n",
       "\n",
       "   B_recent_avg_Blk_against  B_recent_avg_Blk_for  B_recent_avg_DR_against  \\\n",
       "0                       3.0                   2.5                     19.5   \n",
       "1                       2.0                   1.5                     17.5   \n",
       "2                       2.5                   3.0                     14.5   \n",
       "3                       4.0                   2.5                     23.5   \n",
       "4                       7.5                   7.0                     20.0   \n",
       "\n",
       "   B_recent_avg_DR_for  B_recent_avg_FGPct2_against  B_recent_avg_FGPct2_for  \\\n",
       "0                 28.0                          0.0                      0.0   \n",
       "1                 23.5                          0.0                      0.0   \n",
       "2                 25.0                          0.0                      0.5   \n",
       "3                 25.5                          0.0                      0.0   \n",
       "4                 25.0                          0.0                      0.0   \n",
       "\n",
       "   B_recent_avg_FGPct3_against  B_recent_avg_FGPct3_for  \\\n",
       "0                          0.0                      0.0   \n",
       "1                          0.0                      0.0   \n",
       "2                          0.0                      0.5   \n",
       "3                          0.0                      0.0   \n",
       "4                          0.0                      0.0   \n",
       "\n",
       "   B_recent_avg_FTPct_against  B_recent_avg_FTPct_for  \\\n",
       "0                         0.5                     0.5   \n",
       "1                         0.0                     0.0   \n",
       "2                         0.5                     0.5   \n",
       "3                         0.0                     0.0   \n",
       "4                         0.0                     0.0   \n",
       "\n",
       "   B_recent_avg_OR_against  B_recent_avg_OR_for  B_recent_avg_PF_against  \\\n",
       "0                     11.5                  9.0                     19.0   \n",
       "1                      6.5                 14.5                     19.5   \n",
       "2                     13.0                  9.5                     25.5   \n",
       "3                     11.5                  9.0                     23.0   \n",
       "4                     22.5                 16.5                     16.5   \n",
       "\n",
       "   B_recent_avg_PF_for  B_recent_avg_Score_against  B_recent_avg_Score_for  \\\n",
       "0                 17.5                        53.0                    61.5   \n",
       "1                 19.5                        73.5                    78.0   \n",
       "2                 16.0                        69.5                    85.5   \n",
       "3                 24.5                        83.5                    82.0   \n",
       "4                 23.0                        70.5                    68.0   \n",
       "\n",
       "   B_recent_avg_Stl_against  B_recent_avg_Stl_for  B_recent_avg_TO_against  \\\n",
       "0                       7.5                   7.5                     15.0   \n",
       "1                       7.0                   5.0                     15.0   \n",
       "2                      12.0                   7.0                     16.0   \n",
       "3                       6.0                   3.5                     14.0   \n",
       "4                       7.5                   4.5                     18.0   \n",
       "\n",
       "   B_recent_avg_TO_for  B_recent_avg_diff  B_recent_std_Ast_against  \\\n",
       "0                 11.5                8.5                       3.5   \n",
       "1                 15.0                4.5                       0.0   \n",
       "2                 11.5               16.0                       2.0   \n",
       "3                  7.0               -1.5                       0.0   \n",
       "4                 12.0               -2.5                       0.0   \n",
       "\n",
       "   B_recent_std_Ast_for  B_recent_std_Blk_against  B_recent_std_Blk_for  \\\n",
       "0                   1.0                       1.0                   0.5   \n",
       "1                   0.0                       0.0                   0.0   \n",
       "2                   0.5                       1.5                   1.0   \n",
       "3                   0.0                       0.0                   0.0   \n",
       "4                   0.0                       0.0                   0.0   \n",
       "\n",
       "   B_recent_std_DR_against  B_recent_std_DR_for  B_recent_std_FGPct2_against  \\\n",
       "0                      2.5                  2.0                     0.007612   \n",
       "1                      0.0                  0.0                     0.000000   \n",
       "2                      2.5                  4.0                     0.040816   \n",
       "3                      0.0                  0.0                     0.000000   \n",
       "4                      0.0                  0.0                     0.000000   \n",
       "\n",
       "   B_recent_std_FGPct2_for  B_recent_std_FGPct3_against  \\\n",
       "0                 0.033566                     0.115476   \n",
       "1                 0.000000                     0.000000   \n",
       "2                 0.006747                     0.053419   \n",
       "3                 0.000000                     0.000000   \n",
       "4                 0.000000                     0.000000   \n",
       "\n",
       "   B_recent_std_FGPct3_for  B_recent_std_FTPct_against  \\\n",
       "0                 0.047619                    0.017081   \n",
       "1                 0.000000                    0.000000   \n",
       "2                 0.122807                    0.034615   \n",
       "3                 0.000000                    0.000000   \n",
       "4                 0.000000                    0.000000   \n",
       "\n",
       "   B_recent_std_FTPct_for  B_recent_std_OR_against  B_recent_std_OR_for  \\\n",
       "0                0.134647                      1.5                  2.0   \n",
       "1                0.000000                      0.0                  0.0   \n",
       "2                0.015152                      1.0                  1.5   \n",
       "3                0.000000                      0.0                  0.0   \n",
       "4                0.000000                      0.0                  0.0   \n",
       "\n",
       "   B_recent_std_PF_against  B_recent_std_PF_for  B_recent_std_Score_against  \\\n",
       "0                      3.0                  2.5                         2.0   \n",
       "1                      0.0                  0.0                         0.0   \n",
       "2                      0.5                  1.0                         2.5   \n",
       "3                      0.0                  0.0                         0.0   \n",
       "4                      0.0                  0.0                         0.0   \n",
       "\n",
       "   B_recent_std_Score_for  B_recent_std_Stl_against  B_recent_std_Stl_for  \\\n",
       "0                     5.5                       1.5                   3.5   \n",
       "1                     0.0                       0.0                   0.0   \n",
       "2                     8.5                       2.0                   3.0   \n",
       "3                     0.0                       0.0                   0.0   \n",
       "4                     0.0                       0.0                   0.0   \n",
       "\n",
       "   B_recent_std_TO_against  B_recent_std_TO_for  B_recent_win_ratio  \\\n",
       "0                      2.0                  3.5                 1.0   \n",
       "1                      0.0                  0.0                 0.5   \n",
       "2                      1.0                  4.5                 1.0   \n",
       "3                      0.0                  0.0                 0.5   \n",
       "4                      0.0                  0.0                 0.5   \n",
       "\n",
       "   B_std_Ast_against  B_std_Ast_for  B_std_Blk_against  B_std_Blk_for  \\\n",
       "0           3.760553       4.446791           2.668821       1.736469   \n",
       "1           3.299825       3.490526           2.166685       2.609236   \n",
       "2           3.226069       4.475359           1.976508       2.337241   \n",
       "3           3.178106       4.392345           1.627033       2.455885   \n",
       "4           4.197221       5.453745           2.890213       3.407100   \n",
       "\n",
       "   B_std_DR_against  B_std_DR_for  B_std_FGPct2_against  B_std_FGPct2_for  \\\n",
       "0          5.948373      3.196622              0.081048          0.087979   \n",
       "1          4.345663      3.855293              0.076055          0.079974   \n",
       "2          4.047291      4.229913              0.089039          0.088209   \n",
       "3          4.164167      4.038474              0.087979          0.102842   \n",
       "4          4.295928      4.454960              0.077361          0.104143   \n",
       "\n",
       "   B_std_FGPct3_against  B_std_FGPct3_for  B_std_FTPct_against  \\\n",
       "0              0.102801          0.117352             0.145330   \n",
       "1              0.122928          0.127522             0.123120   \n",
       "2              0.144578          0.121457             0.092883   \n",
       "3              0.133451          0.094883             0.116456   \n",
       "4              0.097358          0.117574             0.120166   \n",
       "\n",
       "   B_std_FTPct_for  B_std_OR_against  B_std_OR_for  B_std_PF_against  \\\n",
       "0         0.110197          3.715969      3.598105          4.089516   \n",
       "1         0.118812          3.869106      4.950465          4.978860   \n",
       "2         0.081641          4.440128      3.533716          3.832463   \n",
       "3         0.097546          4.403653      3.778445          4.291807   \n",
       "4         0.112417          5.058491      4.419087          4.543310   \n",
       "\n",
       "   B_std_PF_for  B_std_Score_against  B_std_Score_for  B_std_Stl_against  \\\n",
       "0      4.026731             9.368980        10.750924           2.848674   \n",
       "1      3.058457            10.385999        10.407596           2.365746   \n",
       "2      4.483687            10.130156        10.563408           2.924655   \n",
       "3      3.699352             9.610211        10.864126           2.476665   \n",
       "4      3.465785             9.258780        12.145095           3.478505   \n",
       "\n",
       "   B_std_Stl_for  B_std_TO_against  B_std_TO_for  B_std_diff  B_win_ratio  \\\n",
       "0       3.546084          3.456905      4.303612   12.530245     0.655172   \n",
       "1       2.064380          3.459004      2.994857   17.095164     0.620690   \n",
       "2       2.955999          3.274294      4.503360   13.216876     0.793103   \n",
       "3       3.555092          3.768289      4.663107   16.106577     0.600000   \n",
       "4       2.516280          3.670150      3.510935   16.742518     0.700000   \n",
       "\n",
       "   NumOT  Season  round  \n",
       "0      0    2003      1  \n",
       "1      0    2003      1  \n",
       "2      0    2003      1  \n",
       "3      1    2003      1  \n",
       "4      0    2003      1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose (half) random rows to flip\n",
    "np.random.seed(SEED)\n",
    "flip = np.random.choice(features.index, int(features.shape[0] / 2), replace=False)\n",
    "\n",
    "# flip A, B cols\n",
    "for i, col in enumerate(features.columns):\n",
    "    if col[0] == 'A':\n",
    "        features.loc[flip, col], features.loc[flip, col.replace('A_', 'B_', 1)] = features.loc[flip, col.replace('A_', 'B_', 1)].values, features.loc[flip, col].values\n",
    "\n",
    "# delete vars\n",
    "del flip, col\n",
    "collect()\n",
    "\n",
    "# rearrange cols\n",
    "features = features.reindex(sorted(features.columns), axis=1)\n",
    "\n",
    "# check\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode 'round' col, drop one col\n",
    "features = pd.get_dummies(features, columns=['round'], dtype=int)\n",
    "features = features.drop(columns=['round_6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Labels\n",
    "Here, I will create a point total label that is simply the sum of the points scored by both team A and team B. Overtime periods will make this tricky, and I believe this could hurt my model's ability to generalize. I will aggregate the data to predict the average amount of points scored per overtime period in the data that I have, and then subtract this from the games were overtime occurred. Thus, I am creating a \"adjuster point total\" to predict point total in regulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_total</th>\n",
       "      <th>A_score_diff</th>\n",
       "      <th>A_win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155</td>\n",
       "      <td>-13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   point_total  A_score_diff  A_win\n",
       "0          131            29      1\n",
       "1          155           -13      0\n",
       "2          152            -6      0\n",
       "3          150             2      1\n",
       "4          111            -5      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create regression label, drop cols\n",
    "features['A_score_diff'] = features['A_Score'] - features['B_Score']\n",
    "features['point_total'] = features['A_Score'] + features['B_Score']\n",
    "features = features.drop(columns=['A_Score', 'B_Score'])\n",
    "\n",
    "# create binary label\n",
    "features['A_win'] = features['A_score_diff'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# check\n",
    "features.head()[['point_total', 'A_score_diff', 'A_win']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of games: 2078\n",
      "Total number of non-OT games: 1975\n",
      "Total number of OT games: 103\n",
      "Total single OT games: 88\n",
      "Total double OT games: 15\n",
      "\n",
      "Average point total of not OT games: 136.76303797468356\n",
      "Average point total of single OT games: 152.02272727272728\n",
      "Average point total of double OT games: 171.6\n",
      "\n",
      "Average extra points in single OT games: 15.259689298043725\n",
      "Average extra points in double OT games: 34.83696202531644\n"
     ]
    }
   ],
   "source": [
    "# OT statistics of the dataset\n",
    "print(f'Total number of games: {features.shape[0]}')\n",
    "print(f'Total number of non-OT games: {features[features[\"NumOT\"] == 0].shape[0]}')\n",
    "print(f'Total number of OT games: {features[features[\"NumOT\"] > 0].shape[0]}')\n",
    "print(f'Total single OT games: {features[features[\"NumOT\"] == 1].shape[0]}')\n",
    "print(f'Total double OT games: {features[features[\"NumOT\"] == 2].shape[0]}\\n')\n",
    "\n",
    "# get avg point total of not OT games\n",
    "avg_point_total = features[features['NumOT'] == 0]['point_total'].mean()\n",
    "\n",
    "# get avg point total of single OT games\n",
    "avg_point_total_single_ot = features[features['NumOT'] == 1]['point_total'].mean()\n",
    "\n",
    "# get avg point total of double OT games\n",
    "avg_point_total_double_ot = features[features['NumOT'] == 2]['point_total'].mean()\n",
    "\n",
    "print(f'Average point total of not OT games: {avg_point_total}')\n",
    "print(f'Average point total of single OT games: {avg_point_total_single_ot}')\n",
    "print(f'Average point total of double OT games: {avg_point_total_double_ot}\\n')\n",
    "\n",
    "avg_extra_pts_single_ot = avg_point_total_single_ot - avg_point_total\n",
    "avg_extra_pts_double_ot = avg_point_total_double_ot - avg_point_total\n",
    "\n",
    "print(f'Average extra points in single OT games: {avg_extra_pts_single_ot}')\n",
    "print(f'Average extra points in double OT games: {avg_extra_pts_double_ot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heefj\\AppData\\Local\\Temp\\ipykernel_7368\\2122059425.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '134.74031070195628' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  features.loc[idx, 'point_total_adj'] = row['point_total'] - avg_extra_pts_single_ot\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_total</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>point_total_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>168</td>\n",
       "      <td>2</td>\n",
       "      <td>133.163038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>182</td>\n",
       "      <td>2</td>\n",
       "      <td>147.163038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>145.163038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>100.163038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>197</td>\n",
       "      <td>2</td>\n",
       "      <td>162.163038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     point_total  NumOT  point_total_adj\n",
       "819          168      2       133.163038\n",
       "184          182      2       147.163038\n",
       "864          180      2       145.163038\n",
       "542          135      2       100.163038\n",
       "490          197      2       162.163038"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create adjusted point total col\n",
    "features['point_total_adj'] = 0\n",
    "for idx, row in features.iterrows():\n",
    "    if row['NumOT'] == 0:\n",
    "        features.loc[idx, 'point_total_adj'] = row['point_total']\n",
    "    elif row['NumOT'] == 1:\n",
    "        features.loc[idx, 'point_total_adj'] = row['point_total'] - avg_extra_pts_single_ot\n",
    "    elif row['NumOT'] == 2:\n",
    "        features.loc[idx, 'point_total_adj'] = row['point_total'] - avg_extra_pts_double_ot\n",
    "\n",
    "# check\n",
    "features[features['NumOT'] > 0][['point_total', 'NumOT', 'point_total_adj']].sort_values(by='NumOT', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols now that we have adjusted score diff\n",
    "features = features.drop(columns=['point_total', 'A_score_diff', 'NumOT', 'Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save updated features\n",
    "features.to_csv('data/processed/features_detailed_3_with_point_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differenced Features\n",
    "In combination with the team A/B data, we will also try training models using the differences between both teams' aggregated stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_TeamID</th>\n",
       "      <th>B_TeamID</th>\n",
       "      <th>A_PlayIn</th>\n",
       "      <th>B_PlayIn</th>\n",
       "      <th>A_conf_champs</th>\n",
       "      <th>B_conf_champs</th>\n",
       "      <th>A_close_game_missing</th>\n",
       "      <th>B_close_game_missing</th>\n",
       "      <th>A_away_missing</th>\n",
       "      <th>B_away_missing</th>\n",
       "      <th>A_home_missing</th>\n",
       "      <th>B_home_missing</th>\n",
       "      <th>A_neutral_missing</th>\n",
       "      <th>B_neutral_missing</th>\n",
       "      <th>round_1</th>\n",
       "      <th>round_2</th>\n",
       "      <th>round_3</th>\n",
       "      <th>round_4</th>\n",
       "      <th>round_5</th>\n",
       "      <th>A_win</th>\n",
       "      <th>point_total_adj</th>\n",
       "      <th>avg_Ast_for_diff</th>\n",
       "      <th>avg_Blk_for_diff</th>\n",
       "      <th>avg_DR_for_diff</th>\n",
       "      <th>avg_FGPct2_for_diff</th>\n",
       "      <th>avg_FGPct3_for_diff</th>\n",
       "      <th>avg_FTPct_for_diff</th>\n",
       "      <th>avg_OR_for_diff</th>\n",
       "      <th>avg_PF_for_diff</th>\n",
       "      <th>avg_Score_for_diff</th>\n",
       "      <th>avg_Stl_for_diff</th>\n",
       "      <th>avg_TO_for_diff</th>\n",
       "      <th>recent_avg_Ast_for_diff</th>\n",
       "      <th>recent_avg_Blk_for_diff</th>\n",
       "      <th>recent_avg_DR_for_diff</th>\n",
       "      <th>recent_avg_FGPct2_for_diff</th>\n",
       "      <th>recent_avg_FGPct3_for_diff</th>\n",
       "      <th>recent_avg_FTPct_for_diff</th>\n",
       "      <th>recent_avg_OR_for_diff</th>\n",
       "      <th>recent_avg_PF_for_diff</th>\n",
       "      <th>recent_avg_Score_for_diff</th>\n",
       "      <th>recent_avg_Stl_for_diff</th>\n",
       "      <th>recent_avg_TO_for_diff</th>\n",
       "      <th>recent_std_Ast_for_diff</th>\n",
       "      <th>recent_std_Blk_for_diff</th>\n",
       "      <th>recent_std_DR_for_diff</th>\n",
       "      <th>recent_std_FGPct2_for_diff</th>\n",
       "      <th>recent_std_FGPct3_for_diff</th>\n",
       "      <th>recent_std_FTPct_for_diff</th>\n",
       "      <th>recent_std_OR_for_diff</th>\n",
       "      <th>recent_std_PF_for_diff</th>\n",
       "      <th>recent_std_Score_for_diff</th>\n",
       "      <th>recent_std_Stl_for_diff</th>\n",
       "      <th>recent_std_TO_for_diff</th>\n",
       "      <th>std_Ast_for_diff</th>\n",
       "      <th>std_Blk_for_diff</th>\n",
       "      <th>std_DR_for_diff</th>\n",
       "      <th>std_FGPct2_for_diff</th>\n",
       "      <th>std_FGPct3_for_diff</th>\n",
       "      <th>std_FTPct_for_diff</th>\n",
       "      <th>std_OR_for_diff</th>\n",
       "      <th>std_PF_for_diff</th>\n",
       "      <th>std_Score_for_diff</th>\n",
       "      <th>std_Stl_for_diff</th>\n",
       "      <th>std_TO_for_diff</th>\n",
       "      <th>avg_Ast_against_diff</th>\n",
       "      <th>avg_Blk_against_diff</th>\n",
       "      <th>avg_DR_against_diff</th>\n",
       "      <th>avg_FGPct2_against_diff</th>\n",
       "      <th>avg_FGPct3_against_diff</th>\n",
       "      <th>avg_FTPct_against_diff</th>\n",
       "      <th>avg_OR_against_diff</th>\n",
       "      <th>avg_PF_against_diff</th>\n",
       "      <th>avg_Score_against_diff</th>\n",
       "      <th>avg_Stl_against_diff</th>\n",
       "      <th>avg_TO_against_diff</th>\n",
       "      <th>recent_avg_Ast_against_diff</th>\n",
       "      <th>recent_avg_Blk_against_diff</th>\n",
       "      <th>recent_avg_DR_against_diff</th>\n",
       "      <th>recent_avg_FGPct2_against_diff</th>\n",
       "      <th>recent_avg_FGPct3_against_diff</th>\n",
       "      <th>recent_avg_FTPct_against_diff</th>\n",
       "      <th>recent_avg_OR_against_diff</th>\n",
       "      <th>recent_avg_PF_against_diff</th>\n",
       "      <th>recent_avg_Score_against_diff</th>\n",
       "      <th>recent_avg_Stl_against_diff</th>\n",
       "      <th>recent_avg_TO_against_diff</th>\n",
       "      <th>recent_std_Ast_against_diff</th>\n",
       "      <th>recent_std_Blk_against_diff</th>\n",
       "      <th>recent_std_DR_against_diff</th>\n",
       "      <th>recent_std_FGPct2_against_diff</th>\n",
       "      <th>recent_std_FGPct3_against_diff</th>\n",
       "      <th>recent_std_FTPct_against_diff</th>\n",
       "      <th>recent_std_OR_against_diff</th>\n",
       "      <th>recent_std_PF_against_diff</th>\n",
       "      <th>recent_std_Score_against_diff</th>\n",
       "      <th>recent_std_Stl_against_diff</th>\n",
       "      <th>recent_std_TO_against_diff</th>\n",
       "      <th>std_Ast_against_diff</th>\n",
       "      <th>std_Blk_against_diff</th>\n",
       "      <th>std_DR_against_diff</th>\n",
       "      <th>std_FGPct2_against_diff</th>\n",
       "      <th>std_FGPct3_against_diff</th>\n",
       "      <th>std_FTPct_against_diff</th>\n",
       "      <th>std_OR_against_diff</th>\n",
       "      <th>std_PF_against_diff</th>\n",
       "      <th>std_Score_against_diff</th>\n",
       "      <th>std_Stl_against_diff</th>\n",
       "      <th>std_TO_against_diff</th>\n",
       "      <th>SeedNum_diff</th>\n",
       "      <th>avg_diff_diff</th>\n",
       "      <th>away_win_ratio_diff</th>\n",
       "      <th>close_game_ratio_diff</th>\n",
       "      <th>close_win_ratio_diff</th>\n",
       "      <th>home_win_ratio_diff</th>\n",
       "      <th>neutral_win_ratio_diff</th>\n",
       "      <th>num_games_diff</th>\n",
       "      <th>recent_avg_diff_diff</th>\n",
       "      <th>recent_win_ratio_diff</th>\n",
       "      <th>std_diff_diff</th>\n",
       "      <th>win_ratio_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>1436</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>4.366995</td>\n",
       "      <td>0.559113</td>\n",
       "      <td>5.780788</td>\n",
       "      <td>0.016010</td>\n",
       "      <td>0.011084</td>\n",
       "      <td>0.023399</td>\n",
       "      <td>5.592365</td>\n",
       "      <td>-0.181034</td>\n",
       "      <td>22.076355</td>\n",
       "      <td>1.360837</td>\n",
       "      <td>2.788177</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-0.007612</td>\n",
       "      <td>-0.115476</td>\n",
       "      <td>-0.017081</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.472958</td>\n",
       "      <td>-0.651043</td>\n",
       "      <td>-1.872091</td>\n",
       "      <td>-0.011725</td>\n",
       "      <td>-0.004007</td>\n",
       "      <td>-0.038919</td>\n",
       "      <td>-0.353891</td>\n",
       "      <td>-1.184266</td>\n",
       "      <td>0.853108</td>\n",
       "      <td>0.881831</td>\n",
       "      <td>1.706785</td>\n",
       "      <td>1.257389</td>\n",
       "      <td>-0.572660</td>\n",
       "      <td>-2.438424</td>\n",
       "      <td>-0.019704</td>\n",
       "      <td>-0.059113</td>\n",
       "      <td>-0.049261</td>\n",
       "      <td>0.141626</td>\n",
       "      <td>6.174877</td>\n",
       "      <td>2.456897</td>\n",
       "      <td>-0.897783</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.033566</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>-0.134647</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>-1.196791</td>\n",
       "      <td>-0.272619</td>\n",
       "      <td>0.168554</td>\n",
       "      <td>-0.019523</td>\n",
       "      <td>-0.041857</td>\n",
       "      <td>0.042668</td>\n",
       "      <td>0.466844</td>\n",
       "      <td>0.408988</td>\n",
       "      <td>-1.805821</td>\n",
       "      <td>-0.478881</td>\n",
       "      <td>0.224081</td>\n",
       "      <td>-15</td>\n",
       "      <td>10.309113</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.269704</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.007755</td>\n",
       "      <td>0.237685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1272</td>\n",
       "      <td>1113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>2.551724</td>\n",
       "      <td>1.137931</td>\n",
       "      <td>5.413793</td>\n",
       "      <td>-0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.068966</td>\n",
       "      <td>3.068966</td>\n",
       "      <td>-4.103448</td>\n",
       "      <td>5.344828</td>\n",
       "      <td>1.379310</td>\n",
       "      <td>1.068966</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.100660</td>\n",
       "      <td>1.064821</td>\n",
       "      <td>-0.678987</td>\n",
       "      <td>0.022653</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.031423</td>\n",
       "      <td>1.220992</td>\n",
       "      <td>-0.911404</td>\n",
       "      <td>-1.592834</td>\n",
       "      <td>0.815902</td>\n",
       "      <td>1.291776</td>\n",
       "      <td>-2.241379</td>\n",
       "      <td>-1.068966</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>-0.103448</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>-0.034483</td>\n",
       "      <td>-1.344828</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>-10.137931</td>\n",
       "      <td>2.068966</td>\n",
       "      <td>-1.724138</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.651700</td>\n",
       "      <td>-0.360586</td>\n",
       "      <td>-0.076703</td>\n",
       "      <td>-0.006265</td>\n",
       "      <td>-0.027246</td>\n",
       "      <td>0.006705</td>\n",
       "      <td>-0.761436</td>\n",
       "      <td>3.021522</td>\n",
       "      <td>-1.623669</td>\n",
       "      <td>0.789645</td>\n",
       "      <td>1.023674</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.896552</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>-0.137931</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.141594</td>\n",
       "      <td>0.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1166</td>\n",
       "      <td>1141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>5.025078</td>\n",
       "      <td>1.971787</td>\n",
       "      <td>4.285266</td>\n",
       "      <td>0.066876</td>\n",
       "      <td>0.018809</td>\n",
       "      <td>-0.018809</td>\n",
       "      <td>-1.362591</td>\n",
       "      <td>-4.416928</td>\n",
       "      <td>6.001045</td>\n",
       "      <td>-0.054336</td>\n",
       "      <td>-1.180773</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.018288</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>0.030297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.451716</td>\n",
       "      <td>0.153299</td>\n",
       "      <td>-0.963329</td>\n",
       "      <td>-0.010532</td>\n",
       "      <td>-0.042238</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>-1.673860</td>\n",
       "      <td>-0.713143</td>\n",
       "      <td>1.539942</td>\n",
       "      <td>-0.207333</td>\n",
       "      <td>1.202921</td>\n",
       "      <td>-3.257053</td>\n",
       "      <td>-1.424242</td>\n",
       "      <td>-1.912226</td>\n",
       "      <td>-0.062696</td>\n",
       "      <td>-0.041797</td>\n",
       "      <td>-0.057471</td>\n",
       "      <td>0.474399</td>\n",
       "      <td>-1.571578</td>\n",
       "      <td>-15.011494</td>\n",
       "      <td>-0.770115</td>\n",
       "      <td>-2.705329</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.073170</td>\n",
       "      <td>-0.014964</td>\n",
       "      <td>0.158144</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.236214</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.147403</td>\n",
       "      <td>-0.010739</td>\n",
       "      <td>0.021640</td>\n",
       "      <td>0.037382</td>\n",
       "      <td>0.274265</td>\n",
       "      <td>-1.187793</td>\n",
       "      <td>-1.283217</td>\n",
       "      <td>-0.694744</td>\n",
       "      <td>-1.138788</td>\n",
       "      <td>-5</td>\n",
       "      <td>8.805643</td>\n",
       "      <td>-0.006494</td>\n",
       "      <td>-0.167189</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.659204</td>\n",
       "      <td>0.085684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1143</td>\n",
       "      <td>1301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>134.740311</td>\n",
       "      <td>3.433333</td>\n",
       "      <td>-0.040230</td>\n",
       "      <td>2.945977</td>\n",
       "      <td>-0.018391</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>0.708046</td>\n",
       "      <td>-2.229885</td>\n",
       "      <td>6.482759</td>\n",
       "      <td>-0.881609</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907312</td>\n",
       "      <td>0.060796</td>\n",
       "      <td>0.822367</td>\n",
       "      <td>-0.021877</td>\n",
       "      <td>-0.017281</td>\n",
       "      <td>0.027691</td>\n",
       "      <td>-0.649837</td>\n",
       "      <td>-0.772683</td>\n",
       "      <td>-2.199926</td>\n",
       "      <td>0.158172</td>\n",
       "      <td>0.240630</td>\n",
       "      <td>1.367816</td>\n",
       "      <td>-0.549425</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>-0.017241</td>\n",
       "      <td>-0.022989</td>\n",
       "      <td>-0.145977</td>\n",
       "      <td>1.439080</td>\n",
       "      <td>0.436782</td>\n",
       "      <td>-2.641379</td>\n",
       "      <td>-1.835632</td>\n",
       "      <td>-0.460920</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.160118</td>\n",
       "      <td>-0.921389</td>\n",
       "      <td>-0.433127</td>\n",
       "      <td>-0.013268</td>\n",
       "      <td>-0.012093</td>\n",
       "      <td>0.034840</td>\n",
       "      <td>1.143307</td>\n",
       "      <td>0.077946</td>\n",
       "      <td>-2.734541</td>\n",
       "      <td>-0.134638</td>\n",
       "      <td>-0.150126</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.324138</td>\n",
       "      <td>0.336364</td>\n",
       "      <td>-0.121839</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.529517</td>\n",
       "      <td>0.124138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1140</td>\n",
       "      <td>1163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>-0.180645</td>\n",
       "      <td>-1.617204</td>\n",
       "      <td>2.919355</td>\n",
       "      <td>0.083871</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>-4.462366</td>\n",
       "      <td>2.152688</td>\n",
       "      <td>1.051613</td>\n",
       "      <td>-0.864516</td>\n",
       "      <td>-1.316129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.146537</td>\n",
       "      <td>-1.204993</td>\n",
       "      <td>0.278366</td>\n",
       "      <td>0.004774</td>\n",
       "      <td>0.037156</td>\n",
       "      <td>-0.036455</td>\n",
       "      <td>-1.401772</td>\n",
       "      <td>-1.177783</td>\n",
       "      <td>0.468115</td>\n",
       "      <td>-0.789017</td>\n",
       "      <td>0.402916</td>\n",
       "      <td>-4.859140</td>\n",
       "      <td>-5.507527</td>\n",
       "      <td>-7.706452</td>\n",
       "      <td>-0.047312</td>\n",
       "      <td>-0.108602</td>\n",
       "      <td>0.011828</td>\n",
       "      <td>-4.347312</td>\n",
       "      <td>2.987097</td>\n",
       "      <td>-16.355914</td>\n",
       "      <td>-0.449462</td>\n",
       "      <td>0.375269</td>\n",
       "      <td>8.5</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.168468</td>\n",
       "      <td>-1.765323</td>\n",
       "      <td>-1.618324</td>\n",
       "      <td>-0.032232</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>-1.531441</td>\n",
       "      <td>0.129818</td>\n",
       "      <td>-2.352097</td>\n",
       "      <td>-0.094742</td>\n",
       "      <td>0.473218</td>\n",
       "      <td>7</td>\n",
       "      <td>0.140860</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.009677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.990128</td>\n",
       "      <td>0.041935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A_TeamID  B_TeamID  A_PlayIn  B_PlayIn  A_conf_champs  B_conf_champs  \\\n",
       "0      1112      1436         0         0            0.0            1.0   \n",
       "1      1272      1113         0         0            0.0            0.0   \n",
       "2      1166      1141         0         0            1.0            1.0   \n",
       "3      1143      1301         0         0            0.0            0.0   \n",
       "4      1140      1163         0         0            0.0            0.0   \n",
       "\n",
       "   A_close_game_missing  B_close_game_missing  A_away_missing  B_away_missing  \\\n",
       "0                     0                     0               0               0   \n",
       "1                     0                     0               0               0   \n",
       "2                     0                     0               0               0   \n",
       "3                     0                     0               0               0   \n",
       "4                     0                     0               0               0   \n",
       "\n",
       "   A_home_missing  B_home_missing  A_neutral_missing  B_neutral_missing  \\\n",
       "0               0               0                  0                  0   \n",
       "1               0               0                  0                  0   \n",
       "2               0               0                  0                  0   \n",
       "3               0               0                  0                  0   \n",
       "4               0               0                  0                  0   \n",
       "\n",
       "   round_1  round_2  round_3  round_4  round_5  A_win  point_total_adj  \\\n",
       "0        1        0        0        0        0      1       131.000000   \n",
       "1        1        0        0        0        0      0       155.000000   \n",
       "2        1        0        0        0        0      0       152.000000   \n",
       "3        1        0        0        0        0      1       134.740311   \n",
       "4        1        0        0        0        0      0       111.000000   \n",
       "\n",
       "   avg_Ast_for_diff  avg_Blk_for_diff  avg_DR_for_diff  avg_FGPct2_for_diff  \\\n",
       "0          4.366995          0.559113         5.780788             0.016010   \n",
       "1          2.551724          1.137931         5.413793            -0.034483   \n",
       "2          5.025078          1.971787         4.285266             0.066876   \n",
       "3          3.433333         -0.040230         2.945977            -0.018391   \n",
       "4         -0.180645         -1.617204         2.919355             0.083871   \n",
       "\n",
       "   avg_FGPct3_for_diff  avg_FTPct_for_diff  avg_OR_for_diff  avg_PF_for_diff  \\\n",
       "0             0.011084            0.023399         5.592365        -0.181034   \n",
       "1             0.000000           -0.068966         3.068966        -4.103448   \n",
       "2             0.018809           -0.018809        -1.362591        -4.416928   \n",
       "3             0.044828           -0.011494         0.708046        -2.229885   \n",
       "4             0.054839            0.043011        -4.462366         2.152688   \n",
       "\n",
       "   avg_Score_for_diff  avg_Stl_for_diff  avg_TO_for_diff  \\\n",
       "0           22.076355          1.360837         2.788177   \n",
       "1            5.344828          1.379310         1.068966   \n",
       "2            6.001045         -0.054336        -1.180773   \n",
       "3            6.482759         -0.881609         0.731034   \n",
       "4            1.051613         -0.864516        -1.316129   \n",
       "\n",
       "   recent_avg_Ast_for_diff  recent_avg_Blk_for_diff  recent_avg_DR_for_diff  \\\n",
       "0                      9.0                      0.5                     4.0   \n",
       "1                     -3.5                      2.5                     7.5   \n",
       "2                      8.0                      3.5                    12.0   \n",
       "3                      2.5                      0.5                     6.5   \n",
       "4                      0.0                     -3.5                     3.0   \n",
       "\n",
       "   recent_avg_FGPct2_for_diff  recent_avg_FGPct3_for_diff  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.5                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   recent_avg_FTPct_for_diff  recent_avg_OR_for_diff  recent_avg_PF_for_diff  \\\n",
       "0                       -0.5                     6.0                     1.0   \n",
       "1                        0.0                     8.5                     7.0   \n",
       "2                        0.0                    -2.0                    -7.5   \n",
       "3                        0.0                     4.0                    -7.5   \n",
       "4                        0.0                   -11.5                     5.0   \n",
       "\n",
       "   recent_avg_Score_for_diff  recent_avg_Stl_for_diff  recent_avg_TO_for_diff  \\\n",
       "0                       35.5                      4.0                     3.0   \n",
       "1                       -5.0                     -4.0                    -4.5   \n",
       "2                        5.5                     -6.5                    -3.0   \n",
       "3                      -18.0                     -2.5                    -5.5   \n",
       "4                        5.0                      0.5                    -4.5   \n",
       "\n",
       "   recent_std_Ast_for_diff  recent_std_Blk_for_diff  recent_std_DR_for_diff  \\\n",
       "0                     -3.5                     -1.0                    -2.5   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "2                     -1.0                     -0.5                     1.0   \n",
       "3                      0.0                      0.0                     0.0   \n",
       "4                      0.0                      0.0                     0.0   \n",
       "\n",
       "   recent_std_FGPct2_for_diff  recent_std_FGPct3_for_diff  \\\n",
       "0                   -0.007612                   -0.115476   \n",
       "1                    0.000000                    0.000000   \n",
       "2                   -0.018288                   -0.001383   \n",
       "3                    0.000000                    0.000000   \n",
       "4                    0.000000                    0.000000   \n",
       "\n",
       "   recent_std_FTPct_for_diff  recent_std_OR_for_diff  recent_std_PF_for_diff  \\\n",
       "0                  -0.017081                    -1.5                    -3.0   \n",
       "1                   0.000000                     0.0                     0.0   \n",
       "2                   0.030297                     1.0                     1.5   \n",
       "3                   0.000000                     0.0                     0.0   \n",
       "4                   0.000000                     0.0                     0.0   \n",
       "\n",
       "   recent_std_Score_for_diff  recent_std_Stl_for_diff  recent_std_TO_for_diff  \\\n",
       "0                       -2.0                     -1.5                    -2.0   \n",
       "1                        0.0                      0.0                     0.0   \n",
       "2                        2.5                     -1.5                     0.0   \n",
       "3                        0.0                      0.0                     0.0   \n",
       "4                        0.0                      0.0                     0.0   \n",
       "\n",
       "   std_Ast_for_diff  std_Blk_for_diff  std_DR_for_diff  std_FGPct2_for_diff  \\\n",
       "0          0.472958         -0.651043        -1.872091            -0.011725   \n",
       "1          1.100660          1.064821        -0.678987             0.022653   \n",
       "2          1.451716          0.153299        -0.963329            -0.010532   \n",
       "3          0.907312          0.060796         0.822367            -0.021877   \n",
       "4         -0.146537         -1.204993         0.278366             0.004774   \n",
       "\n",
       "   std_FGPct3_for_diff  std_FTPct_for_diff  std_OR_for_diff  std_PF_for_diff  \\\n",
       "0            -0.004007           -0.038919        -0.353891        -1.184266   \n",
       "1             0.002199            0.031423         1.220992        -0.911404   \n",
       "2            -0.042238            0.000875        -1.673860        -0.713143   \n",
       "3            -0.017281            0.027691        -0.649837        -0.772683   \n",
       "4             0.037156           -0.036455        -1.401772        -1.177783   \n",
       "\n",
       "   std_Score_for_diff  std_Stl_for_diff  std_TO_for_diff  \\\n",
       "0            0.853108          0.881831         1.706785   \n",
       "1           -1.592834          0.815902         1.291776   \n",
       "2            1.539942         -0.207333         1.202921   \n",
       "3           -2.199926          0.158172         0.240630   \n",
       "4            0.468115         -0.789017         0.402916   \n",
       "\n",
       "   avg_Ast_against_diff  avg_Blk_against_diff  avg_DR_against_diff  \\\n",
       "0              1.257389             -0.572660            -2.438424   \n",
       "1             -2.241379             -1.068966             0.275862   \n",
       "2             -3.257053             -1.424242            -1.912226   \n",
       "3              1.367816             -0.549425             0.966667   \n",
       "4             -4.859140             -5.507527            -7.706452   \n",
       "\n",
       "   avg_FGPct2_against_diff  avg_FGPct3_against_diff  avg_FTPct_against_diff  \\\n",
       "0                -0.019704                -0.059113               -0.049261   \n",
       "1                -0.103448                 0.034483               -0.034483   \n",
       "2                -0.062696                -0.041797               -0.057471   \n",
       "3                -0.017241                -0.022989               -0.145977   \n",
       "4                -0.047312                -0.108602                0.011828   \n",
       "\n",
       "   avg_OR_against_diff  avg_PF_against_diff  avg_Score_against_diff  \\\n",
       "0             0.141626             6.174877                2.456897   \n",
       "1            -1.344828             0.517241              -10.137931   \n",
       "2             0.474399            -1.571578              -15.011494   \n",
       "3             1.439080             0.436782               -2.641379   \n",
       "4            -4.347312             2.987097              -16.355914   \n",
       "\n",
       "   avg_Stl_against_diff  avg_TO_against_diff  recent_avg_Ast_against_diff  \\\n",
       "0             -0.897783             1.785714                          4.5   \n",
       "1              2.068966            -1.724138                         -5.5   \n",
       "2             -0.770115            -2.705329                         -2.0   \n",
       "3             -1.835632            -0.460920                         -3.5   \n",
       "4             -0.449462             0.375269                          8.5   \n",
       "\n",
       "   recent_avg_Blk_against_diff  recent_avg_DR_against_diff  \\\n",
       "0                          1.0                         1.5   \n",
       "1                          0.5                         1.0   \n",
       "2                          0.0                        -5.5   \n",
       "3                          1.5                        -2.0   \n",
       "4                         -4.0                        -3.0   \n",
       "\n",
       "   recent_avg_FGPct2_against_diff  recent_avg_FGPct3_against_diff  \\\n",
       "0                             0.0                             0.0   \n",
       "1                             0.0                             0.0   \n",
       "2                             0.0                            -0.5   \n",
       "3                             0.0                             0.0   \n",
       "4                             0.0                             0.0   \n",
       "\n",
       "   recent_avg_FTPct_against_diff  recent_avg_OR_against_diff  \\\n",
       "0                           -0.5                         3.5   \n",
       "1                            0.0                         2.0   \n",
       "2                            0.0                         0.5   \n",
       "3                            0.0                         7.5   \n",
       "4                            0.0                        -8.5   \n",
       "\n",
       "   recent_avg_PF_against_diff  recent_avg_Score_against_diff  \\\n",
       "0                         6.5                           26.5   \n",
       "1                         5.0                          -11.0   \n",
       "2                         1.5                          -23.0   \n",
       "3                        -7.5                          -19.5   \n",
       "4                        -1.0                            3.0   \n",
       "\n",
       "   recent_avg_Stl_against_diff  recent_avg_TO_against_diff  \\\n",
       "0                         -1.0                         0.5   \n",
       "1                          2.5                        -2.0   \n",
       "2                         -0.5                         3.0   \n",
       "3                          5.5                         8.0   \n",
       "4                         -3.0                        -4.0   \n",
       "\n",
       "   recent_std_Ast_against_diff  recent_std_Blk_against_diff  \\\n",
       "0                         -1.0                         -0.5   \n",
       "1                          0.0                          0.0   \n",
       "2                          0.0                          2.0   \n",
       "3                          0.0                          0.0   \n",
       "4                          0.0                          0.0   \n",
       "\n",
       "   recent_std_DR_against_diff  recent_std_FGPct2_against_diff  \\\n",
       "0                        -2.0                       -0.033566   \n",
       "1                         0.0                        0.000000   \n",
       "2                        -2.5                        0.073170   \n",
       "3                         0.0                        0.000000   \n",
       "4                         0.0                        0.000000   \n",
       "\n",
       "   recent_std_FGPct3_against_diff  recent_std_FTPct_against_diff  \\\n",
       "0                       -0.047619                      -0.134647   \n",
       "1                        0.000000                       0.000000   \n",
       "2                       -0.014964                       0.158144   \n",
       "3                        0.000000                       0.000000   \n",
       "4                        0.000000                       0.000000   \n",
       "\n",
       "   recent_std_OR_against_diff  recent_std_PF_against_diff  \\\n",
       "0                        -2.0                        -2.5   \n",
       "1                         0.0                         0.0   \n",
       "2                         1.5                         0.5   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   recent_std_Score_against_diff  recent_std_Stl_against_diff  \\\n",
       "0                           -5.5                         -3.5   \n",
       "1                            0.0                          0.0   \n",
       "2                           -2.0                         -1.5   \n",
       "3                            0.0                          0.0   \n",
       "4                            0.0                          0.0   \n",
       "\n",
       "   recent_std_TO_against_diff  std_Ast_against_diff  std_Blk_against_diff  \\\n",
       "0                        -3.5             -1.196791             -0.272619   \n",
       "1                         0.0             -0.651700             -0.360586   \n",
       "2                        -2.0             -1.236214              0.000726   \n",
       "3                         0.0             -1.160118             -0.921389   \n",
       "4                         0.0             -2.168468             -1.765323   \n",
       "\n",
       "   std_DR_against_diff  std_FGPct2_against_diff  std_FGPct3_against_diff  \\\n",
       "0             0.168554                -0.019523                -0.041857   \n",
       "1            -0.076703                -0.006265                -0.027246   \n",
       "2             0.147403                -0.010739                 0.021640   \n",
       "3            -0.433127                -0.013268                -0.012093   \n",
       "4            -1.618324                -0.032232                 0.000800   \n",
       "\n",
       "   std_FTPct_against_diff  std_OR_against_diff  std_PF_against_diff  \\\n",
       "0                0.042668             0.466844             0.408988   \n",
       "1                0.006705            -0.761436             3.021522   \n",
       "2                0.037382             0.274265            -1.187793   \n",
       "3                0.034840             1.143307             0.077946   \n",
       "4               -0.001054            -1.531441             0.129818   \n",
       "\n",
       "   std_Score_against_diff  std_Stl_against_diff  std_TO_against_diff  \\\n",
       "0               -1.805821             -0.478881             0.224081   \n",
       "1               -1.623669              0.789645             1.023674   \n",
       "2               -1.283217             -0.694744            -1.138788   \n",
       "3               -2.734541             -0.134638            -0.150126   \n",
       "4               -2.352097             -0.094742             0.473218   \n",
       "\n",
       "   SeedNum_diff  avg_diff_diff  away_win_ratio_diff  close_game_ratio_diff  \\\n",
       "0           -15      10.309113             0.416667              -0.269704   \n",
       "1            -3       1.896552             0.236364              -0.137931   \n",
       "2            -5       8.805643            -0.006494              -0.167189   \n",
       "3            -1       0.324138             0.336364              -0.121839   \n",
       "4             7       0.140860             0.090909              -0.009677   \n",
       "\n",
       "   close_win_ratio_diff  home_win_ratio_diff  neutral_win_ratio_diff  \\\n",
       "0              0.369231             0.033333                   -0.60   \n",
       "1              0.012987             0.075000                    0.50   \n",
       "2              0.025974             0.083333                    0.00   \n",
       "3              0.057143             0.116071                   -0.25   \n",
       "4              0.000000             0.053571                    0.00   \n",
       "\n",
       "   num_games_diff  recent_avg_diff_diff  recent_win_ratio_diff  std_diff_diff  \\\n",
       "0              -1                  -8.0                   -0.5      -0.007755   \n",
       "1               0                  -3.0                    0.0      -6.141594   \n",
       "2               4                  -3.5                    0.0       1.659204   \n",
       "3              -1                   4.5                    0.0      -5.529517   \n",
       "4               1                   7.0                    0.0      -3.990128   \n",
       "\n",
       "   win_ratio_diff  \n",
       "0        0.237685  \n",
       "1        0.172414  \n",
       "2        0.085684  \n",
       "3        0.124138  \n",
       "4        0.041935  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identifiers, categorical features, and labels to be kept (shouldn't be differenced)\n",
    "non_diff_cols = ['A_TeamID', 'B_TeamID', 'A_PlayIn', 'B_PlayIn', 'A_conf_champs', 'B_conf_champs', 'A_close_game_missing', 'B_close_game_missing', 'A_away_missing', 'B_away_missing', \n",
    "                 'A_home_missing', 'B_home_missing', 'A_neutral_missing', 'B_neutral_missing', 'round_1', 'round_2', 'round_3', 'round_4', 'round_5', 'A_win', 'point_total_adj']\n",
    "\n",
    "# create diff df\n",
    "features_diff = features[non_diff_cols].copy()\n",
    "\n",
    "# get cols that start with A\n",
    "a_cols_for = [col for col in features.columns if col[0] == 'A' and col[-3:] == 'for']\n",
    "a_cols_against = [col for col in features.columns if col[0] == 'A' and col[-7:] == 'against']\n",
    "a_cols_other = [col for col in features.columns if col[0] == 'A' and col not in a_cols_for and col not in a_cols_against and col not in non_diff_cols]\n",
    "\n",
    "# create diff cols\n",
    "# team A's offensive advantage/disadvantage over team B\n",
    "for a_col in a_cols_for:\n",
    "        features_diff[a_col.replace('A_', '', 1) + '_diff'] = features[a_col] - features[a_col.replace('A_', 'B_', 1).replace('for', 'against', 1)]\n",
    "        features_diff = features_diff.copy()\n",
    "# team A's defensive advantage/disadvantage over team B\n",
    "for a_col in a_cols_against:\n",
    "    features_diff[a_col.replace('A_', '', 1) + '_diff'] = features[a_col] - features[a_col.replace('A_', 'B_', 1).replace('against', 'for', 1)]\n",
    "    features_diff = features_diff.copy()\n",
    "\n",
    "for a_col in a_cols_other:\n",
    "    features_diff[a_col.replace('A_', '', 1) + '_diff'] = features[a_col] - features[a_col.replace('A_', 'B_', 1)]\n",
    "    features_diff = features_diff.copy()\n",
    "\n",
    "# check\n",
    "features_diff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save updated features\n",
    "features_diff.to_csv('data/processed/features_detailed_diff_3_with_point_total.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data (Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in features (checkpoint)\n",
    "features = pd.read_csv('data/processed/features_detailed_3_with_point_total.csv')\n",
    "features_diff = pd.read_csv('data/processed/features_detailed_diff_3_with_point_total.csv')\n",
    "\n",
    "# split on gender\n",
    "mfeatures = features[features['A_TeamID'] < 3000]\n",
    "wfeatures = features[features['A_TeamID'] >= 3000]\n",
    "del features\n",
    "\n",
    "# same for diff data\n",
    "mfeatures_diff = features_diff[features_diff['A_TeamID'] < 3000]\n",
    "wfeatures_diff = features_diff[features_diff['A_TeamID'] >= 3000]\n",
    "del features_diff\n",
    "\n",
    "# drop team id cols\n",
    "mfeatures = mfeatures.drop(columns=['A_TeamID', 'B_TeamID'])\n",
    "wfeatures = wfeatures.drop(columns=['A_TeamID', 'B_TeamID'])\n",
    "mfeatures_diff = mfeatures_diff.drop(columns=['A_TeamID', 'B_TeamID'])\n",
    "wfeatures_diff = wfeatures_diff.drop(columns=['A_TeamID', 'B_TeamID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chalk Bracket\n",
    "Here, we will simply predict the better seed. If seeds are equal (only possible in rounds 5 and 6), we will predict the team with the better win_ratio from the regular season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_preds(data):\n",
    "    \"\"\"\n",
    "    Function to get dummy predictions based on seed and win percentage\n",
    "    \"\"\"\n",
    "\n",
    "    # create a container\n",
    "    dummy_preds = []\n",
    "\n",
    "    # loop through the dataframe based on conditions\n",
    "    for idx, row in data.iterrows():\n",
    "        if data.loc[idx, \"A_Seed\"] < data.loc[idx, \"B_Seed\"]:\n",
    "            dummy_preds.append(1)\n",
    "        elif data.loc[idx, \"A_Seed\"] > data.loc[idx, \"B_Seed\"]:\n",
    "            dummy_preds.append(0)\n",
    "        else:\n",
    "            if data.loc[idx, \"A_win_ratio\"] > data.loc[idx, \"B_win_ratio\"]:\n",
    "                dummy_preds.append(1)\n",
    "            else:\n",
    "                dummy_preds.append(0)\n",
    "    \n",
    "    return np.array(dummy_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of dummy predictions: 70.61%.\n"
     ]
    }
   ],
   "source": [
    "# get dummy preds\n",
    "mchalk_preds = get_dummy_preds(mfeatures)\n",
    "\n",
    "# compare preds to win col\n",
    "mchalk_acc = accuracy_score(mfeatures['A_win'], mchalk_preds)\n",
    "\n",
    "print(f\"Accuracy of dummy predictions: {mchalk_acc*100:.2f}%.\")\n",
    "\n",
    "# delete vars\n",
    "del mchalk_preds, mchalk_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Women's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of dummy predictions: 78.02%\n"
     ]
    }
   ],
   "source": [
    "# get dummy preds\n",
    "wchalk_preds = get_dummy_preds(wfeatures)\n",
    "\n",
    "# compare preds to win col\n",
    "wchalk_acc = accuracy_score(wfeatures['A_win'], wchalk_preds)\n",
    "\n",
    "print(f\"Accuracy of dummy predictions: {wchalk_acc*100:.2f}%\")\n",
    "\n",
    "# delete vars\n",
    "del wchalk_preds, wchalk_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "Here, we train some models to predict __point total__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of both team A and B: 217\n",
      "Features of the DIFFERENCE between team A and B: 117\n"
     ]
    }
   ],
   "source": [
    "# non-feature cols\n",
    "labels = ['A_win', 'point_total_adj']\n",
    "\n",
    "# look at num feats for the 2 datasets\n",
    "print(f'Features of both team A and B: {mfeatures.shape[1] - len(labels)}')\n",
    "print(f'Features of the DIFFERENCE between team A and B: {mfeatures_diff.shape[1] - len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature order list so our models are trained on the same feature order\n",
    "sorted_cols = sorted(mfeatures.columns)\n",
    "diff_cols = sorted(mfeatures_diff.columns)\n",
    "\n",
    "# sort all dataframes\n",
    "mfeatures = mfeatures[sorted_cols]\n",
    "wfeatures = wfeatures[sorted_cols]\n",
    "mfeatures_diff = mfeatures_diff[diff_cols]\n",
    "wfeatures_diff = wfeatures_diff[diff_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_point_total(estimator, data, models_df, tournament, scaler=None, folds=5):\n",
    "    \"\"\"\n",
    "    Validate a regression model on input data and save results to models_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : sklearn estimator\n",
    "        Estimator to use for modeling.\n",
    "    data : pd.DataFrame\n",
    "        Data to model.\n",
    "    scaler : sklearn scaler\n",
    "        Scaler to use for data.\n",
    "    models_df : pd.DataFrame\n",
    "        DataFrame to save results to.\n",
    "    tournament : str\n",
    "        Gender - 'M' or 'F'.\n",
    "    folds : int\n",
    "        Number of cross-validation folds to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # create copy to avoid modification\n",
    "    data = data.copy()\n",
    "\n",
    "    # define cross-validation\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # define X and y\n",
    "    X = data.drop(columns=['point_total_adj', 'A_win'])\n",
    "    y = data['point_total_adj']\n",
    "\n",
    "    # initialize lists to store metrics\n",
    "    rmse_scores_train = []\n",
    "    rmse_scores_test = []\n",
    "    r2_scores_train = []\n",
    "    r2_scores_test = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        if scaler:\n",
    "            # scale data\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "        # fit model\n",
    "        estimator.fit(X_train, y_train)\n",
    "\n",
    "        # predict\n",
    "        train_preds = estimator.predict(X_train)\n",
    "        test_preds = estimator.predict(X_test)\n",
    "\n",
    "        # evaluate\n",
    "        rmse_scores_train.append(root_mean_squared_error(y_train, train_preds))\n",
    "        rmse_scores_test.append(root_mean_squared_error(y_test, test_preds))\n",
    "        r2_scores_train.append(r2_score(y_train, train_preds))\n",
    "        r2_scores_test.append(r2_score(y_test, test_preds))\n",
    "\n",
    "    # average the metrics across folds\n",
    "    train_rmse = np.mean(rmse_scores_train)\n",
    "    test_rmse = np.mean(rmse_scores_test)\n",
    "    train_r2 = np.mean(r2_scores_train)\n",
    "    test_r2 = np.mean(r2_scores_test)\n",
    "    \n",
    "    # Save results to models_df\n",
    "    models_df.loc[len(models_df.index)] = [tournament, 'point_total_adj', estimator, scaler, X.shape[1], X.columns.to_list(), folds, train_r2, test_r2, train_rmse, test_rmse]\n",
    "\n",
    "    # delete variables\n",
    "    collect()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions vs. actual values\n",
    "def plot_preds(data, estimator, scaler, title):\n",
    "    \"\"\"\n",
    "    Plot predictions vs. actual values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Series\n",
    "        Label (point_total_adj).\n",
    "    estimator : sklearn estimator\n",
    "        Estimator to use for modeling.\n",
    "    scaler : sklearn scaler\n",
    "        Scaler to use for data.\n",
    "    title : str\n",
    "        Title of the plot.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # get label\n",
    "    label = data['point_total_adj']\n",
    "\n",
    "    # drop label cols\n",
    "    data = data.drop(columns=['point_total_adj', 'A_win'])\n",
    "\n",
    "    # scale data\n",
    "    if scaler:\n",
    "        data = scaler.fit_transform(data)\n",
    "\n",
    "    # cross validate, get preds\n",
    "    preds = cross_val_predict(estimator, data, label, cv=5)\n",
    "\n",
    "    # create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # scatter plot\n",
    "    ax.scatter(label, preds, alpha=0.5, color='red')\n",
    "\n",
    "    # add line\n",
    "    ax.plot([data.min(), data.max()], [data.min(), data.max()], 'k--', lw=2)\n",
    "\n",
    "    # make sure axes only go from min to max of data\n",
    "\n",
    "    # labels\n",
    "    ax.set_xlabel('Actual Pt Total')\n",
    "    ax.set_ylabel('Predicted Pt Total')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used later in the \"Classification\" section\n",
    "def predict_win(estimator, data, models_df, tournament, scaler=None, folds=5):\n",
    "    \"\"\"\n",
    "    Validate a classification model on input data and save results to models_df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : sklearn estimator\n",
    "        Estimator to use for modeling.\n",
    "    data : pd.DataFrame\n",
    "        Data to model.\n",
    "    scaler : sklearn scaler\n",
    "        Scaler to use for data.\n",
    "    models_df : pd.DataFrame\n",
    "        DataFrame to save results to.\n",
    "    tournament : str\n",
    "        Gender - 'M' or 'F'.\n",
    "    folds : int\n",
    "        Number of cross-validation folds to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # create copy to avoid modification\n",
    "    data = data.copy()\n",
    "\n",
    "    # define cross-validation\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # define X and y\n",
    "    X = data.drop(columns=['A_score_diff_adj', 'A_win'])\n",
    "    y = data['A_win']\n",
    "\n",
    "    # initialize lists to store metrics\n",
    "    log_loss_train = []\n",
    "    log_loss_test = []\n",
    "    acc_scores_train = []\n",
    "    acc_scores_test = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # split data\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        if scaler:\n",
    "            # scale data\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "        # fit model\n",
    "        estimator.fit(X_train, y_train)\n",
    "\n",
    "        # predict\n",
    "        train_preds = estimator.predict(X_train)\n",
    "        test_preds = estimator.predict(X_test)\n",
    "\n",
    "\n",
    "        # evaluate\n",
    "        log_loss_train.append(log_loss(y_train, train_preds))\n",
    "        log_loss_test.append(log_loss(y_test, test_preds))\n",
    "        acc_scores_train.append(accuracy_score(np.sign(y_train), np.sign(train_preds)))\n",
    "        acc_scores_test.append(accuracy_score(np.sign(y_test), np.sign(test_preds)))\n",
    "\n",
    "    # average the metrics across folds\n",
    "    train_log_loss = np.mean(log_loss_train)\n",
    "    test_log_loss = np.mean(log_loss_test)\n",
    "    train_acc = np.mean(acc_scores_train)\n",
    "    test_acc = np.mean(acc_scores_test)\n",
    "    \n",
    "    # save results to models_df\n",
    "    models_df.loc[len(models_df.index)] = [tournament, 'A_win', estimator, scaler, X.shape[1], X.columns.to_list(), folds, train_log_loss, test_log_loss, train_acc, test_acc]\n",
    "\n",
    "    # delete variables\n",
    "    collect()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df to hold regression models\n",
    "pt_df = pd.DataFrame(columns=['Tournament', 'Label', 'Model', 'Scaler', 'Num_Features', 'Features', 'Num_CV_Folds', 'Train_R2', 'Val_R2', 'Train_RMSE', 'Val_RMSE'])\n",
    "\n",
    "# load models df\n",
    "# pt_df = pd.read_csv('models/point_total_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 100%|| 5/5 [00:57<00:00, 11.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# define regression models\n",
    "reg_models = [LinearRegression(n_jobs=-1), RandomForestRegressor(n_jobs=-1, random_state=SEED), XGBRegressor(n_jobs=-1, random_state=SEED), SVR(), KNeighborsRegressor(n_jobs=-1)] \n",
    "mdatasets = [mfeatures, mfeatures_diff]\n",
    "scalers = [None, StandardScaler(), MinMaxScaler()]\n",
    "\n",
    "# run regression models\n",
    "for model in tqdm(reg_models, desc='Model', file=sys.stdout):\n",
    "    for dataset in mdatasets:\n",
    "        for scaler in scalers:\n",
    "            # run model\n",
    "            predict_point_total(estimator=model, data=dataset, models_df=pt_df, tournament='M', scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888022</td>\n",
       "      <td>0.197783</td>\n",
       "      <td>6.128866</td>\n",
       "      <td>16.346590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888082</td>\n",
       "      <td>0.197452</td>\n",
       "      <td>6.127237</td>\n",
       "      <td>16.350171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>None</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888038</td>\n",
       "      <td>0.197251</td>\n",
       "      <td>6.128447</td>\n",
       "      <td>16.352207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>0.080477</td>\n",
       "      <td>16.841529</td>\n",
       "      <td>17.506696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.127755</td>\n",
       "      <td>0.080295</td>\n",
       "      <td>17.105904</td>\n",
       "      <td>17.508898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tournament            Label  \\\n",
       "7           M  point_total_adj   \n",
       "8           M  point_total_adj   \n",
       "6           M  point_total_adj   \n",
       "19          M  point_total_adj   \n",
       "20          M  point_total_adj   \n",
       "\n",
       "                                                Model            Scaler  \\\n",
       "7   (DecisionTreeRegressor(max_features=1.0, rando...  StandardScaler()   \n",
       "8   (DecisionTreeRegressor(max_features=1.0, rando...    MinMaxScaler()   \n",
       "6   (DecisionTreeRegressor(max_features=1.0, rando...              None   \n",
       "19                                              SVR()  StandardScaler()   \n",
       "20                                              SVR()    MinMaxScaler()   \n",
       "\n",
       "    Num_Features                                           Features  \\\n",
       "7            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "8            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "6            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "19           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "20           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "\n",
       "    Num_CV_Folds  Train_R2    Val_R2  Train_RMSE   Val_RMSE  \n",
       "7              5  0.888022  0.197783    6.128866  16.346590  \n",
       "8              5  0.888082  0.197452    6.127237  16.350171  \n",
       "6              5  0.888038  0.197251    6.128447  16.352207  \n",
       "19             5  0.154495  0.080477   16.841529  17.506696  \n",
       "20             5  0.127755  0.080295   17.105904  17.508898  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect\n",
    "pt_df[(pt_df['Tournament'] == 'M') & ((pt_df['Num_Features'] == 117) | (pt_df['Num_Features'] == 217))].sort_values(by='Val_RMSE', ascending=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Women's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 100%|| 5/5 [00:36<00:00,  7.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# women's data\n",
    "wdatasets = [wfeatures, wfeatures_diff]\n",
    "\n",
    "# run regression models\n",
    "for model in tqdm(reg_models, desc='Model', file=sys.stdout):\n",
    "    for dataset in wdatasets:\n",
    "        for scaler in scalers:\n",
    "            # run model\n",
    "            predict_point_total(estimator=model, data=dataset, models_df=pt_df, tournament='W', scaler=scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>W</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>None</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.889429</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>6.374306</td>\n",
       "      <td>17.224268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>W</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.889372</td>\n",
       "      <td>0.180350</td>\n",
       "      <td>6.375929</td>\n",
       "      <td>17.228394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>W</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.889380</td>\n",
       "      <td>0.180274</td>\n",
       "      <td>6.375717</td>\n",
       "      <td>17.229490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>W</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.087234</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>18.182759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>W</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.087234</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>18.182759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tournament            Label  \\\n",
       "36          W  point_total_adj   \n",
       "37          W  point_total_adj   \n",
       "38          W  point_total_adj   \n",
       "44          W  point_total_adj   \n",
       "43          W  point_total_adj   \n",
       "\n",
       "                                                Model            Scaler  \\\n",
       "36  (DecisionTreeRegressor(max_features=1.0, rando...              None   \n",
       "37  (DecisionTreeRegressor(max_features=1.0, rando...  StandardScaler()   \n",
       "38  (DecisionTreeRegressor(max_features=1.0, rando...    MinMaxScaler()   \n",
       "44  XGBRegressor(base_score=None, booster=None, ca...    MinMaxScaler()   \n",
       "43  XGBRegressor(base_score=None, booster=None, ca...  StandardScaler()   \n",
       "\n",
       "    Num_Features                                           Features  \\\n",
       "36           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "37           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "38           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "44           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "43           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "\n",
       "    Num_CV_Folds  Train_R2    Val_R2  Train_RMSE   Val_RMSE  \n",
       "36             5  0.889429  0.180749    6.374306  17.224268  \n",
       "37             5  0.889372  0.180350    6.375929  17.228394  \n",
       "38             5  0.889380  0.180274    6.375717  17.229490  \n",
       "44             5  0.999999  0.087234    0.016169  18.182759  \n",
       "43             5  0.999999  0.087234    0.016169  18.182759  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect\n",
    "pt_df[(pt_df['Tournament'] == 'W') & ((pt_df['Num_Features'] == 117) | (pt_df['Num_Features'] == 217))].sort_values(by='Val_RMSE', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models df\n",
    "pt_df.to_csv('models/point_total_models.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "- As expected, the individual team data (non-differenced) performed better when predicting a game's point total.\n",
    "- Predicting point total resulted in a much higher R2 for the validation data, and a much lower RMSE (in proportion to the label) than predicting score differential.\n",
    "- Now, we can iterate through models to find the best predictor of point total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n",
    "In this section, we look at some of the most promising models from the \"Regression/Classification\" section and attempt to milk as much accuracy as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective functions for Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "def rf_objective_function_wrapper(data, tournament, regression, scaler=None):\n",
    "    \"\"\"\n",
    "    Wrapper function to include non-hyperparameter arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def rf_objective_function(max_samples, max_features, max_depth, min_samples_split, min_samples_leaf):\n",
    "        \"\"\"\n",
    "        Objective function for Bayesian optimization of random forest hyperparameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to integer\n",
    "        max_depth = int(max_depth)\n",
    "        min_samples_split = int(min_samples_split)\n",
    "        min_samples_leaf = int(min_samples_leaf)\n",
    "\n",
    "        # create model\n",
    "        if regression == True:\n",
    "            model = RandomForestRegressor(n_estimators=1000, max_samples=max_samples, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, \n",
    "                                        min_samples_leaf=min_samples_leaf, n_jobs=-1, random_state=SEED)\n",
    "            \n",
    "            # run model\n",
    "            predict_point_total(estimator=model, data=data, models_df=pt_df, tournament=tournament, scaler=scaler)\n",
    "\n",
    "            # return negative validation RMSE\n",
    "            return -pt_df[pt_df['Tournament'] == tournament]['Val_RMSE'].values[-1]\n",
    "\n",
    "        elif regression == False:\n",
    "            model = RandomForestClassifier(n_estimators=1000, max_samples=max_samples, max_features=max_features, max_depth=max_depth, min_samples_split=min_samples_split, \n",
    "                                        min_samples_leaf=min_samples_leaf, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "            # run model\n",
    "            predict_win(estimator=model, data=data, models_df=class_df, tournament=tournament, scaler=scaler)\n",
    "\n",
    "            # return negative validation LogLoss\n",
    "            return -pt_df[pt_df['Tournament'] == tournament]['Val_LogLoss'].values[-1]\n",
    "        \n",
    "    return rf_objective_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "def xgb_objective_function_wrapper(data, tournament, regression, scaler=None):\n",
    "    \"\"\"\n",
    "    Wrapper function to include non-hyperparameter arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def xgb_objective_function(max_depth, learning_rate, gamma, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda):\n",
    "        \"\"\"\n",
    "        Objective function for Bayesian optimization of random forest hyperparameters.\n",
    "        \"\"\"\n",
    "\n",
    "        # convert to integer\n",
    "        max_depth = int(max_depth)\n",
    "        min_samples_split = int(min_samples_split)\n",
    "        min_samples_leaf = int(min_samples_leaf)\n",
    "\n",
    "        # create model\n",
    "        if regression == True:\n",
    "            model = XGBRegressor(n_estimators=10000, max_depth=max_depth, learning_rate=learning_rate, gamma=gamma, min_child_weight=min_child_weight, subsample=subsample, \n",
    "                                    colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, n_jobs=-1, random_state=SEED)\n",
    "            \n",
    "            # run model\n",
    "            predict_point_total(estimator=model, data=data, models_df=pt_df, tournament=tournament, scaler=scaler)\n",
    "\n",
    "            # return negative validation RMSE\n",
    "            return -pt_df[pt_df['Tournament'] == tournament]['Val_RMSE'].values[-1]\n",
    "\n",
    "        elif regression == False:\n",
    "            model = XGBClassifier(n_estimators=10000, max_depth=max_depth, learning_rate=learning_rate, gamma=gamma, min_child_weight=min_child_weight, subsample=subsample, \n",
    "                                  colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, reg_lambda=reg_lambda, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "            # run model\n",
    "            predict_win(estimator=model, data=data, models_df=class_df, tournament=tournament, scaler=scaler)\n",
    "\n",
    "            # return negative validation LogLoss\n",
    "            return -pt_df[pt_df['Tournament'] == tournament]['Val_LogLoss'].values[-1]\n",
    "        \n",
    "    return xgb_objective_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888022</td>\n",
       "      <td>0.197783</td>\n",
       "      <td>6.128866</td>\n",
       "      <td>16.346590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888082</td>\n",
       "      <td>0.197452</td>\n",
       "      <td>6.127237</td>\n",
       "      <td>16.350171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>None</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.888038</td>\n",
       "      <td>0.197251</td>\n",
       "      <td>6.128447</td>\n",
       "      <td>16.352207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>0.080477</td>\n",
       "      <td>16.841529</td>\n",
       "      <td>17.506696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.127755</td>\n",
       "      <td>0.080295</td>\n",
       "      <td>17.105904</td>\n",
       "      <td>17.508898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>LinearRegression(n_jobs=-1)</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.411491</td>\n",
       "      <td>0.069910</td>\n",
       "      <td>14.050426</td>\n",
       "      <td>17.600464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>LinearRegression(n_jobs=-1)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.411340</td>\n",
       "      <td>0.069546</td>\n",
       "      <td>14.052242</td>\n",
       "      <td>17.603969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>None</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.065309</td>\n",
       "      <td>0.050728</td>\n",
       "      <td>17.707541</td>\n",
       "      <td>17.786479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>LinearRegression(n_jobs=-1)</td>\n",
       "      <td>None</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.387202</td>\n",
       "      <td>0.046446</td>\n",
       "      <td>14.335005</td>\n",
       "      <td>17.821401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>KNeighborsRegressor(n_jobs=-1)</td>\n",
       "      <td>None</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.366134</td>\n",
       "      <td>0.043283</td>\n",
       "      <td>14.580434</td>\n",
       "      <td>17.849255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.040857</td>\n",
       "      <td>0.077697</td>\n",
       "      <td>17.865185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>None</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.040857</td>\n",
       "      <td>0.077697</td>\n",
       "      <td>17.865185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.040857</td>\n",
       "      <td>0.077697</td>\n",
       "      <td>17.865185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>117</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.068305</td>\n",
       "      <td>-0.007959</td>\n",
       "      <td>17.679213</td>\n",
       "      <td>18.328691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>117</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.040939</td>\n",
       "      <td>-0.010469</td>\n",
       "      <td>17.936974</td>\n",
       "      <td>18.351548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>None</td>\n",
       "      <td>117</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043836</td>\n",
       "      <td>-0.012413</td>\n",
       "      <td>17.909877</td>\n",
       "      <td>18.369733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>KNeighborsRegressor(n_jobs=-1)</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>217</td>\n",
       "      <td>[A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.339135</td>\n",
       "      <td>-0.015198</td>\n",
       "      <td>14.889141</td>\n",
       "      <td>18.376557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>117</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.858742</td>\n",
       "      <td>-0.018123</td>\n",
       "      <td>6.883786</td>\n",
       "      <td>18.421219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>117</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.858652</td>\n",
       "      <td>-0.018242</td>\n",
       "      <td>6.885978</td>\n",
       "      <td>18.422028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>point_total_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, rando...</td>\n",
       "      <td>None</td>\n",
       "      <td>117</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.858635</td>\n",
       "      <td>-0.018429</td>\n",
       "      <td>6.886363</td>\n",
       "      <td>18.423897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tournament            Label  \\\n",
       "7           M  point_total_adj   \n",
       "8           M  point_total_adj   \n",
       "6           M  point_total_adj   \n",
       "19          M  point_total_adj   \n",
       "20          M  point_total_adj   \n",
       "2           M  point_total_adj   \n",
       "1           M  point_total_adj   \n",
       "18          M  point_total_adj   \n",
       "0           M  point_total_adj   \n",
       "24          M  point_total_adj   \n",
       "13          M  point_total_adj   \n",
       "12          M  point_total_adj   \n",
       "14          M  point_total_adj   \n",
       "22          M  point_total_adj   \n",
       "23          M  point_total_adj   \n",
       "21          M  point_total_adj   \n",
       "25          M  point_total_adj   \n",
       "10          M  point_total_adj   \n",
       "11          M  point_total_adj   \n",
       "9           M  point_total_adj   \n",
       "\n",
       "                                                Model            Scaler  \\\n",
       "7   (DecisionTreeRegressor(max_features=1.0, rando...  StandardScaler()   \n",
       "8   (DecisionTreeRegressor(max_features=1.0, rando...    MinMaxScaler()   \n",
       "6   (DecisionTreeRegressor(max_features=1.0, rando...              None   \n",
       "19                                              SVR()  StandardScaler()   \n",
       "20                                              SVR()    MinMaxScaler()   \n",
       "2                         LinearRegression(n_jobs=-1)    MinMaxScaler()   \n",
       "1                         LinearRegression(n_jobs=-1)  StandardScaler()   \n",
       "18                                              SVR()              None   \n",
       "0                         LinearRegression(n_jobs=-1)              None   \n",
       "24                     KNeighborsRegressor(n_jobs=-1)              None   \n",
       "13  XGBRegressor(base_score=None, booster=None, ca...  StandardScaler()   \n",
       "12  XGBRegressor(base_score=None, booster=None, ca...              None   \n",
       "14  XGBRegressor(base_score=None, booster=None, ca...    MinMaxScaler()   \n",
       "22                                              SVR()  StandardScaler()   \n",
       "23                                              SVR()    MinMaxScaler()   \n",
       "21                                              SVR()              None   \n",
       "25                     KNeighborsRegressor(n_jobs=-1)  StandardScaler()   \n",
       "10  (DecisionTreeRegressor(max_features=1.0, rando...  StandardScaler()   \n",
       "11  (DecisionTreeRegressor(max_features=1.0, rando...    MinMaxScaler()   \n",
       "9   (DecisionTreeRegressor(max_features=1.0, rando...              None   \n",
       "\n",
       "    Num_Features                                           Features  \\\n",
       "7            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "8            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "6            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "19           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "20           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "2            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "1            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "18           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "0            217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "24           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "13           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "12           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "14           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "22           117  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "23           117  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "21           117  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "25           217  [A_PlayIn, A_SeedNum, A_avg_Ast_against, A_avg...   \n",
       "10           117  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "11           117  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "9            117  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "\n",
       "    Num_CV_Folds  Train_R2    Val_R2  Train_RMSE   Val_RMSE  \n",
       "7              5  0.888022  0.197783    6.128866  16.346590  \n",
       "8              5  0.888082  0.197452    6.127237  16.350171  \n",
       "6              5  0.888038  0.197251    6.128447  16.352207  \n",
       "19             5  0.154495  0.080477   16.841529  17.506696  \n",
       "20             5  0.127755  0.080295   17.105904  17.508898  \n",
       "2              5  0.411491  0.069910   14.050426  17.600464  \n",
       "1              5  0.411340  0.069546   14.052242  17.603969  \n",
       "18             5  0.065309  0.050728   17.707541  17.786479  \n",
       "0              5  0.387202  0.046446   14.335005  17.821401  \n",
       "24             5  0.366134  0.043283   14.580434  17.849255  \n",
       "13             5  0.999981  0.040857    0.077697  17.865185  \n",
       "12             5  0.999981  0.040857    0.077697  17.865185  \n",
       "14             5  0.999981  0.040857    0.077697  17.865185  \n",
       "22             5  0.068305 -0.007959   17.679213  18.328691  \n",
       "23             5  0.040939 -0.010469   17.936974  18.351548  \n",
       "21             5  0.043836 -0.012413   17.909877  18.369733  \n",
       "25             5  0.339135 -0.015198   14.889141  18.376557  \n",
       "10             5  0.858742 -0.018123    6.883786  18.421219  \n",
       "11             5  0.858652 -0.018242    6.885978  18.422028  \n",
       "9              5  0.858635 -0.018429    6.886363  18.423897  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_df[(pt_df['Tournament'] == 'M') & ((pt_df['Num_Features'] == 117) | (pt_df['Num_Features'] == 217))].sort_values(by='Val_RMSE', ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | max_sa... | min_sa... | min_sa... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-16.23   \u001b[0m | \u001b[0m55.33    \u001b[0m | \u001b[0m0.8576   \u001b[0m | \u001b[0m0.8014   \u001b[0m | \u001b[0m27.7     \u001b[0m | \u001b[0m22.34    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-16.32   \u001b[0m | \u001b[0m64.94    \u001b[0m | \u001b[0m0.7188   \u001b[0m | \u001b[0m0.9459   \u001b[0m | \u001b[0m48.22    \u001b[0m | \u001b[0m20.41    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-16.32   \u001b[0m | \u001b[0m79.38    \u001b[0m | \u001b[0m0.7644   \u001b[0m | \u001b[0m0.784    \u001b[0m | \u001b[0m46.35    \u001b[0m | \u001b[0m5.41     \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-16.29   \u001b[0m | \u001b[0m9.626    \u001b[0m | \u001b[0m0.5101   \u001b[0m | \u001b[0m0.9163   \u001b[0m | \u001b[0m39.13    \u001b[0m | \u001b[0m43.76    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-16.29   \u001b[0m | \u001b[0m97.88    \u001b[0m | \u001b[0m0.8996   \u001b[0m | \u001b[0m0.7307   \u001b[0m | \u001b[0m39.25    \u001b[0m | \u001b[0m7.677    \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "# define bounds\n",
    "rf_bounds = {\n",
    "    'max_samples': (0.5, 1),\n",
    "    'max_features': (0.5, 1),\n",
    "    'max_depth': (1, 100),\n",
    "    'min_samples_split': (2, 50),\n",
    "    'min_samples_leaf': (1, 50)\n",
    "}\n",
    "\n",
    "# create optimizer\n",
    "rf_optimizer = BayesianOptimization(f=rf_objective_function_wrapper(data=mfeatures, tournament='M', regression=True, scaler=StandardScaler()), pbounds=rf_bounds, random_state=SEED)\n",
    "\n",
    "# run optimization\n",
    "rf_optimizer.maximize(init_points=10, n_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bounds\n",
    "xgb_bounds = {\n",
    "    'max_depth': (1, 10),\n",
    "    'learning_rate': (0.0001, 0.01),\n",
    "    'gamma': (0, 10),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.5, 1),\n",
    "    'colsample_bytree': (0.5, 1),\n",
    "    'reg_alpha': (0, 10),\n",
    "    'reg_lambda': (0, 10)\n",
    "}\n",
    "\n",
    "# create optimizer\n",
    "xgb_optimizer = BayesianOptimization(f=xgb_objective_function_wrapper(data=mfeatures, tournament='M', regression=True, scaler=StandardScaler()), pbounds=xgb_bounds, random_state=SEED)\n",
    "\n",
    "# run optimization\n",
    "xgb_optimizer.maximize(init_points=10, n_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': -11.789767053451914,\n",
       " 'params': {'max_depth': 64.94351719359895,\n",
       "  'max_features': 0.7187936056313462,\n",
       "  'max_samples': 0.9458865003910399,\n",
       "  'min_samples_leaf': 19.309592449519556,\n",
       "  'min_samples_split': 8.901947338863998}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get best params\n",
    "rf_optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1oAAANVCAYAAABh9I9LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhNUlEQVR4nO3deZQddZ034M/t7vSSpTuE7CEkCMi+QzCIgoDsiiwqMkZgAEVERBlQdBRwdKK8OuKGgLIoMIAO4jpGGRWQTTYjyKYoYQmEBJJ0Z+v9vn/ENGk66XRCQdPkec655/Stql/Vt+6trqrPra1ULpfLAQAAoDAV/V0AAADA642gBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAWwHrviiitSKpVyzz33rHaYWbNmpVQq5YorrlinaUyePDmlUqnrNWTIkOy888751re+lXK5vI6Vvzwr11MqldLQ0JC99947v/zlL1+V6Z977rkplUrduk2ePDnHHXfcWo1n6dKlOffcc3PTTTf16Lfiu501a9a6FwrAOhO0AOjVuHHjcscdd+SQQw5Z53G8+c1vzh133JE77rgjV155ZQYPHpyPfvSjmT59eoGVrp2jjjoqd9xxR2677bZ8+9vfzpw5c/KOd7zjVQtbL3XDDTfks5/97Fq1Wbp0ac4777xVBq1DDjkkd9xxR8aNG1dQhQCsjar+LgCA17aampq86U1velnjGD58eLdx7Lffftl4441z8cUX59Of/vTLLXGdjBkzpqumPfbYI1OnTs1mm22WCy64YLWhsq2tLaVSKVVVxW8+d9ppp0LHN2rUqIwaNarQcQLQd45oAdCrVZ06uOLUtwcffDDve9/70tDQkDFjxuRf//Vf09jYuMZx1tfX541vfGOee+65bt1bW1vzhS98IVtuuWVqamoyatSoHH/88Zk3b1634VpaWnLGGWdk7NixGTx4cN761rfm3nvvXafT71bYdNNNM2rUqDzxxBNJkptuuimlUilXXnllzjjjjEyYMCE1NTV57LHHkiT/93//l3333Tf19fUZPHhw3vzmN+e3v/1tj/H+8pe/zI477piamppssskm+cpXvrLK6a+q9oULF+aMM87IG97whtTU1GT06NE5+OCD88gjj2TWrFldQeq8887rOg1yxThWd+rgZZddlh122CG1tbUZMWJEDj/88Dz88MPdhjnuuOMydOjQPPbYYzn44IMzdOjQTJw4MWeccUZaWlq6Dfud73wnO+ywQ4YOHZphw4Zlyy237LfwDPBa4ogWAOvsyCOPzHvf+96ccMIJeeCBB3L22WcnWb4z35v29vY89dRTeeMb39jVrbOzM4cddlj+8Ic/5Kyzzsoee+yRJ554Iuecc0723nvv3HPPPamrq0uSHH/88bnuuuty1llnZZ999slDDz2Uww8/PE1NTes8LwsWLMgLL7yQzTffvFv3s88+O1OnTs1FF12UioqKjB49OldddVU+8IEP5LDDDsv3v//9DBo0KBdffHEOOOCA/PrXv86+++6bJPntb3+bww47LFOnTs21116bjo6OnH/++T0C5qosWrQoe+65Z2bNmpVPfvKT2X333bN48eLccsstefbZZ7PHHntkxowZOfDAA3PCCSfkxBNPTJJej2JNnz49n/70p/O+970v06dPzwsvvJBzzz03U6dOzd13391t3tva2vLOd74zJ5xwQs4444zccsst+Y//+I80NDTkc5/7XJLk2muvzSmnnJKPfvSj+cpXvpKKioo89thjeeihh9b68wd43SkDsN66/PLLy0nKd99992qHefzxx8tJypdffnlXt3POOaecpHz++ed3G/aUU04p19bWljs7O7u6TZo0qXzwwQeX29raym1tbeUnnniifNJJJ5UHDRpU/sUvftE13DXXXFNOUr7++uu7jfPuu+8uJylfeOGF5XK5XH7wwQfLScqf/OQnuw23ov2xxx67xvlOUj7llFPKbW1t5dbW1vLDDz9cPuigg8pJyt/+9rfL5XK5/Pvf/76cpPzWt761W9slS5aUR4wYUX7HO97RrXtHR0d5hx12KE+ZMqWr2+67714eP358edmyZV3dmpqayiNGjCi/dBM8adKkbrV//vOfLycp33jjjaudj3nz5pWTlM8555we/VZ8t48//ni5XC6XFyxYUK6rqysffPDB3YZ78sknyzU1NeVjjjmmq9uxxx5bTlL+4Q9/2G3Ygw8+uLzFFlt0vT/11FPLw4cPX219AOszpw6uhVtuuSXveMc7Mn78+JRKpfzkJz9Zq/bNzc057rjjst1226Wqqirvete7egyz4lSVl74eeeSRYmYCoEDvfOc7u73ffvvt09zcnLlz53br/r//+78ZNGhQBg0alEmTJuW73/1uvvnNb3a7FuoXv/hFhg8fnne84x1pb2/veu24444ZO3Zs1w0fbr755iTJe97znm7TOOqoo9bq2qkLL7wwgwYNSnV1dbbaaqvcfvvt+fznP59TTjml23BHHnlkt/e333575s+fn2OPPbZbnZ2dnTnwwANz9913Z8mSJVmyZEnuvvvuHHHEEamtre1qP2zYsLzjHe9YY32/+tWv8sY3vjH77bdfn+epN3fccUeWLVvW4/TEiRMnZp999ulx2mOpVOpR5/bbb991amWSTJkyJQsXLsz73ve+/PSnP83zzz9fSK0ArwdOHVwLS5YsyQ477JDjjz++x4a3Lzo6OlJXV5fTTjst119/fa/DPvroo6mvr+9674Jm4LVoww037Pa+pqYmSbJs2bJu3ffcc8987WtfS0dHR/72t7/ls5/9bE499dRss8022XPPPZMkzz33XBYuXJjq6upVTmvFTvwLL7yQZPnNLFZWVVXVo57evOc978mZZ56ZUqmUYcOGZdNNN01lZWWP4V56174Vp/0dddRRqx33/PnzUyqV0tnZmbFjx/bov6puLzVv3rxsvPHGaxyur1Z8bqu6C+H48eNz4403dus2ePDgbgExWf79Njc3d72fNm1a2tvb893vfjdHHnlkOjs7s9tuu+ULX/hC3v72txdWO8BAJGithYMOOigHHXTQavu3trbm3//933P11Vdn4cKF2XbbbfPlL385e++9d5JkyJAh+c53vpMkue2227Jw4cLVjmv06NEZPnx4gdUD9J+GhobsuuuuSZLdd989u+++e3bYYYeccsopmTlzZioqKjJy5MhsuOGGmTFjxirHMWzYsCQvhrvnnnsuEyZM6Orf3t7eFSb6YtSoUV019ealz7saOXJkkuSb3/zmau/GOGbMmK47FM6ZM6dH/1V1W1V9Tz/99BqH66sVn9uzzz7bo98zzzzTNV9r6/jjj8/xxx+fJUuW5JZbbsk555yTQw89NH/9618zadKkl1UzwEDm1MECHX/88bntttty7bXX5v7778+73/3uHHjggfnb3/621uPaaaedMm7cuOy77775/e9//wpUC9B/Nt9885x11ll54IEHct111yVJDj300Lzwwgvp6OjIrrvu2uO1xRZbJEne+ta3JklXuxX+53/+J+3t7a947W9+85szfPjwPPTQQ6usc9ddd011dXWGDBmSKVOm5Mc//nG3o0CLFi3Kz3/+8zVO56CDDspf//rX/O53v1vtMKs7grgqU6dOTV1dXa666qpu3Z9++un87ne/67qBx7oaMmRIDjrooHzmM59Ja2trHnzwwZc1PoCBzhGtgvz973/PNddck6effjrjx49Pkvzbv/1bZsyYkcsvvzz/+Z//2afxjBs3Lpdcckl22WWXtLS05Morr8y+++6bm266qWvnAqBov/vd73rcBjxJDj744Fdsmv/2b/+Wiy66KOedd17e85735Oijj87VV1+dgw8+OB/72McyZcqUDBo0KE8//XR+//vf57DDDsvhhx+ebbbZJu973/vy1a9+NZWVldlnn33y4IMP5qtf/WoaGhpSUfHK/oY4dOjQfPOb38yxxx6b+fPn56ijjsro0aMzb968/PnPf868efO6zl74j//4jxx44IF5+9vfnjPOOCMdHR358pe/nCFDhmT+/Pm9Tuf000/Pddddl8MOOyyf+tSnMmXKlCxbtiw333xzDj300LztbW/LsGHDMmnSpPz0pz/NvvvumxEjRmTkyJGZPHlyj/ENHz48n/3sZ/PpT386H/jAB/K+970vL7zwQs4777zU1tbmnHPOWevP4qSTTkpdXV3e/OY3Z9y4cZkzZ06mT5+ehoaG7Lbbbms9PoDXE0GrIPfdd1/K5XK3WxUny5/1sjbXDGyxxRZdv9omy3+BfOqpp/KVr3xF0AJeMZ/85CdX2f3xxx9/xaY5dOjQfO5zn8tHPvKRXH311fnABz6Qn/3sZ/n617+eK6+8MtOnT09VVVU22mij7LXXXtluu+262l5++eUZN25cLr300nzta1/LjjvumB/+8Ic58MADX5XTrt///vdn4403zvnnn58PfehDWbRoUUaPHp0dd9yx280m3v72t+cnP/lJ/v3f/z3vfe97M3bs2JxyyilZtmxZzjvvvF6nMWzYsNx6660599xzc8kll+S8887LBhtskN122y0f/OAHu4a79NJLc+aZZ+ad73xnWlpacuyxx3Z75tnKzj777IwePTrf+MY3ct1116Wuri577713/vM//7PHbe374i1veUuuuOKK/PCHP8yCBQsycuTI7LnnnvnBD37g2mJgvVcql8vl/i5iICqVSrnhhhu67hx43XXX5V/+5V/y4IMP9riYeujQoT0ufD7uuOOycOHCPt258Itf/GKuuuqqHg+UBOBFt99+e9785jfn6quvzjHHHNPf5QCwnnNEqyA77bRTOjo6Mnfu3LzlLW8pdNx/+tOfVnmXKID11Y033pg77rgju+yyS+rq6vLnP/85X/rSl7L55pvniCOO6O/yAEDQWhuLFy/OY4891vX+8ccfz8yZMzNixIi88Y1vzL/8y7/kAx/4QL761a9mp512yvPPP5/f/e532W677bquc3jooYfS2tqa+fPnZ9GiRZk5c2aSZMcdd0ySXHDBBZk8eXK22WabtLa25qqrrsr111+/xtvBA6xP6uvr85vf/CYXXHBBFi1alJEjR+aggw7K9OnTe9ySHAD6g1MH18JNN92Ut73tbT26rzgfvq2tLV/4whfygx/8ILNnz86GG26YqVOn5rzzzuu6tmDy5MndHva4woqv4fzzz88ll1yS2bNnp66uLttss03OPvvsV/SCdAAAoFiCFgAAQME8RwsAAKBgghYAAEDB3AxjDTo7O/PMM89k2LBhKZVK/V0OAADQT8rlchYtWpTx48enoqL3Y1aC1ho888wzmThxYn+XAQAAvEY89dRT2WijjXodRtBag2HDhiVZ/mHW19f3czUAAEB/aWpqysSJE7syQm8ErTVYcbpgfX29oAUAAPTpkiI3wwAAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgVf1dAH3T2VnO7IXLsqS1PUOqqzJheF0qKkr9XVa/WpfPpLc27e2due+pBXlhSWs2HFKdnSdukKqqinWe1prG2Vu/orW2duQ3j8zJnMaWjG2oyf5bjk11deUa52117XobX3Nze66778nMXtCcCRvU5r07b5za2qo1znPT4pZ8/lcP5en5zdloRG0+d9DWqR9as8b6Vze95xuX5iPX/inPNrZkXENNvn30ThnZMDhJsnRpWy65/e9d0/rgHptm8OBBa6y/t3EuXNScs3/6QFe76Ydtl+HDatfY7rHn5ueo79yVxa0dGVpdmf/58JRsNmZE/jF3Qd590V1pamlPfU1VfnTylLxh9AZJkiefb8zRl9yVBcvas0FdVa794JRsPLIhSfLQM/Ny5IV3pbk9qa1Krj9lSrYeP6rXaSXJXbNm5+iLZqYzy3+Bu/bkHTNl8oQkydMvNOWY792V+UvbM2JwVf77xCnZaMP6Xtvc//RzOerCe9LamVRXJP9zyq7ZfqMxSZL7nnw277nwvrRn+Uboh6fsnJ03HpckmfnUnLz7wnvTVk4GlZIfnbJLdpw4do3tfvPI3/PBKx7pWuYvOW7L7L/lprn170/m/d99oKv7VSdtlz033XiN47v9H0/lmEvu72r33x/cPnu8YWKS5BcP/jWnXvm3rn7fmrZ5Dt3mjUmSH9//cD7x3//o6nfYDvUZU79hJmxQm5q6pfnUdU909du8Ptlo3MhUdCzNbx9b2tX9omO3yHMLKrotg9XVlZm9cFmeXbQkl/9hVuYvbku5uTF3zensavdvbx+dF5bWpmlpe1oWzc8vHmvu6rfPZkNTOWhwNhpRm+0mVXWr8aQ9RmbMBqMytqEmFVVLc8oP/trV78Q9R2Zsw/J+Q+vactylD/b4THpbDu58/Om87+I/p5yklOSaD+2QN22yUZLkL7Pn5sgL705LR1JTmVx/ym7ZdsLoNfa754ln8t7v/CkdSSqTXPfhnbLrpPG9trvpb7O61X7FCdtk780nr3FavbXr7X/tkWefz5EX3pVlbeXUDSrl+lOmZMtxI5Osfhnv7f+6t/VBb/16W9fNb1qWj//Pn/PMwuaMH16brx21Q0bU1yVJ5ixYnOO/f0/mLmrN6GHVufzYXTN2g6HrvJ7urY7e+vU2vd7W1avr11uNRW8n19RuXfYBemvTWx3rOs7XioG8D1wql8vl/i5ibVx44YX5f//v/+XZZ5/NNttskwsuuCBvectbVjv8zTffnE984hN58MEHM378+Jx11lk5+eST+zy9pqamNDQ0pLGxMfX19UXMwlp7bO6i/Povz+Xv8xanub0jtVWV2XTU0Byw7ZhsNnpYv9TU39blM+mtzRMvLM0Vt83KrBeWpK2jM4MqKzJ5wyE57s2TM2nDwev0+f/24edWO84kq+2371ZjCv2srrxjVr73h8czb1FzOsrlVJZKGTWsNie+ZZNM3XTD1c7bHX9/YZXtttuoIQ883bjK8c1d1JIf3P5EFre0pbOcVJSSoTWD8oE9JmXHicNXO88/uGNWbvnr81l5ZVRK8tY3jsx+W41Zbf2rm16pVM7CZe09PovNRg3O7m8Ymf+55+m0dLy4g1pTWZGjdt0oI4ZUr7b+Xz3wTB6bt3SV4xw+uDr3PLGwR79dJw3PwqWtq2335Pylae3o+3dZW1XKoMqKLGrp2WhYTWWWtnRkVaOrTFJZmVVOq3o13VcYXle1ys+yN6Ukq9qwrK47q1bK8mVwcHVlth1fn8fmLcm8xa39XVYhKpJ0rqZ7eum3qu597b+2dfQ2rspktf9rpVLSvooFvWo13XszrKYybR2daV5Fw9qq5Tuaq+t35C4TV7uum/nUgjz4zKIe7bYZPyzzF7fk2aaey9mgiqS9M2u9nn5kzqLV1pFktf2eXrB0tduF7Tcavtp1dZJV9ttx44bMen7pKmtMUuh2ck3tetvnWN0+QG/7FM8sXLbaOqZNnbzK8a1pnEXvi6yr1+I+8NpkgwEVtK677rpMmzYtF154Yd785jfn4osvzve+97089NBD2XjjjXsM//jjj2fbbbfNSSedlA996EO57bbbcsopp+Saa67JkUce2adp9nfQemzuolx+26zMX9KacQ21GVxdlaWt7Xm2sTkjhlTn+DdPXu/C1rp8Jr21WdrakcefX5KW9o5sOKQ6ddWVWdbakReWtKamqjKbjBySwdWVa/X5//bh5zL9V49kUXNbj3FWlJZvHDvL5R79htUOytkHbVnYCu7KO2bl//360bS0d2RwdVVqqkppaS9naWt7BlVWZsuxwzJqWE2PeZu3qCWPzFmUto7u7ZqWtaejXE5VKRlWN6jb+MoppaOjM51JqitKqaxIOjqT1s7lK/2htVWpqaroMc+Ny9rS3Lb6XZrKiqSqotSj/tVNr6Wjb6u0iizfISqXX9yhqigt34l4af0DZy3J6011RdK6tukBVrKqdV3RKktJVWXP9XRnOWn75zq5tzrWpcZSqee6upQk5aRcWvV2oSJJw+Du267S8lYpp7ya7eTQjBpWu4rtZHMembO4x3ZyaWt7qioqsvX4+owc2nP72ts+x+r2AXrbp+jsLGdJS0c6yp096qipqsyZB2yxyrDV2ziL3hdZV6/VfeC1yQavrWODa/Bf//VfOeGEE3LiiSdmq622ygUXXJCJEyfmO9/5ziqHv+iii7LxxhvnggsuyFZbbZUTTzwx//qv/5qvfOUrr3Ll66azs5xf/+W5zF/Sms1HD82w2kGprChlWO2gbD56aOYvac1vHnwunZ3rz17gunwmvbXZdOSQ/GV2Y15Y0pKJw2szrHZQqioqMqx2UCYOr80LS1ry4OzGbDpySJ8///b2zlxx26wsam7LxhvUdRvnRg01eX5xS15Y3JKJw7v323iDuixqbsv3b5+V9vaXvzlsbe3I9/7weFraOzJi8KAMrq5MZUVFBldXZoO6QVnW2p6HnmnMGzYc3G3e3rDh4Dz4TGOWtbZng7oX29VWVWTF7zKd5aS2qqJrfMNrq9La3pmOclJbmVT/s191VUXqqkpp7yxn4dK2TKiv7jbPY4dU9hqykuUbyfrqim71r256FaW+/y9UVZZSWVFKVWXpxV/RV1F/9cA4O4HXKSGLl2vldd0rpaOcNNRU9lhPrxyyVq5j5Uoq17HG6lL3dXVt5fJ1eGe6r8cHrXQaXGeWnx66osaGmso0t3emub2z2/ZucHVlRgxevp188JmmvGHDupdsJ+vy4DNNWdbavorta1Wa2zry4OymHtvX3vY5VrcP0Ns+xfL9lNYsa+vI8NqqHvW3tHfk0lsfT+tLTlvobZxF74usq9fLPvCAuUartbU19957bz71qU91677//vvn9ttvX2WbO+64I/vvv3+3bgcccEAuvfTStLW1ZdCgQT3atLS0pKWlpet9U1NTAdWvm9kLl+Xv8xZnXENtSv88CnLT/1yem398eZJ//vJTLuc/aqpS+ZJzVXfeeef87Gc/69btne98Z+677741TvcTn/hEPvGJT3S9X7RoUbbaaqs+1fzTn/40u+yyS9f7X/ziF306VXPo0KF55JFHunU788wzc80113Tr1tFZzuKW9lSUSvnnR5Ktpuyd95z++ZRKpYxrqM1jcxdnx513yfzn5662zQpvPuZjaRm+UypLpbR3JkvmPZGbLzgtyfIjG+0d5STlPDqossdnfNJXrs1jc5dfNzFxxOBccskl+fznP5/Wjs40Lm1LqVTKAy+ZXmc5KQ0flwnH/GeWtHWkvnL5BuDOS8/NvL/9KZ3lZGa5nBn/PijVlT1/BznppJNyzjnndOu20UYbrfIzbW7rSOOy5XXMSrL5ez+Vhjfs+M95K6X5yfvzxM+/mvO+Xdlt49beUU5z+/KV8lOl5RvEXc++Ns1tnV2/NM6/9b/z9P2/6fo8O8vpCmErf8ZDJm2Xce86KytONnl+SXsevfzjWfTcU8un1Vle7Uqy4c3vy7AdD0ySLG4tZ2jzC7n/2x/pdXorjjyNOfqLGbThi5/LkoduyoLfX95t/KtqVzl0g2x8/NdTtdJp7c/+77ey9O93r7LGlQ3Zeq9s8LZ/7dZt9ndPTrl12RrbjjjgIxm82ZSu9y1zHsu86/9jje2SZPyJ30lFzeCu90133ZCmu3+yxnbVYzfN6CM/163b3Os/n9Y5f19j2/rd3pX6KYd3ve9sWZpnvvfhPtU76sjPpmbsZl3vlz52V+b/+ttrbFeqrsuEky7q1m3B7y/LkoduXmPbuk13y4YHntqt27PfPz0dixesse0Gbzs+Q7beu+t92wtP57lrP7PGdkky9tivpWroiK73i2bOSONt1/TSYrmqERMy9n3/2a3bvJ//v7Q8+Zc1th26wwEZvucx3bo9/e1j+1TvyHeckdqNt+963/zk/Xn+51/tU9uNPvL9bu8X3vrfWfznX6+xXc3G22bUO87s1m3ONZ9O+/zZa2y78joiSdoXz8+c73+8T/X2ZR2xKpVDN8i4Yy/o1u2FGd/KstfwOmLyhy56VdYRs0vpOmNj/J5HZeiuh3X1K7cuzeMrrSNeepbAyuvjkUf0fR2xcrvSoLqM/+c6oqNcSlWSOTd+Lwv/clO36c0ulbJic75iP6pu092ywREfz+CV1v33f/sjaV00P+VktdvJUpIc+MGM3XnfF+ud93SeuOysZBXtOjrLaW7rSFLKPypLefvZl6auYfm1exUVFVny5xm5bsb3u+0DrGqfYtiYidn749/KwmXLjxqWkjz2oy9l8awXr0dNlm95/1EuZ/xXBqV20PKZO+mkk3LQB07NrBeWZMMh1amoqMjPP3VYt3ar2xe56qqrsvfee3e9v+mmm/L+97+/5xezCk8//XS39+edd16++93vrnb4Fftum2y3W974mf/q1m/l/b0V+2CvVQMmaD3//PPp6OjImDHdD2OOGTMmc+bMWWWbOXPmrHL49vb2PP/88xk3blyPNtOnT895551XXOEvw5LW9jS3d2RwdV1Xt+ali9P4/HPdhlu8irYTJ07s0W3evHmZPXvNG6+XhstyudyndsnyQLyyZcuW9antsGE9D/0uWLCgT22XLVrY9XdddWWea2rO3LnP5blnn1lj26VLlqazvpyqqlI6yuWUOzuybOG8HsOt6jOurkwWtndkSevy61cWL17cp3oHDapLuZy0r3ROesuShd2mO281+b6xsbFHt75+N53tbV1/l1NOOtvSseiFLOmlzcq/gXWstKXqbFmStkXPr3GaHUubum3gWjs609y0YJWf8UutvPPRXi6nXO5Ma9Oap5kk5c7uv951trWkY/ELfWv7kj2AjuZFfWrb2dxzKelY/EKfdqLK7S+5HqKjrc/19qijdWmf2nYsHbmKbo19m9fWntec9bnejrZub8vtrX1qW1ppPdhVR/PiPn43Pa9F6Vi8oG9t21q6vS93dvR9Xju7/xpcbl3Wp7YVNUN6jmppU9/qben5H93nZb/9pd/Ny1gOW5b0rd6lPVd2nUsW9qltj/+tzs6+z+vLWEe8VOdrfB3x0p+yXo11RHvLkrSv9CNauZy0L3r11hGd/1yPdzQv7tN0O5sXpeMlP/q1LV7Q1ba37WRnW3O39+XOjnT0oV17kvJL1hEVHS1pbXp+tfsAKwyqG5pk+Ta1q94ljavdRr6w0iqwsbExLyxpTVtHZ+r+ebOM1W2TX1rHygciVrzv6z7ISzU2NvapbctK+3grW7G/t2If7LVqwAStFUovOSRRLpd7dFvT8KvqvsLZZ5/d7WhOU1PTKkPLq2FIdVVqqyqztLU9w2qXH32rHTw0DSOXh8cVv8QMXcURrVGjRvUY36hRozJhwoQ1Tvel55uWSqU+tUuS6urqbu/r6ur61Hbo0KE9um2wwQY92q7q6FTdsOFd/Ze1dqSmqjKjR49J1T8/k96OaA0eMjgVFaV0lrP8qFZFZeqGL//sVj6iVbuKI1qtHUlNbWWGVFd1zcOECRO6/fr00pvidJaT0pDhKZWSqpV+JaoZMjx1w0d1HalpGLzqI1oNDQ09uq3u8135iFYpSUXVi0dwSyklFYNSOWzD1Fat/ohWRenFUzwqV/rwKmqGZNCwkWs8olU5uL7b++rKitTWb5C2Zct3OHo7orXyRrOqVEqpVJHq+pG9Tm9FRipVdL/TUsWgmlQO3bD7+FdzROul64bK2mE92q5KRW3PZbhy6IZ92okqVXX/v0nloD5Nc5V1VA/uU9vKwT2XpcrBDX2b1+qevx72ud7K7mcSlKqq+9R2VUGronZoH7+bnj/kVA7dYI3tkuXLTrc6Kir7Pq8V3f+HS9V1fat3yPCe3QbX963tKkJaX+stVb30u3kZy2HNkL7VO7jn9Q0VQ4anchWB8aV6LBMVFX2f1z6sI1ZlVctNxWt8HfHSvZ1Xah1RWumIVlXNkK5t8Ip+VcNebNfbEa21WUe89IjWCivqqKwdmqphG3ab3srb5hX7URW1w3ps4wcN3SDlcjnlZLXbyVKSikG13WuqqEzlP+f1pe1WPqJVVVlK6SXriM7KmlTXj+y2D7CqfYra+uXL4cr7CYOGNHRtI1co55/7FHUvHtFqaGjIhkOqM6iyIstaOzKstqJrv6erjtXsi9TUdF8f1tTU9Hn/8KUaGhp6bbti361mpX28la3Y31uxD/ZaNWBuhtHa2prBgwfnRz/6UQ4//MVTVj72sY9l5syZufnmnqePvPWtb81OO+2Ur3/9613dbrjhhrznPe/J0qVLV3nq4Ev1580wOjvL+c5Nf89fnmnM5qOHdtsBLJfL+dvcxdluQkNO3mvTAXOby5drXT6T3tp0dHTmmrufSnN7R7YaMzSVlZUr9evIw88tTl1VZY7ebWIqV1rZ9Pb5t7d35vgr7s5f5y7KxhvUpWKlFWl7e3vuf2ZRSkm2n9DQbZydnZ15csGybDFmWC47dreXfXvV1taOvP2CWzKnaVlGDB7UrY6Ojs7MW9yamqqKHL/H5FStdK5ce3tHLrt9VlrbOzNqaHVXjZ2d5TzX1NJ1C+cx9TUr3aa2I88tXv4rZF1VUrXy59jZmaVty1czO280rNv/XXNzc/78bM+jIy81anBlqqtfbLe66bV3dKSvN8irfslGc8Xvgi+tv7WtIy2ukwEGqJXXda19vFnQuhg9pCqDBr2407vyenrFNVortHWUu460Lb8r6trXWFORVA9aadu10vp/5fV4Ocnile7UOnbYoK5tfVtbe+Yuaf9n9+oe2+S5i1pTXVWRf91jUqqqVp639lx2+xNpbe/M6GHVL9m+dmTe4rbUVFXm+D0mddu+9rbPsbp9gN72KTo6OnLfU40pl5PRQwd1m1ZnZ2fmL23LuIa6/OZjb+12q/fexln0vsi6ei3vA78ub4ZRXV2dXXbZJTfeeGO37jfeeGP22GOPVbaZOnVqj+F/85vfZNddd+1TyOpvFRWlHLDtmIwYUp2/zV2cRc1tae/szKLmtvxt7uKMGFKd/bcZs96ErGTdPpPe2vz9+SXZbkJDNhxSk6cWNnfr99TC5owcUpNtJjTk788v6fPnX1VVkePePDnDagflyQXLurV7urElo4bWZMOhNXlqYfd+Ty5YlvraQTl2j8mFrNiqqytz4ls2SU1VZeYvbcvS1o7loae1IwuWtaWuuipbj2/IP15Y2q2Of7ywNNuOb0hddVUWLHuxXXN7Z9eKrqKUNLd3do1vYXP78guPS0lzR5bfqKKzM63tnVnWXk5VRSnDBw/K7KbWbtOas6QjtYN6n9fKiqSptbNb/aubXme57/8L7R3ldHSWu4WsilXU3zogfori9ap6wGylea1aeV33SqksJY0tHT3W04P+GaA6X1LHypV0rGONreXu6+rmjuXr8Ip0X4+3rXRDh4okLR3pqrGxpSO1Vctv9rTy9m5pa0fmL12+ndxmfH3+8cKyl2wnl2Wb8fWpq65axfa1PXWDKrPNhPoe29fe9jlWtw/Q2z7FUwubl98xcFBlFja396i/tqoyJ+y5SY/nafU2zqL3RdbV62UfeMAc0UpevL37RRddlKlTp+aSSy7Jd7/73Tz44IOZNGlSzj777MyePTs/+MEPkrx4e/cPfehDOemkk3LHHXfk5JNPHlC3d0+6P0OgpX35odLNRg/N/tt4jtbafCa9tVnVMy02GTkkx+7R/Tlaa/P5r+r5FCvGmfR8jtaKfq/Gc7RG19fmhD27P+fjpfO2queDjK6vzbYTej5Ha8X4VvVcq2G1gzJt6qqfo7Vintf2OVprml6yls/RqqrIUbus+jlaK+r3HK2+K8VztIpQiudo9aVNX/uvbR2vy+do/XNd92o8R2vFenqVz9H6Zx3JKp6j9c9+a/scrRXr6qTnc7SG1Q7KDhN7PkdrRY1Jz+dovZzt5Jra9bbPsTbP0VrRZlXP0VpRx9o+R+uV2hdZV6/FfeDX7XO0kuUPLD7//PPz7LPPZtttt83Xvva1vPWtb02SHHfccZk1a1ZuuummruFvvvnmfPzjH+96YPEnP/nJAffA4mRgPxX7lbIun8m6PqV9XT//3sb5aj6Nvegn3vc2vubm9lx335OZvaA5EzaozXt33ji1tVVrnOemxS35/K8eytPzm7PRiNp87qCtUz+0Zo31r256zzcuzUeu/VOebWzJuIaafPvonTKyYfm1RUuXtuWS2//eNa0P7rFpBg8etMb6exvnwkXNOfunD3S1m37Ydhk+rHaN7R57bn6O+s5dWdzakaHVlfmfD0/JZmNG5B9zF+TdF92Vppb21NdU5UcnT8kbRi8/L//J5xtz9CV3ZcGy9mxQV5VrPzglG49cfj3FQ8/My5EX3pXm9qS2Krn+lCnZevyoXqeVJHfNmp2jL5rZdWrotSfvmCmTl58///QLTTnme3dl/tL2jBhclf8+cUo22rC+1zb3P/1cjrrwnrR2Lj8q8z+n7JrtN1q+4b7vyWfzngvvS3uWXyj8w1N2zs4bL7850cyn5uTdF96btnIyqJT86JRdsuPEsWts95tH/p4PXvHinUsvOW7L7L/lprn170/m/d998U5cV520XfbcdOM1ju/2fzyVYy65v6vdf39w++zxhuXX6/7iwb/m1Cv/1tXvW9M2z6HbvDFJ8uP7H84n/vsfXf0O26E+Y+o3zIQNalNTtzSfuu6Jrn6b1ycbjRuZio6l+e1jL4bxi47dIs8tqOi2DFZXL7/D6bOLluTyP8zK/MVtKTc35q45L+6g/tvbR+eFpbVpWtqelkXz84vHXrxIf5/NhqZy0OBsNKI2202q6lbjSXuMzJgNRmVsQ00qqpbmlB/8tavfiXuOzNiG5f2G1rXluEsf7PGZ9LYc3Pn403nfxX9OOct3kq/50A550ybLd7D/Mntujrzw7rR0LL/l9vWn7JZtJ4xeY797nngm7/3On9KR5cHmug/vlF0nje+13U1/m9Wt9itO2CZ7bz55jdPqrV1v/2uPPPt8jrzwrixrK6duUCnXnzIlW45bfv3M6pbx3v6ve1sf9Navt3Xd/KZl+fj//DnPLGzO+OG1+dpRO2RE/fLrnOYsWJzjv39P5i5qzehh1bn82F0zdoOh67ye7q2O3vr1Nr3e1tWr69dbjUVvJ9fUbl32AXpr01sd6zrO14rX2j7w6zpovdpeK0ELAADoX6/La7QAAAAGCkELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBkzQWrBgQaZNm5aGhoY0NDRk2rRpWbhw4WqHb2tryyc/+clst912GTJkSMaPH58PfOADeeaZZ169ogEAgPXSgAlaxxxzTGbOnJkZM2ZkxowZmTlzZqZNm7ba4ZcuXZr77rsvn/3sZ3Pfffflxz/+cf7617/mne9856tYNQAAsD4qlcvlcn8XsSYPP/xwtt5669x5553ZfffdkyR33nlnpk6dmkceeSRbbLFFn8Zz9913Z8qUKXniiSey8cYb96lNU1NTGhoa0tjYmPr6+nWeBwAAYGBbm2wwII5o3XHHHWloaOgKWUnypje9KQ0NDbn99tv7PJ7GxsaUSqUMHz58tcO0tLSkqamp2wsAAGBtDIigNWfOnIwePbpH99GjR2fOnDl9Gkdzc3M+9alP5Zhjjuk1fU6fPr3rOrCGhoZMnDhxnesGAADWT/0atM4999yUSqVeX/fcc0+SpFQq9WhfLpdX2f2l2tracvTRR6ezszMXXnhhr8OeffbZaWxs7Ho99dRT6zZzAADAequqPyd+6qmn5uijj+51mMmTJ+f+++/Pc88916PfvHnzMmbMmF7bt7W15T3veU8ef/zx/O53v1vjuZQ1NTWpqalZc/EAAACr0a9Ba+TIkRk5cuQah5s6dWoaGxtz1113ZcqUKUmSP/7xj2lsbMwee+yx2nYrQtbf/va3/P73v8+GG25YWO0AAACrMyCu0dpqq61y4IEH5qSTTsqdd96ZO++8MyeddFIOPfTQbncc3HLLLXPDDTckSdrb23PUUUflnnvuydVXX52Ojo7MmTMnc+bMSWtra3/NCgAAsB4YEEErSa6++upst9122X///bP//vtn++23z5VXXtltmEcffTSNjY1Jkqeffjo/+9nP8vTTT2fHHXfMuHHjul5rc6dCAACAtTUgnqPVnzxHCwAASF6Hz9ECAAAYSAQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKNmCC1oIFCzJt2rQ0NDSkoaEh06ZNy8KFC/vc/kMf+lBKpVIuuOCCV6xGAACAZAAFrWOOOSYzZ87MjBkzMmPGjMycOTPTpk3rU9uf/OQn+eMf/5jx48e/wlUCAAAkVf1dQF88/PDDmTFjRu68887svvvuSZLvfve7mTp1ah599NFsscUWq207e/bsnHrqqfn1r3+dQw455NUqGQAAWI8NiCNad9xxRxoaGrpCVpK86U1vSkNDQ26//fbVtuvs7My0adNy5plnZptttunTtFpaWtLU1NTtBQAAsDYGRNCaM2dORo8e3aP76NGjM2fOnNW2+/KXv5yqqqqcdtppfZ7W9OnTu64Da2hoyMSJE9epZgAAYP3Vr0Hr3HPPTalU6vV1zz33JElKpVKP9uVyeZXdk+Tee+/N17/+9VxxxRWrHWZVzj777DQ2Nna9nnrqqXWbOQAAYL3Vr9donXrqqTn66KN7HWby5Mm5//7789xzz/XoN2/evIwZM2aV7f7whz9k7ty52Xjjjbu6dXR05IwzzsgFF1yQWbNmrbJdTU1Nampq+j4TAAAAL9GvQWvkyJEZOXLkGoebOnVqGhsbc9ddd2XKlClJkj/+8Y9pbGzMHnvssco206ZNy3777det2wEHHJBp06bl+OOPf/nFAwAArMaAuOvgVlttlQMPPDAnnXRSLr744iTJBz/4wRx66KHd7ji45ZZbZvr06Tn88MOz4YYbZsMNN+w2nkGDBmXs2LG93qUQAADg5RoQN8NIkquvvjrbbbdd9t9//+y///7Zfvvtc+WVV3Yb5tFHH01jY2M/VQgAALBcqVwul/u7iNeypqamNDQ0pLGxMfX19f1dDgAA0E/WJhsMmCNaAAAAA4WgBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCVfV1wPvvv7/PI91+++3XqRgAAIDXgz4HrR133DGlUinlcjmlUqnXYTs6Ol52YQAAAANVn08dfPzxx/OPf/wjjz/+eK6//vpssskmufDCC/OnP/0pf/rTn3LhhRdm0003zfXXX/9K1gsAAPCa1+cjWpMmTer6+93vfne+8Y1v5OCDD+7qtv3222fixIn57Gc/m3e9612FFgkAADCQrNPNMB544IFssskmPbpvsskmeeihh152UQAAAAPZOgWtrbbaKl/4whfS3Nzc1a2lpSVf+MIXstVWWxVWHAAAwEDU51MHV3bRRRflHe94RyZOnJgddtghSfLnP/85pVIpv/jFLwotEAAAYKAplcvl8ro0XLp0aa666qo88sgjKZfL2XrrrXPMMcdkyJAhRdfYr5qamtLQ0JDGxsbU19f3dzkAAEA/WZtssE5HtJJk8ODB+eAHP7iuzQEAAF631ukarSS58sors+eee2b8+PF54oknkiRf+9rX8tOf/rSw4gAAAAaidQpa3/nOd/KJT3wiBx10UBYsWND1gOINNtggF1xwQZH1AQAADDjrFLS++c1v5rvf/W4+85nPpKrqxbMPd9111zzwwAOFFQcAADAQrVPQevzxx7PTTjv16F5TU5MlS5a87KIAAAAGsnUKWptssklmzpzZo/uvfvWrbL311i+3JgAAgAFtne46eOaZZ+YjH/lImpubUy6Xc9ddd+Waa67J9OnT873vfa/oGgEAAAaUdQpaxx9/fNrb23PWWWdl6dKlOeaYYzJhwoR8/etfz9FHH110jQAAAAPKOj+weIXnn38+nZ2dGT16dFE1vaZ4YDEAAJCsXTZYp2u09tlnnyxcuDBJMnLkyK6Q1dTUlH322WddRgkAAPC6sU5B66abbkpra2uP7s3NzfnDH/7wsosCAAAYyNbqGq3777+/6++HHnooc+bM6Xrf0dGRGTNmZMKECcVVBwAAMACtVdDacccdUyqVUiqVVnmKYF1dXb75zW8WVhwAAMBAtFZB6/HHH0+5XM4b3vCG3HXXXRk1alRXv+rq6owePTqVlZWFFwkAADCQrFXQmjRpUpKks7PzFSkGAADg9WCdboYxffr0XHbZZT26X3bZZfnyl7/8sosCAAAYyNYpaF188cXZcsste3TfZpttctFFF73sogAAAAaydQpac+bMybhx43p0HzVqVJ599tmXXRQAAMBAtk5Ba+LEibntttt6dL/tttsyfvz4l10UAADAQLZWN8NY4cQTT8zpp5+etra2rtu8//a3v81ZZ52VM844o9ACAQAABpp1ClpnnXVW5s+fn1NOOSWtra1Jktra2nzyk5/M2WefXWiBAAAAA02pXC6X17Xx4sWL8/DDD6euri6bb755ampqiqztNaGpqSkNDQ1pbGxMfX19f5cDAAD0k7XJBut0RGuFoUOHZrfddns5owAAAHjd6XPQOuKII3LFFVekvr4+RxxxRK/D/vjHP37ZhQEAAAxUfQ5aDQ0NKZVKXX8DAACwai/rGq31gWu0AACAZO2ywTo9RwsAAIDV6/OpgzvttFPXqYNrct99961zQQAAAANdn4PWu971rq6/m5ubc+GFF2brrbfO1KlTkyR33nlnHnzwwZxyyimFFwkAADCQ9DlonXPOOV1/n3jiiTnttNPyH//xHz2Geeqpp4qrDgAAYABap5thNDQ05J577snmm2/erfvf/va37LrrrmlsbCyswP7mZhgAAEDyKtwMo66uLrfeemuP7rfeemtqa2vXZZQAAACvG30+dXBlp59+ej784Q/n3nvvzZve9KYky6/Ruuyyy/K5z32u0AIBAAAGmnUKWp/61Kfyhje8IV//+tfz3//930mSrbbaKldccUXe8573FFogAADAQOOBxWvgGi0AACB5lR5YvHDhwnzve9/Lpz/96cyfPz/J8udnzZ49e11HCQAA8LqwTqcO3n///dlvv/3S0NCQWbNm5cQTT8yIESNyww035IknnsgPfvCDousEAAAYMNbpiNYnPvGJHHfccfnb3/7W7S6DBx10UG655ZbCigMAABiI1ilo3X333fnQhz7Uo/uECRMyZ86cl10UAADAQLZOQau2tjZNTU09uj/66KMZNWrUyy4KAABgIFunoHXYYYfl85//fNra2pIkpVIpTz75ZD71qU/lyCOPLLRAAACAgWadgtZXvvKVzJs3L6NHj86yZcuy1157ZbPNNsuwYcPyxS9+segaAQAABpR1uutgfX19br311vzud7/Lfffdl87Ozuy8887Zb7/9iq4PAABgwFnroNXe3p7a2trMnDkz++yzT/bZZ59Xoi4AAIABa61PHayqqsqkSZPS0dHxStQDAAAw4K3TNVr//u//nrPPPjvz588vuh4AAIABb52C1je+8Y384Q9/yPjx47PFFltk55137vZ6JSxYsCDTpk1LQ0NDGhoaMm3atCxcuHCN7R5++OG8853vTENDQ4YNG5Y3velNefLJJ1+RGgEAAJJ1vBnGu971rpRKpZTL5aLrWa1jjjkmTz/9dGbMmJEk+eAHP5hp06bl5z//+Wrb/P3vf8+ee+6ZE044Ieedd14aGhry8MMPp7a29tUqGwAAWA+VymuRlpYuXZozzzwzP/nJT9LW1pZ999033/zmNzNy5MhXssY8/PDD2XrrrXPnnXdm9913T5LceeedmTp1ah555JFsscUWq2x39NFHZ9CgQbnyyivXedpNTU1paGhIY2Nj6uvr13k8AADAwLY22WCtTh0855xzcsUVV+SQQw7J+973vvzf//1fPvzhD7+sYvvijjvuSENDQ1fISpI3velNaWhoyO23377KNp2dnfnlL3+ZN77xjTnggAMyevTo7L777vnJT37S67RaWlrS1NTU7QUAALA21ipo/fjHP86ll16aSy65JF//+tfzy1/+Mj/5yU9e8TsQzpkzJ6NHj+7RffTo0ZkzZ84q28ydOzeLFy/Ol770pRx44IH5zW9+k8MPPzxHHHFEbr755tVOa/r06V3XgTU0NGTixImFzQcAALB+WKug9dRTT+Utb3lL1/spU6akqqoqzzzzzDpN/Nxzz02pVOr1dc899yRJSqVSj/blcnmV3ZPlR7SS5LDDDsvHP/7x7LjjjvnUpz6VQw89NBdddNFqazr77LPT2NjY9XrqqafWad4AAID111rdDKOjoyPV1dXdR1BVlfb29nWa+Kmnnpqjjz6612EmT56c+++/P88991yPfvPmzcuYMWNW2W7kyJGpqqrK1ltv3a37VlttlVtvvXW106upqUlNTU0fqgcAAFi1tQpa5XI5xx13XLcg0tzcnJNPPjlDhgzp6vbjH/+4T+MbOXJkn26kMXXq1DQ2Nuauu+7KlClTkiR//OMf09jYmD322GOVbaqrq7Pbbrvl0Ucf7db9r3/9ayZNmtSn+gAAANbFWgWtY489tke397///YUVszpbbbVVDjzwwJx00km5+OKLkyy/vfuhhx7a7Y6DW265ZaZPn57DDz88SXLmmWfmve99b9761rfmbW97W2bMmJGf//znuemmm17xmgEAgPXXWgWtyy+//JWqY42uvvrqnHbaadl///2TJO985zvzrW99q9swjz76aBobG7veH3744bnooosyffr0nHbaadliiy1y/fXXZ88993xVawcAANYva/UcrfWR52gBAADJK/gcLQAAANZM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGADJmgtWLAg06ZNS0NDQxoaGjJt2rQsXLiw1zaLFy/Oqaeemo022ih1dXXZaqut8p3vfOfVKRgAAFhvDZigdcwxx2TmzJmZMWNGZsyYkZkzZ2batGm9tvn4xz+eGTNm5KqrrsrDDz+cj3/84/noRz+an/70p69S1QAAwPpoQASthx9+ODNmzMj3vve9TJ06NVOnTs13v/vd/OIXv8ijjz662nZ33HFHjj322Oy9996ZPHlyPvjBD2aHHXbIPffc8ypWDwAArG8GRNC644470tDQkN13372r25ve9KY0NDTk9ttvX227PffcMz/72c8ye/bslMvl/P73v89f//rXHHDAAatt09LSkqampm4vAACAtTEggtacOXMyevToHt1Hjx6dOXPmrLbdN77xjWy99dbZaKONUl1dnQMPPDAXXnhh9txzz9W2mT59etd1YA0NDZk4cWIh8wAAAKw/+jVonXvuuSmVSr2+VpzmVyqVerQvl8ur7L7CN77xjdx555352c9+lnvvvTdf/epXc8opp+T//u//Vtvm7LPPTmNjY9frqaeeevkzCgAArFeq+nPip556ao4++uheh5k8eXLuv//+PPfccz36zZs3L2PGjFllu2XLluXTn/50brjhhhxyyCFJku233z4zZ87MV77yley3336rbFdTU5Oampq1nBMAAIAX9WvQGjlyZEaOHLnG4aZOnZrGxsbcddddmTJlSpLkj3/8YxobG7PHHnussk1bW1va2tpSUdH9oF1lZWU6OztffvEAAACrMSCu0dpqq61y4IEH5qSTTsqdd96ZO++8MyeddFIOPfTQbLHFFl3DbbnllrnhhhuSJPX19dlrr71y5pln5qabbsrjjz+eK664Ij/4wQ9y+OGH99esAAAA64F+PaK1Nq6++uqcdtpp2X///ZMk73znO/Otb32r2zCPPvpoGhsbu95fe+21Ofvss/Mv//IvmT9/fiZNmpQvfvGLOfnkk1/V2gEAgPVLqVwul/u7iNeypqamNDQ0pLGxMfX19f1dDgAA0E/WJhsMiFMHAQAABhJBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgg2YoPXFL34xe+yxRwYPHpzhw4f3qU25XM65556b8ePHp66uLnvvvXcefPDBV7ZQAABgvTdgglZra2ve/e5358Mf/nCf25x//vn5r//6r3zrW9/K3XffnbFjx+btb397Fi1a9ApWCgAArO9K5XK53N9FrI0rrrgip59+ehYuXNjrcOVyOePHj8/pp5+eT37yk0mSlpaWjBkzJl/+8pfzoQ99aJXtWlpa0tLS0vW+qakpEydOTGNjY+rr6wubDwAAYGBpampKQ0NDn7LBgDmitbYef/zxzJkzJ/vvv39Xt5qamuy11165/fbbV9tu+vTpaWho6HpNnDjx1SgXAAB4HXndBq05c+YkScaMGdOt+5gxY7r6rcrZZ5+dxsbGrtdTTz31itYJAAC8/vRr0Dr33HNTKpV6fd1zzz0vaxqlUqnb+3K53KPbympqalJfX9/tBQAAsDaq+nPip556ao4++uheh5k8efI6jXvs2LFJlh/ZGjduXFf3uXPn9jjKBQAAUKR+DVojR47MyJEjX5Fxb7LJJhk7dmxuvPHG7LTTTkmW37nw5ptvzpe//OVXZJoAAADJALpG68knn8zMmTPz5JNPpqOjIzNnzszMmTOzePHirmG23HLL3HDDDUmWnzJ4+umn5z//8z9zww035C9/+UuOO+64DB48OMccc0x/zQYAALAe6NcjWmvjc5/7XL7//e93vV9xlOr3v/999t577yTJo48+msbGxq5hzjrrrCxbtiynnHJKFixYkN133z2/+c1vMmzYsFe1dgAAYP0y4J6j9Wpbm3vlAwAAr1+eowUAANCPBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFq+rvAgCAl6+zs5zZC5dlSWt7hlRXZcLwulRUlPq7rPWS74LXM8t33wlaADDAPTZ3UX79l+fy93mL09zekdqqymw6amgO2HZMNhs9rL/LW6/4Lng9s3yvHUELAAawx+YuyuW3zcr8Ja0Z11CbwdV1Wdranr8805hnGpfl+DdPtgP0KvFd8Hpm+V57rtECgAGqs7OcX//lucxf0prNRw/NsNpBqawoZVjtoGw+emjmL2nNbx58Lp2d5f4u9XXPd8HrmeV73QhaAK9BnZ3lPDV/aR6Z05Sn5i/ttvHqrR/rl9kLl+Xv8xZnXENtSqXu10iUSqWMa6jNY3MXZ/bCZf1U4fpj5e8iSZqWteX5xS1pWtaWJL4LBjTrmnXj1EGA15jezoFP4vx4uixpbU9ze0cGV9etsn9ddWWea2rOktb2V7my9c+K76K5rTIPP7sgC5a2pr2jM1WVFdlgcHUmjxyclvYO3wUDknXNuhG0AF5DejsH/uE5TUmSjs6y8+NJkgyprkptVWWWtrZnWO2gHv2XtXakpqoyQ6pt7l9pQ6qr0tremXufmJ+OznKG1g7KoNqqtHWUM29Rc+YvacnEEYN9FwxI1jXrxqmDAK8RvZ0Dv9moIfnrnEX563OLstmoIc6PJ0kyYXhdNh01NM82Nqdc7v7dl8vlPNvYnM1GD82E4av+FZrijKuvTUtbZxYua8sGgwelpqoiFaVSaqoqssHgQVm4rC2t7Z0ZV1/b36XCWrOuWTeCFsBrRG/nwC9u6UhHuZyOznIWt3R06+f8+PVXRUUpB2w7JiOGVOdvcxdnUXNb2js7s6i5LX+buzgjhlRn/23GeMbNq+DZpubUDFoeqhYsbUtLe0c6y+W0tHdkwdK2DK8blOqqijzb1NzfpcJas65ZN4IWwGvEi+fA9zz1orWjM0l5pb+7q6uudP3Hemqz0cNy/JsnZ9vxDVm4tC2znl+ShUvbst2EBqeTvoqWtLanuqoiO2+8QUYPq01zW2cWLm1Nc1tnRtfXZpdJG6SmqsL/KAOWdc3acyIlwGtEb+fAV1dWJCmt9Hd3zo9fv202eljesPfQzF64LEta2zOkuioThtf5dflVtOL/t3ZQZXadvEEWNbentaMz1ZUVGVZblcUt7Wlu6/Q/yoBmXbN2/LcDvEasOAf+L880ZmhNVbfTB4fWVKayVEpKy/9e2Yrz47eb0OD8+PVYRUUpE0cM7u8y1lsr//9uPnpo6ute/LHE/yivJ9Y1fSdoAbxGrDgH/pnGZfnb3OXXatVVV2ZZa0eebWzOG8cuPy3jsXlLevRzfjz0rzX9//ofhfVPqfzSW4fQTVNTUxoaGtLY2Jj6+vr+LgdYD6z8HK2W9uWnBG42emj236b7c7Re2s/58dD/evv/9T8KA9/aZANBaw0ELaA/dHaWV3sOfG/9gP7nfxRev9YmGzh1EOA1qLdz4J0fD69t/keBxO3dAQAACidoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtAAAAAomaAEAABRM0AIAACiYoAUAAFAwQQsAAKBgghYAAEDBBC0AAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKFhVfxfwWlcul5MkTU1N/VwJAADQn1ZkghUZoTeC1hosWrQoSTJx4sR+rgQAAHgtWLRoURoaGnodplTuSxxbj3V2duaZZ57JsGHDUiqVevRvamrKxIkT89RTT6W+vr4fKuS1zjLCmlhG6I3lgzWxjLAmlpHilMvlLFq0KOPHj09FRe9XYTmitQYVFRXZaKON1jhcfX29BZdeWUZYE8sIvbF8sCaWEdbEMlKMNR3JWsHNMAAAAAomaAEAABRM0HqZampqcs4556Smpqa/S+E1yjLCmlhG6I3lgzWxjLAmlpH+4WYYAAAABXNECwAAoGCCFgAAQMEELQAAgIIJWgAAAAUTtF6mX/7yl9l9991TV1eXkSNH5ogjjujW/8knn8w73vGODBkyJCNHjsxpp52W1tbWfqqW/tLS0pIdd9wxpVIpM2fO7NbPMrJ+mjVrVk444YRssskmqaury6abbppzzjmnx3dv+eDCCy/MJptsktra2uyyyy75wx/+0N8l0Q+mT5+e3XbbLcOGDcvo0aPzrne9K48++mi3Ycrlcs4999yMHz8+dXV12XvvvfPggw/2U8X0t+nTp6dUKuX000/v6mYZeXUJWi/D9ddfn2nTpuX444/Pn//859x222055phjuvp3dHTkkEMOyZIlS3Lrrbfm2muvzfXXX58zzjijH6umP5x11lkZP358j+6WkfXXI488ks7Ozlx88cV58MEH87WvfS0XXXRRPv3pT3cNY/nguuuuy+mnn57PfOYz+dOf/pS3vOUtOeigg/Lkk0/2d2m8ym6++eZ85CMfyZ133pkbb7wx7e3t2X///bNkyZKuYc4///z813/9V771rW/l7rvvztixY/P2t789ixYt6sfK6Q933313Lrnkkmy//fbdultGXmVl1klbW1t5woQJ5e9973urHeZ///d/yxUVFeXZs2d3dbvmmmvKNTU15cbGxlejTF4D/vd//7e85ZZblh988MFykvKf/vSnbv0sI6xw/vnnlzfZZJOu95YPpkyZUj755JO7ddtyyy3Ln/rUp/qpIl4r5s6dW05Svvnmm8vlcrnc2dlZHjt2bPlLX/pS1zDNzc3lhoaG8kUXXdRfZdIPFi1aVN58883LN954Y3mvvfYqf+xjHyuXy5aR/uCI1jq67777Mnv27FRUVGSnnXbKuHHjctBBB3U7/HrHHXdk22237XYk44ADDkhLS0vuvffe/iibV9lzzz2Xk046KVdeeWUGDx7co79lhJU1NjZmxIgRXe8tH+u31tbW3Hvvvdl///27dd9///1z++2391NVvFY0NjYmSdc64/HHH8+cOXO6LS81NTXZa6+9LC/rmY985CM55JBDst9++3Xrbhl59Qla6+gf//hHkuTcc8/Nv//7v+cXv/hFNthgg+y1116ZP39+kmTOnDkZM2ZMt3YbbLBBqqurM2fOnFe9Zl5d5XI5xx13XE4++eTsuuuuqxzGMsIKf//73/PNb34zJ598clc3y8f67fnnn09HR0ePZWDMmDG+//VcuVzOJz7xiey5557Zdtttk6RrmbC8rN+uvfba3HfffZk+fXqPfpaRV5+g9RLnnntuSqVSr6977rknnZ2dSZLPfOYzOfLII7PLLrvk8ssvT6lUyo9+9KOu8ZVKpR7TKJfLq+zOwNDXZeSb3/xmmpqacvbZZ/c6PsvI60tfl4+VPfPMMznwwAPz7ne/OyeeeGK3fpYPXvpd+/459dRTc//99+eaa67p0c/ysv566qmn8rGPfSxXXXVVamtrVzucZeTVU9XfBbzWnHrqqTn66KN7HWby5MldFw1uvfXWXd1ramryhje8oesi5bFjx+aPf/xjt7YLFixIW1tbj18TGDj6uox84QtfyJ133pmamppu/Xbdddf8y7/8S77//e9bRl6H+rp8rPDMM8/kbW97W6ZOnZpLLrmk23CWj/XbyJEjU1lZ2eOX5rlz5/r+12Mf/ehH87Of/Sy33HJLNtpoo67uY8eOTbL8qMW4ceO6ulte1h/33ntv5s6dm1122aWrW0dHR2655ZZ861vf6rpLpWXk1SNovcTIkSMzcuTINQ63yy67pKamJo8++mj23HPPJElbW1tmzZqVSZMmJUmmTp2aL37xi3n22We7Fujf/OY3qamp6fZPwMDS12XkG9/4Rr7whS90vX/mmWdywAEH5Lrrrsvuu++exDLyetTX5SNJZs+enbe97W1dR8QrKrqfZGD5WL9VV1dnl112yY033pjDDz+8q/uNN96Yww47rB8roz+Uy+V89KMfzQ033JCbbropm2yySbf+m2yyScaOHZsbb7wxO+20U5Ll1/ndfPPN+fKXv9wfJfMq23ffffPAAw9063b88cdnyy23zCc/+cm84Q1vsIy82vrtNhyvAx/72MfKEyZMKP/6178uP/LII+UTTjihPHr06PL8+fPL5XK53N7eXt52223L++67b/m+++4r/9///V95o402Kp966qn9XDn94fHHH+9x10HLyPpr9uzZ5c0226y8zz77lJ9++unys88+2/VawfLBtddeWx40aFD50ksvLT/00EPl008/vTxkyJDyrFmz+rs0XmUf/vCHyw0NDeWbbrqp2/pi6dKlXcN86UtfKjc0NJR//OMflx944IHy+973vvK4cePKTU1N/Vg5/Wnluw6Wy5aRV5ug9TK0traWzzjjjPLo0aPLw4YNK++3337lv/zlL92GeeKJJ8qHHHJIua6urjxixIjyqaeeWm5ubu6niulPqwpa5bJlZH11+eWXl5Os8rUyywff/va3y5MmTSpXV1eXd955567bebN+Wd364vLLL+8aprOzs3zOOeeUx44dW66pqSm/9a1vLT/wwAP9VzT97qVByzLy6iqVy+VyfxxJAwAAeL1y10EAAICCCVoAAAAFE7QAAAAKJmgBAAAUTNACAAAomKAFAABQMEELAACgYIIWAABAwQQtAHiVlUql/OQnP+nvMgB4BQlaALyu3X777amsrMyBBx64Vu0mT56cCy644JUpCoDXPUELgNe1yy67LB/96Edz66235sknn+zvcgBYTwhaALxuLVmyJD/84Q/z4Q9/OIceemiuuOKKbv1/9rOfZdddd01tbW1GjhyZI444Ikmy995754knnsjHP/7xlEqllEqlJMm5556bHXfcsds4LrjggkyePLnr/d133523v/3tGTlyZBoaGrLXXnvlvvvueyVnE4DXIEELgNet6667LltssUW22GKLvP/978/ll1+ecrmcJPnlL3+ZI444Ioccckj+9Kc/5be//W123XXXJMmPf/zjbLTRRvn85z+fZ599Ns8++2yfp7lo0aIce+yx+cMf/pA777wzm2++eQ4++OAsWrToFZlHAF6bqvq7AAB4pVx66aV5//vfnyQ58MADs3jx4vz2t7/Nfvvtly9+8Ys5+uijc95553UNv8MOOyRJRowYkcrKygwbNixjx45dq2nus88+3d5ffPHF2WCDDXLzzTfn0EMPfZlzBMBA4YgWAK9Ljz76aO66664cffTRSZKqqqq8973vzWWXXZYkmTlzZvbdd9/Cpzt37tycfPLJeeMb35iGhoY0NDRk8eLFrg8DWM84ogXA69Kll16a9vb2TJgwoatbuVzOoEGDsmDBgtTV1a31OCsqKrpOPVyhra2t2/vjjjsu8+bNywUXXJBJkyalpqYmU6dOTWtr67rNCAADkiNaALzutLe35wc/+EG++tWvZubMmV2vP//5z5k0aVKuvvrqbL/99vntb3+72nFUV1eno6OjW7dRo0Zlzpw53cLWzJkzuw3zhz/8IaeddloOPvjgbLPNNqmpqcnzzz9f6PwB8NrniBYArzu/+MUvsmDBgpxwwglpaGjo1u+oo47KpZdemq997WvZd999s+mmm+boo49Oe3t7fvWrX+Wss85Ksvw5WrfcckuOPvro1NTUZOTIkdl7770zb968nH/++TnqqKMyY8aM/OpXv0p9fX3X+DfbbLNceeWV2XXXXdPU1JQzzzxznY6eATCwOaIFwOvOpZdemv32269HyEqSI488MjNnzkx9fX1+9KMf5Wc/+1l23HHH7LPPPvnjH//YNdznP//5zJo1K5tuumlGjRqVJNlqq61y4YUX5tvf/nZ22GGH3HXXXfm3f/u3buO/7LLLsmDBguy0006ZNm1aTjvttIwePfqVnWEAXnNK5ZeebA4AAMDL4ogWAABAwQQtAACAgglaAAAABRO0AAAACiZoAQAAFEzQAgAAKJigBQAAUDBBCwAAoGCCFgAAQMEELQAAgIIJWgAAAAX7/yKMVgs8+kBvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create model\n",
    "rf_model = RandomForestRegressor(**rf_optimizer.max['params'], n_estimators=1000, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "# plot preds\n",
    "plot_preds(mfeatures, rf_model, scaler=StandardScaler(), 'XGBRegressor Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stacking Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.435117</td>\n",
       "      <td>0.184568</td>\n",
       "      <td>10.817868</td>\n",
       "      <td>11.803782</td>\n",
       "      <td>0.736099</td>\n",
       "      <td>0.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.445907</td>\n",
       "      <td>0.183591</td>\n",
       "      <td>10.714050</td>\n",
       "      <td>11.810515</td>\n",
       "      <td>0.741858</td>\n",
       "      <td>0.671125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.441347</td>\n",
       "      <td>0.183332</td>\n",
       "      <td>10.758056</td>\n",
       "      <td>11.812226</td>\n",
       "      <td>0.740468</td>\n",
       "      <td>0.675096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>StackingRegressor(estimators=[('rf',\\n        ...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493412</td>\n",
       "      <td>0.190769</td>\n",
       "      <td>10.244076</td>\n",
       "      <td>11.745068</td>\n",
       "      <td>0.764496</td>\n",
       "      <td>0.687011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>StackingRegressor(estimators=[('rf',\\n        ...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.492329</td>\n",
       "      <td>0.190661</td>\n",
       "      <td>10.255009</td>\n",
       "      <td>11.745254</td>\n",
       "      <td>0.764297</td>\n",
       "      <td>0.686217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tournament             Label  \\\n",
       "973          M  A_score_diff_adj   \n",
       "974          M  A_score_diff_adj   \n",
       "975          M  A_score_diff_adj   \n",
       "976          M  A_score_diff_adj   \n",
       "977          M  A_score_diff_adj   \n",
       "\n",
       "                                                 Model          Scaler  \\\n",
       "973  XGBRegressor(base_score=None, booster=None, ca...  MinMaxScaler()   \n",
       "974  XGBRegressor(base_score=None, booster=None, ca...  MinMaxScaler()   \n",
       "975  XGBRegressor(base_score=None, booster=None, ca...  MinMaxScaler()   \n",
       "976  StackingRegressor(estimators=[('rf',\\n        ...  MinMaxScaler()   \n",
       "977  StackingRegressor(estimators=[('rf',\\n        ...  MinMaxScaler()   \n",
       "\n",
       "     Num_Features                                           Features  \\\n",
       "973           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "974           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "975           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "976           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "977           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "\n",
       "     Num_CV_Folds  Train_R2    Val_R2  Train_RMSE   Val_RMSE  Train_Acc  \\\n",
       "973             5  0.435117  0.184568   10.817868  11.803782   0.736099   \n",
       "974             5  0.445907  0.183591   10.714050  11.810515   0.741858   \n",
       "975             5  0.441347  0.183332   10.758056  11.812226   0.740468   \n",
       "976             5  0.493412  0.190769   10.244076  11.745068   0.764496   \n",
       "977             5  0.492329  0.190661   10.255009  11.745254   0.764297   \n",
       "\n",
       "      Val_Acc  \n",
       "973  0.675100  \n",
       "974  0.671125  \n",
       "975  0.675096  \n",
       "976  0.687011  \n",
       "977  0.686217  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create stacking regressor\n",
    "stacking_model = StackingRegressor(estimators=[('rf', rf_model), ('xgb', xgb_model), ('linreg', linreg_model)], final_estimator=LinearRegression(n_jobs=-1))\n",
    "\n",
    "# cross validate\n",
    "run_model(estimator=stacking_model, data=mfeatures_diff, regression=True, models_df=reg_df, tournament='M', scaler=MinMaxScaler())\n",
    "\n",
    "# look at results\n",
    "reg_df[reg_df['Tournament'] == 'M'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>StackingRegressor(estimators=[('rf',\\n        ...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493412</td>\n",
       "      <td>0.190769</td>\n",
       "      <td>10.244076</td>\n",
       "      <td>11.745068</td>\n",
       "      <td>0.764496</td>\n",
       "      <td>0.687011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>StackingRegressor(estimators=[('rf',\\n        ...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.492329</td>\n",
       "      <td>0.190661</td>\n",
       "      <td>10.255009</td>\n",
       "      <td>11.745254</td>\n",
       "      <td>0.764297</td>\n",
       "      <td>0.686217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.432860</td>\n",
       "      <td>0.186923</td>\n",
       "      <td>10.839437</td>\n",
       "      <td>11.782002</td>\n",
       "      <td>0.734709</td>\n",
       "      <td>0.676677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.427267</td>\n",
       "      <td>0.186173</td>\n",
       "      <td>10.892734</td>\n",
       "      <td>11.787612</td>\n",
       "      <td>0.733518</td>\n",
       "      <td>0.676677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>M</td>\n",
       "      <td>A_score_diff_adj</td>\n",
       "      <td>(DecisionTreeRegressor(max_depth=64, max_featu...</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.560212</td>\n",
       "      <td>0.185458</td>\n",
       "      <td>9.545322</td>\n",
       "      <td>11.789767</td>\n",
       "      <td>0.803614</td>\n",
       "      <td>0.677481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Tournament             Label  \\\n",
       "976          M  A_score_diff_adj   \n",
       "977          M  A_score_diff_adj   \n",
       "875          M  A_score_diff_adj   \n",
       "871          M  A_score_diff_adj   \n",
       "757          M  A_score_diff_adj   \n",
       "\n",
       "                                                 Model          Scaler  \\\n",
       "976  StackingRegressor(estimators=[('rf',\\n        ...  MinMaxScaler()   \n",
       "977  StackingRegressor(estimators=[('rf',\\n        ...  MinMaxScaler()   \n",
       "875  XGBRegressor(base_score=None, booster=None, ca...  MinMaxScaler()   \n",
       "871  XGBRegressor(base_score=None, booster=None, ca...  MinMaxScaler()   \n",
       "757  (DecisionTreeRegressor(max_depth=64, max_featu...  MinMaxScaler()   \n",
       "\n",
       "     Num_Features                                           Features  \\\n",
       "976           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "977           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "875           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "871           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "757           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "\n",
       "     Num_CV_Folds  Train_R2    Val_R2  Train_RMSE   Val_RMSE  Train_Acc  \\\n",
       "976             5  0.493412  0.190769   10.244076  11.745068   0.764496   \n",
       "977             5  0.492329  0.190661   10.255009  11.745254   0.764297   \n",
       "875             5  0.432860  0.186923   10.839437  11.782002   0.734709   \n",
       "871             5  0.427267  0.186173   10.892734  11.787612   0.733518   \n",
       "757             5  0.560212  0.185458    9.545322  11.789767   0.803614   \n",
       "\n",
       "      Val_Acc  \n",
       "976  0.687011  \n",
       "977  0.686217  \n",
       "875  0.676677  \n",
       "871  0.676677  \n",
       "757  0.677481  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_df[(reg_df['Tournament'] == 'M') & ((reg_df['Num_Features'] == 187) | (reg_df['Num_Features'] == 103))].sort_values(by='Val_RMSE', ascending=True).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Men's regression model resulted in a RMSE of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "reg_df.to_csv('models/regression_models_detailed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizer for Classification\n",
    "We will use KBinsDiscretizer to transform our continuous statistics into discrete variables. This adds non-linearity to linear models like LogReg and SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "#### Men's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_LogLoss</th>\n",
       "      <th>Val_LogLoss</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[A_PlayIn, A_away_missing, A_close_game_missin...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.061052</td>\n",
       "      <td>12.254341</td>\n",
       "      <td>0.748609</td>\n",
       "      <td>0.660014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tournament  Label                                          Model Scaler  \\\n",
       "1105          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "\n",
       "      Num_Features                                           Features  \\\n",
       "1105           103  [A_PlayIn, A_away_missing, A_close_game_missin...   \n",
       "\n",
       "      Num_CV_Folds  Train_LogLoss  Val_LogLoss  Train_Acc   Val_Acc  \n",
       "1105             5       9.061052    12.254341   0.748609  0.660014  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at default logreg performance on men's\n",
    "logreg_model = LogisticRegression(n_jobs=-1, random_state=SEED)\n",
    "\n",
    "# cross validate\n",
    "run_model(estimator=logreg_model, data=mfeatures_diff, regression=False, models_df=class_df, tournament='M', scaler=None)\n",
    "\n",
    "# look at results\n",
    "class_df[class_df['Tournament'] == 'M'].tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_LogLoss</th>\n",
       "      <th>Val_LogLoss</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.125402</td>\n",
       "      <td>12.168978</td>\n",
       "      <td>0.746824</td>\n",
       "      <td>0.662382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.096774</td>\n",
       "      <td>11.825022</td>\n",
       "      <td>0.747618</td>\n",
       "      <td>0.671925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.268574</td>\n",
       "      <td>11.968508</td>\n",
       "      <td>0.742851</td>\n",
       "      <td>0.667944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.168375</td>\n",
       "      <td>11.968394</td>\n",
       "      <td>0.745631</td>\n",
       "      <td>0.667947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.182706</td>\n",
       "      <td>11.996772</td>\n",
       "      <td>0.745234</td>\n",
       "      <td>0.667160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tournament  Label                                          Model Scaler  \\\n",
       "1126          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1127          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1128          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1129          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1130          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "\n",
       "      Num_Features                                           Features  \\\n",
       "1126           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1127           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1128           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1129           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1130           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "\n",
       "      Num_CV_Folds  Train_LogLoss  Val_LogLoss  Train_Acc   Val_Acc  \n",
       "1126             5       9.125402    12.168978   0.746824  0.662382  \n",
       "1127             5       9.096774    11.825022   0.747618  0.671925  \n",
       "1128             5       9.268574    11.968508   0.742851  0.667944  \n",
       "1129             5       9.168375    11.968394   0.745631  0.667947  \n",
       "1130             5       9.182706    11.996772   0.745234  0.667160  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get categorical cols\n",
    "cat_cols = [col for col in cols_to_keep if col not in ['A_TeamID', 'B_TeamID']]\n",
    "\n",
    "# 5 different bin sizes\n",
    "for bin in [60, 70, 80, 90, 100]:\n",
    "    mfeatures_discrete = KBinsDiscretizer(n_bins=bin, encode='ordinal', strategy='uniform', subsample=None).fit_transform(mfeatures_diff.drop(columns=cat_cols))\n",
    "    mfeatures_discrete = pd.DataFrame(mfeatures_discrete, columns=mfeatures_diff.drop(columns=cat_cols).columns)\n",
    "    mfeatures_discrete = pd.concat([mfeatures_discrete, mfeatures_diff[cat_cols]], axis=1)\n",
    "\n",
    "    # run model\n",
    "    run_model(estimator=logreg_model, data=mfeatures_discrete, regression=False, models_df=class_df, tournament='M', scaler=None)\n",
    "\n",
    "# look at results\n",
    "class_df[class_df['Tournament'] == 'M'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tournament</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Num_CV_Folds</th>\n",
       "      <th>Train_LogLoss</th>\n",
       "      <th>Val_LogLoss</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.003776</td>\n",
       "      <td>12.053870</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>0.665576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.139797</td>\n",
       "      <td>12.025036</td>\n",
       "      <td>0.746424</td>\n",
       "      <td>0.666376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.061059</td>\n",
       "      <td>12.626676</td>\n",
       "      <td>0.748609</td>\n",
       "      <td>0.649684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.061045</td>\n",
       "      <td>11.967482</td>\n",
       "      <td>0.748609</td>\n",
       "      <td>0.667973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.168375</td>\n",
       "      <td>12.053414</td>\n",
       "      <td>0.745631</td>\n",
       "      <td>0.665588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.039548</td>\n",
       "      <td>11.595717</td>\n",
       "      <td>0.749206</td>\n",
       "      <td>0.678287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.032411</td>\n",
       "      <td>11.825136</td>\n",
       "      <td>0.749404</td>\n",
       "      <td>0.671922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.125465</td>\n",
       "      <td>11.853856</td>\n",
       "      <td>0.746822</td>\n",
       "      <td>0.671125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.154050</td>\n",
       "      <td>12.025492</td>\n",
       "      <td>0.746029</td>\n",
       "      <td>0.666363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.082485</td>\n",
       "      <td>11.738748</td>\n",
       "      <td>0.748014</td>\n",
       "      <td>0.674319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.125402</td>\n",
       "      <td>12.168978</td>\n",
       "      <td>0.746824</td>\n",
       "      <td>0.662382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.096774</td>\n",
       "      <td>11.825022</td>\n",
       "      <td>0.747618</td>\n",
       "      <td>0.671925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.268574</td>\n",
       "      <td>11.968508</td>\n",
       "      <td>0.742851</td>\n",
       "      <td>0.667944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.168375</td>\n",
       "      <td>11.968394</td>\n",
       "      <td>0.745631</td>\n",
       "      <td>0.667947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>M</td>\n",
       "      <td>A_win</td>\n",
       "      <td>LogisticRegression(n_jobs=-1, random_state=0)</td>\n",
       "      <td>None</td>\n",
       "      <td>103</td>\n",
       "      <td>[Seed_diff, avg_Ast_against_diff, avg_Ast_for_...</td>\n",
       "      <td>5</td>\n",
       "      <td>9.182706</td>\n",
       "      <td>11.996772</td>\n",
       "      <td>0.745234</td>\n",
       "      <td>0.667160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tournament  Label                                          Model Scaler  \\\n",
       "1116          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1117          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1118          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1119          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1120          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1121          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1122          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1123          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1124          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1125          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1126          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1127          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1128          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1129          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "1130          M  A_win  LogisticRegression(n_jobs=-1, random_state=0)   None   \n",
       "\n",
       "      Num_Features                                           Features  \\\n",
       "1116           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1117           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1118           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1119           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1120           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1121           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1122           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1123           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1124           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1125           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1126           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1127           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1128           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1129           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "1130           103  [Seed_diff, avg_Ast_against_diff, avg_Ast_for_...   \n",
       "\n",
       "      Num_CV_Folds  Train_LogLoss  Val_LogLoss  Train_Acc   Val_Acc  \n",
       "1116             5       9.003776    12.053870   0.750198  0.665576  \n",
       "1117             5       9.139797    12.025036   0.746424  0.666376  \n",
       "1118             5       9.061059    12.626676   0.748609  0.649684  \n",
       "1119             5       9.061045    11.967482   0.748609  0.667973  \n",
       "1120             5       9.168375    12.053414   0.745631  0.665588  \n",
       "1121             5       9.039548    11.595717   0.749206  0.678287  \n",
       "1122             5       9.032411    11.825136   0.749404  0.671922  \n",
       "1123             5       9.125465    11.853856   0.746822  0.671125  \n",
       "1124             5       9.154050    12.025492   0.746029  0.666363  \n",
       "1125             5       9.082485    11.738748   0.748014  0.674319  \n",
       "1126             5       9.125402    12.168978   0.746824  0.662382  \n",
       "1127             5       9.096774    11.825022   0.747618  0.671925  \n",
       "1128             5       9.268574    11.968508   0.742851  0.667944  \n",
       "1129             5       9.168375    11.968394   0.745631  0.667947  \n",
       "1130             5       9.182706    11.996772   0.745234  0.667160  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_df[class_df['Tournament'] == 'M'].tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the predicted total as a feature for the classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultate N Brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root dirs\n",
    "root = 'data/'\n",
    "mroot = 'data/mens/'\n",
    "wroot = 'data/womens/'\n",
    "\n",
    "# load data, get slots for 2024, drop play-ins\n",
    "slots = pd.read_csv(mroot + 'MNCAATourneySlots.csv')\n",
    "slots = slots[slots['Season'] == 2024]\n",
    "slots = slots[slots['Slot'].str.contains('R')].reset_index(drop=True)\n",
    "\n",
    "# load seed data\n",
    "seeds_2024 = pd.read_csv(root + '2024_tourney_seeds.csv')\n",
    "\n",
    "# load in 2024 data\n",
    "df_2024 = pd.read_csv(root + 'processed/2024_features.csv').drop(columns=['Region', 'Season'])\n",
    "\n",
    "# drop teams not in 2024 tourney\n",
    "df_2024 = df_2024[df_2024['TeamID'].isin(seeds_2024['TeamID'])]\n",
    "\n",
    "# drop play in char\n",
    "df_2024['FullSeed'] = df_2024['FullSeed'].apply(lambda x: x[:3])\n",
    "\n",
    "# split into mens and womens\n",
    "mdf_2024 = df_2024[df_2024['TeamID'] < 3000].reset_index(drop=True)\n",
    "wdf_2024 = df_2024[df_2024['TeamID'] >= 3000].reset_index(drop=True)\n",
    "\n",
    "# delete vars\n",
    "del root, mroot, wroot, seeds_2024, df_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bracket(data, estimator, tournament, num_brackets, slots_df=slots):\n",
    "    \"\"\"\n",
    "    Generate a single bracket for the 2024 NCAA tournament.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Regular season data for the 2024 teams competing in the tournament.\n",
    "    estimator : sklearn estimator\n",
    "        Pre-trained estimator to use for modeling.\n",
    "    tournament : str\n",
    "        'M' or 'W'.\n",
    "    num_brackets : int\n",
    "        Number of brackets to generate.\n",
    "    slots : pd.DataFrame\n",
    "        Slots for the 2024 tournament.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_brackets : pd.DataFrame\n",
    "        DataFrame with the predicted outcomes of the tournament.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # get a copies of data to avoid modifying the original\n",
    "    features = data.copy()\n",
    "\n",
    "    # define scaler\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # create empty df for all brackets\n",
    "    all_brackets = pd.DataFrame()\n",
    "\n",
    "    # loop for each bracket\n",
    "    for n in range(1, num_brackets+1):\n",
    "        # create bracket-specific slots table\n",
    "        slots = slots_df.copy()\n",
    "\n",
    "        # create empty results for round\n",
    "        result_df = pd.DataFrame(columns=[\"Slot\", \"Team\"])\n",
    "\n",
    "        # 6 rounds in a single bracket\n",
    "        for i in range(1, 7):\n",
    "            # get slots for round\n",
    "            slots_round = slots[slots['Slot'].str.contains(f'R{i}')].reset_index(drop=True)\n",
    "\n",
    "            # holds data for each matchup\n",
    "            round_matchups = []\n",
    "\n",
    "            # loop through the slots\n",
    "            for idx, row in slots_round.iterrows():\n",
    "                # get team A and team B\n",
    "                A = features[features['FullSeed'] == row['StrongSeed']].reset_index(drop=True)\n",
    "                B = features[features['FullSeed'] == row['WeakSeed']].reset_index(drop=True)\n",
    "\n",
    "                # rename cols\n",
    "                A = A.add_prefix('A_')\n",
    "                B = B.add_prefix('B_')\n",
    "\n",
    "                # create matchup dataframe\n",
    "                combined = pd.concat([A, B], axis=1)\n",
    "\n",
    "                # append combined row to the list\n",
    "                round_matchups.append(combined)\n",
    "\n",
    "            # concatenate all matchup rows into a single DataFrame\n",
    "            round_df = pd.concat(round_matchups, axis=0).reset_index(drop=True)\n",
    "            \n",
    "            # calculate seed diff\n",
    "            round_df['A_seed_diff'] = round_df['B_Seed'] - round_df['A_Seed']\n",
    "\n",
    "            # add some cols that were in training data\n",
    "            round_df['A_Loc_A'] = 0\n",
    "            round_df['A_Loc_H'] = 0\n",
    "            round_df[['round_2', 'round_3', 'round_4', 'round_5', 'round_6']] = 0\n",
    "            if i == 1:\n",
    "                pass\n",
    "            else:\n",
    "                round_df[f'round_{i}'] = 1\n",
    "\n",
    "            # drop cols\n",
    "            round_df = round_df.drop(columns=['A_Seed', 'B_Seed', 'A_seed_diff'])\n",
    "\n",
    "            # add placeholder cols for sorting\n",
    "            round_df['score_diff_adj'] = 0\n",
    "            round_df['win'] = 0\n",
    "            round_df = round_df[sorted_cols]\n",
    "\n",
    "            # define X and reorder cols\n",
    "            X = round_df.drop(columns=['A_TeamID', 'A_FullSeed', 'B_TeamID', 'B_FullSeed', 'score_diff_adj', 'win'])\n",
    "\n",
    "            # scale data\n",
    "            X = scaler.fit_transform(X)\n",
    "            \n",
    "            # predict the outcomes of the round\n",
    "            preds = estimator.predict_proba(X)\n",
    "\n",
    "            # if n == 1:\n",
    "            #     print(f'\\n\\nround {i}:')\n",
    "            #     print(f'preds before: {preds}')\n",
    "\n",
    "            # generate random values, update preds\n",
    "            random_values = np.random.rand(len(preds))\n",
    "            preds = (random_values > preds[:, 0]).astype(int)\n",
    "\n",
    "            # if n == 1:\n",
    "            #     print(f'random values: {random_values}')\n",
    "            #     print(f'preds after: {preds}')\n",
    "\n",
    "            # replace preds with full seed of winning team\n",
    "            preds = np.where(preds > 0, round_df['A_FullSeed'], round_df['B_FullSeed'])\n",
    "\n",
    "            for slot, winner_seed in zip(slots_round['Slot'], preds):\n",
    "                # save results to result_df\n",
    "                result_df.loc[len(result_df.index)] = [slot, winner_seed]\n",
    "\n",
    "            # edit slots df for next round\n",
    "            if i != 6:\n",
    "                next_round_slots = slots[slots['Slot'].str.contains(f'R{i+1}')]\n",
    "\n",
    "                for idx, row in next_round_slots.iterrows():\n",
    "                    # get the teams playing in that slot for the next round\n",
    "                    team1 = result_df[result_df['Slot'] == row['StrongSeed']]['Team'].values[0]\n",
    "                    team2 = result_df[result_df['Slot'] == row['WeakSeed']]['Team'].values[0]\n",
    "\n",
    "                    # update the slots df\n",
    "                    slots.loc[slots['Slot'] == row['Slot'], 'StrongSeed'] = team1\n",
    "                    slots.loc[slots['Slot'] == row['Slot'], 'WeakSeed'] = team2\n",
    "\n",
    "            # drop teams that have been eliminated\n",
    "            # features = features[features['FullSeed'].isin(result_df['Team'])].reset_index(drop=True)\n",
    "\n",
    "        # add bracket col\n",
    "        result_df['Bracket'] = n\n",
    "\n",
    "        # append to all_brackets\n",
    "        all_brackets = pd.concat([all_brackets, result_df], axis=0)\n",
    "\n",
    "    # add tournament col\n",
    "    all_brackets['Tournament'] = tournament\n",
    "\n",
    "    return all_brackets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model R2: 0.4284\n",
      "Model RMSE: 11.1122\n",
      "Model Accuracy: 0.7192\n"
     ]
    }
   ],
   "source": [
    "# train best men's model on entire dataset\n",
    "mmodel_reg = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# sort features\n",
    "mfeatures = mfeatures[sorted_cols]\n",
    "\n",
    "# define X and y\n",
    "X = mfeatures.drop(columns=['A_TeamID', 'A_FullSeed', 'B_TeamID', 'B_FullSeed', 'score_diff_adj', 'win'])\n",
    "y = mfeatures['score_diff_adj']\n",
    "\n",
    "# scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# fit model\n",
    "mmodel_reg.fit(X_scaled, y)\n",
    "\n",
    "# get train metrics\n",
    "train_preds = mmodel_reg.predict(X_scaled)\n",
    "r2 = r2_score(y, train_preds)\n",
    "rmse = mean_squared_error(y, train_preds, squared=False)\n",
    "acc = accuracy_score(np.sign(y), np.sign(train_preds))\n",
    "\n",
    "print(f'Model R2: {r2:.4f}')\n",
    "print(f'Model RMSE: {rmse:.4f}')\n",
    "print(f'Model Accuracy: {acc:.4f}')\n",
    "\n",
    "# delete vars\n",
    "del X, y, X_scaled, train_preds, r2, rmse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Log Loss: 9.5694\n",
      "Model Accuracy: 0.7229\n"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "mmodel_class = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "# sort features\n",
    "mfeatures = mfeatures[sorted_cols]\n",
    "\n",
    "# define X and y\n",
    "X = mfeatures.drop(columns=['A_TeamID', 'A_FullSeed', 'B_TeamID', 'B_FullSeed', 'score_diff_adj', 'win'])\n",
    "y = mfeatures['win']\n",
    "\n",
    "# scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# fit model\n",
    "mmodel_class.fit(X_scaled, y)\n",
    "\n",
    "# get train metrics\n",
    "train_preds = mmodel_class.predict(X_scaled)\n",
    "loss = log_loss(y, train_preds)\n",
    "acc = accuracy_score(y, train_preds)\n",
    "\n",
    "print(f'Model Log Loss: {loss:.4f}')\n",
    "print(f'Model Accuracy: {acc:.4f}')\n",
    "\n",
    "# delete vars\n",
    "del X, y, X_scaled, train_preds, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate men's bracket\n",
    "# result_m_reg = generate_bracket(data=mdf_2024, estimator=mmodel_reg, tournament='M')\n",
    "m_brackets = generate_bracket(data=mdf_2024, estimator=mmodel_class, tournament='M', num_brackets=1)\n",
    "\n",
    "# delete vars\n",
    "# del mmodel_reg, mmodel_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Slot</th>\n",
       "      <td>R1W1</td>\n",
       "      <td>R1W2</td>\n",
       "      <td>R1W3</td>\n",
       "      <td>R1W4</td>\n",
       "      <td>R1W5</td>\n",
       "      <td>R1W6</td>\n",
       "      <td>R1W7</td>\n",
       "      <td>R1W8</td>\n",
       "      <td>R1X1</td>\n",
       "      <td>R1X2</td>\n",
       "      <td>R1X3</td>\n",
       "      <td>R1X4</td>\n",
       "      <td>R1X5</td>\n",
       "      <td>R1X6</td>\n",
       "      <td>R1X7</td>\n",
       "      <td>R1X8</td>\n",
       "      <td>R1Y1</td>\n",
       "      <td>R1Y2</td>\n",
       "      <td>R1Y3</td>\n",
       "      <td>R1Y4</td>\n",
       "      <td>R1Y5</td>\n",
       "      <td>R1Y6</td>\n",
       "      <td>R1Y7</td>\n",
       "      <td>R1Y8</td>\n",
       "      <td>R1Z1</td>\n",
       "      <td>R1Z2</td>\n",
       "      <td>R1Z3</td>\n",
       "      <td>R1Z4</td>\n",
       "      <td>R1Z5</td>\n",
       "      <td>R1Z6</td>\n",
       "      <td>R1Z7</td>\n",
       "      <td>R1Z8</td>\n",
       "      <td>R2W1</td>\n",
       "      <td>R2W2</td>\n",
       "      <td>R2W3</td>\n",
       "      <td>R2W4</td>\n",
       "      <td>R2X1</td>\n",
       "      <td>R2X2</td>\n",
       "      <td>R2X3</td>\n",
       "      <td>R2X4</td>\n",
       "      <td>R2Y1</td>\n",
       "      <td>R2Y2</td>\n",
       "      <td>R2Y3</td>\n",
       "      <td>R2Y4</td>\n",
       "      <td>R2Z1</td>\n",
       "      <td>R2Z2</td>\n",
       "      <td>R2Z3</td>\n",
       "      <td>R2Z4</td>\n",
       "      <td>R3W1</td>\n",
       "      <td>R3W2</td>\n",
       "      <td>R3X1</td>\n",
       "      <td>R3X2</td>\n",
       "      <td>R3Y1</td>\n",
       "      <td>R3Y2</td>\n",
       "      <td>R3Z1</td>\n",
       "      <td>R3Z2</td>\n",
       "      <td>R4W1</td>\n",
       "      <td>R4X1</td>\n",
       "      <td>R4Y1</td>\n",
       "      <td>R4Z1</td>\n",
       "      <td>R5WX</td>\n",
       "      <td>R5YZ</td>\n",
       "      <td>R6CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Team</th>\n",
       "      <td>W01</td>\n",
       "      <td>W02</td>\n",
       "      <td>W03</td>\n",
       "      <td>W04</td>\n",
       "      <td>W12</td>\n",
       "      <td>W06</td>\n",
       "      <td>W10</td>\n",
       "      <td>W09</td>\n",
       "      <td>X01</td>\n",
       "      <td>X15</td>\n",
       "      <td>X14</td>\n",
       "      <td>X04</td>\n",
       "      <td>X12</td>\n",
       "      <td>X11</td>\n",
       "      <td>X10</td>\n",
       "      <td>X08</td>\n",
       "      <td>Y01</td>\n",
       "      <td>Y02</td>\n",
       "      <td>Y14</td>\n",
       "      <td>Y04</td>\n",
       "      <td>Y12</td>\n",
       "      <td>Y11</td>\n",
       "      <td>Y10</td>\n",
       "      <td>Y09</td>\n",
       "      <td>Z01</td>\n",
       "      <td>Z02</td>\n",
       "      <td>Z14</td>\n",
       "      <td>Z13</td>\n",
       "      <td>Z12</td>\n",
       "      <td>Z11</td>\n",
       "      <td>Z07</td>\n",
       "      <td>Z09</td>\n",
       "      <td>W01</td>\n",
       "      <td>W02</td>\n",
       "      <td>W06</td>\n",
       "      <td>W04</td>\n",
       "      <td>X01</td>\n",
       "      <td>X10</td>\n",
       "      <td>X11</td>\n",
       "      <td>X04</td>\n",
       "      <td>Y01</td>\n",
       "      <td>Y02</td>\n",
       "      <td>Y11</td>\n",
       "      <td>Y04</td>\n",
       "      <td>Z01</td>\n",
       "      <td>Z02</td>\n",
       "      <td>Z11</td>\n",
       "      <td>Z12</td>\n",
       "      <td>W01</td>\n",
       "      <td>W06</td>\n",
       "      <td>X01</td>\n",
       "      <td>X11</td>\n",
       "      <td>Y04</td>\n",
       "      <td>Y02</td>\n",
       "      <td>Z12</td>\n",
       "      <td>Z02</td>\n",
       "      <td>W01</td>\n",
       "      <td>X01</td>\n",
       "      <td>Y02</td>\n",
       "      <td>Z12</td>\n",
       "      <td>W01</td>\n",
       "      <td>Z12</td>\n",
       "      <td>W01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bracket</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tournament</th>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1     2     3     4     5     6     7     8     9     10  \\\n",
       "Slot        R1W1  R1W2  R1W3  R1W4  R1W5  R1W6  R1W7  R1W8  R1X1  R1X2  R1X3   \n",
       "Team         W01   W02   W03   W04   W12   W06   W10   W09   X01   X15   X14   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     M     M     M     M     M     M     M     M     M     M     M   \n",
       "\n",
       "              11    12    13    14    15    16    17    18    19    20    21  \\\n",
       "Slot        R1X4  R1X5  R1X6  R1X7  R1X8  R1Y1  R1Y2  R1Y3  R1Y4  R1Y5  R1Y6   \n",
       "Team         X04   X12   X11   X10   X08   Y01   Y02   Y14   Y04   Y12   Y11   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     M     M     M     M     M     M     M     M     M     M     M   \n",
       "\n",
       "              22    23    24    25    26    27    28    29    30    31    32  \\\n",
       "Slot        R1Y7  R1Y8  R1Z1  R1Z2  R1Z3  R1Z4  R1Z5  R1Z6  R1Z7  R1Z8  R2W1   \n",
       "Team         Y10   Y09   Z01   Z02   Z14   Z13   Z12   Z11   Z07   Z09   W01   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     M     M     M     M     M     M     M     M     M     M     M   \n",
       "\n",
       "              33    34    35    36    37    38    39    40    41    42    43  \\\n",
       "Slot        R2W2  R2W3  R2W4  R2X1  R2X2  R2X3  R2X4  R2Y1  R2Y2  R2Y3  R2Y4   \n",
       "Team         W02   W06   W04   X01   X10   X11   X04   Y01   Y02   Y11   Y04   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     M     M     M     M     M     M     M     M     M     M     M   \n",
       "\n",
       "              44    45    46    47    48    49    50    51    52    53    54  \\\n",
       "Slot        R2Z1  R2Z2  R2Z3  R2Z4  R3W1  R3W2  R3X1  R3X2  R3Y1  R3Y2  R3Z1   \n",
       "Team         Z01   Z02   Z11   Z12   W01   W06   X01   X11   Y04   Y02   Z12   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     M     M     M     M     M     M     M     M     M     M     M   \n",
       "\n",
       "              55    56    57    58    59    60    61    62  \n",
       "Slot        R3Z2  R4W1  R4X1  R4Y1  R4Z1  R5WX  R5YZ  R6CH  \n",
       "Team         Z02   W01   X01   Y02   Z12   W01   Z12   W01  \n",
       "Bracket        1     1     1     1     1     1     1     1  \n",
       "Tournament     M     M     M     M     M     M     M     M  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_brackets.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Women's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model R2: 0.6467\n",
      "Model RMSE: 12.3932\n",
      "Model Accuracy: 0.7956\n"
     ]
    }
   ],
   "source": [
    "# train best men's model on entire dataset\n",
    "wmodel_reg = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# define X and y\n",
    "X = wfeatures.drop(columns=['A_TeamID', 'A_FullSeed', 'B_TeamID', 'B_FullSeed', 'score_diff_adj', 'win'])\n",
    "y = wfeatures['score_diff_adj']\n",
    "\n",
    "# scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# fit model\n",
    "wmodel_reg.fit(X_scaled, y)\n",
    "\n",
    "# get train metrics\n",
    "train_preds = wmodel_reg.predict(X_scaled)\n",
    "r2 = r2_score(y, train_preds)\n",
    "rmse = mean_squared_error(y, train_preds, squared=False)\n",
    "acc = accuracy_score(np.sign(y), np.sign(train_preds))\n",
    "\n",
    "print(f'Model R2: {r2:.4f}')\n",
    "print(f'Model RMSE: {rmse:.4f}')\n",
    "print(f'Model Accuracy: {acc:.4f}')\n",
    "\n",
    "# delete vars\n",
    "del X, y, X_scaled, train_preds, r2, rmse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Log Loss: 6.8640\n",
      "Model Accuracy: 0.8013\n"
     ]
    }
   ],
   "source": [
    "# classification\n",
    "wmodel_class = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# define X and y\n",
    "X = wfeatures.drop(columns=['A_TeamID', 'A_FullSeed', 'B_TeamID', 'B_FullSeed', 'score_diff_adj', 'win'])\n",
    "y = wfeatures['win']\n",
    "\n",
    "# scale data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# fit model\n",
    "wmodel_class.fit(X_scaled, y)\n",
    "\n",
    "# get train metrics\n",
    "train_preds = wmodel_class.predict(X_scaled)\n",
    "loss = log_loss(y, train_preds)\n",
    "acc = accuracy_score(y, train_preds)\n",
    "\n",
    "print(f'Model Log Loss: {loss:.4f}')\n",
    "print(f'Model Accuracy: {acc:.4f}')\n",
    "\n",
    "# delete vars\n",
    "del X, y, X_scaled, train_preds, loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate women's bracket\n",
    "# result_w_reg = generate_bracket(data=wdf_2024, estimator=wmodel_reg, tournament='W')\n",
    "w_brackets = generate_bracket(data=wdf_2024, estimator=wmodel_class, tournament='W', num_brackets=1)\n",
    "\n",
    "# delete vars\n",
    "# del wmodel_reg, wmodel_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Slot</th>\n",
       "      <td>R1W1</td>\n",
       "      <td>R1W2</td>\n",
       "      <td>R1W3</td>\n",
       "      <td>R1W4</td>\n",
       "      <td>R1W5</td>\n",
       "      <td>R1W6</td>\n",
       "      <td>R1W7</td>\n",
       "      <td>R1W8</td>\n",
       "      <td>R1X1</td>\n",
       "      <td>R1X2</td>\n",
       "      <td>R1X3</td>\n",
       "      <td>R1X4</td>\n",
       "      <td>R1X5</td>\n",
       "      <td>R1X6</td>\n",
       "      <td>R1X7</td>\n",
       "      <td>R1X8</td>\n",
       "      <td>R1Y1</td>\n",
       "      <td>R1Y2</td>\n",
       "      <td>R1Y3</td>\n",
       "      <td>R1Y4</td>\n",
       "      <td>R1Y5</td>\n",
       "      <td>R1Y6</td>\n",
       "      <td>R1Y7</td>\n",
       "      <td>R1Y8</td>\n",
       "      <td>R1Z1</td>\n",
       "      <td>R1Z2</td>\n",
       "      <td>R1Z3</td>\n",
       "      <td>R1Z4</td>\n",
       "      <td>R1Z5</td>\n",
       "      <td>R1Z6</td>\n",
       "      <td>R1Z7</td>\n",
       "      <td>R1Z8</td>\n",
       "      <td>R2W1</td>\n",
       "      <td>R2W2</td>\n",
       "      <td>R2W3</td>\n",
       "      <td>R2W4</td>\n",
       "      <td>R2X1</td>\n",
       "      <td>R2X2</td>\n",
       "      <td>R2X3</td>\n",
       "      <td>R2X4</td>\n",
       "      <td>R2Y1</td>\n",
       "      <td>R2Y2</td>\n",
       "      <td>R2Y3</td>\n",
       "      <td>R2Y4</td>\n",
       "      <td>R2Z1</td>\n",
       "      <td>R2Z2</td>\n",
       "      <td>R2Z3</td>\n",
       "      <td>R2Z4</td>\n",
       "      <td>R3W1</td>\n",
       "      <td>R3W2</td>\n",
       "      <td>R3X1</td>\n",
       "      <td>R3X2</td>\n",
       "      <td>R3Y1</td>\n",
       "      <td>R3Y2</td>\n",
       "      <td>R3Z1</td>\n",
       "      <td>R3Z2</td>\n",
       "      <td>R4W1</td>\n",
       "      <td>R4X1</td>\n",
       "      <td>R4Y1</td>\n",
       "      <td>R4Z1</td>\n",
       "      <td>R5WX</td>\n",
       "      <td>R5YZ</td>\n",
       "      <td>R6CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Team</th>\n",
       "      <td>W01</td>\n",
       "      <td>W02</td>\n",
       "      <td>W03</td>\n",
       "      <td>W04</td>\n",
       "      <td>W05</td>\n",
       "      <td>W11</td>\n",
       "      <td>W10</td>\n",
       "      <td>W09</td>\n",
       "      <td>X01</td>\n",
       "      <td>X02</td>\n",
       "      <td>X03</td>\n",
       "      <td>X04</td>\n",
       "      <td>X12</td>\n",
       "      <td>X06</td>\n",
       "      <td>X10</td>\n",
       "      <td>X09</td>\n",
       "      <td>Y01</td>\n",
       "      <td>Y02</td>\n",
       "      <td>Y03</td>\n",
       "      <td>Y04</td>\n",
       "      <td>Y12</td>\n",
       "      <td>Y11</td>\n",
       "      <td>Y10</td>\n",
       "      <td>Y09</td>\n",
       "      <td>Z01</td>\n",
       "      <td>Z02</td>\n",
       "      <td>Z03</td>\n",
       "      <td>Z04</td>\n",
       "      <td>Z05</td>\n",
       "      <td>Z06</td>\n",
       "      <td>Z10</td>\n",
       "      <td>Z09</td>\n",
       "      <td>W01</td>\n",
       "      <td>W02</td>\n",
       "      <td>W03</td>\n",
       "      <td>W05</td>\n",
       "      <td>X01</td>\n",
       "      <td>X02</td>\n",
       "      <td>X06</td>\n",
       "      <td>X04</td>\n",
       "      <td>Y01</td>\n",
       "      <td>Y02</td>\n",
       "      <td>Y03</td>\n",
       "      <td>Y04</td>\n",
       "      <td>Z01</td>\n",
       "      <td>Z10</td>\n",
       "      <td>Z03</td>\n",
       "      <td>Z05</td>\n",
       "      <td>W01</td>\n",
       "      <td>W02</td>\n",
       "      <td>X04</td>\n",
       "      <td>X02</td>\n",
       "      <td>Y01</td>\n",
       "      <td>Y02</td>\n",
       "      <td>Z01</td>\n",
       "      <td>Z03</td>\n",
       "      <td>W01</td>\n",
       "      <td>X02</td>\n",
       "      <td>Y01</td>\n",
       "      <td>Z03</td>\n",
       "      <td>W01</td>\n",
       "      <td>Y01</td>\n",
       "      <td>Y01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bracket</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tournament</th>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     1     2     3     4     5     6     7     8     9     10  \\\n",
       "Slot        R1W1  R1W2  R1W3  R1W4  R1W5  R1W6  R1W7  R1W8  R1X1  R1X2  R1X3   \n",
       "Team         W01   W02   W03   W04   W05   W11   W10   W09   X01   X02   X03   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     W     W     W     W     W     W     W     W     W     W     W   \n",
       "\n",
       "              11    12    13    14    15    16    17    18    19    20    21  \\\n",
       "Slot        R1X4  R1X5  R1X6  R1X7  R1X8  R1Y1  R1Y2  R1Y3  R1Y4  R1Y5  R1Y6   \n",
       "Team         X04   X12   X06   X10   X09   Y01   Y02   Y03   Y04   Y12   Y11   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     W     W     W     W     W     W     W     W     W     W     W   \n",
       "\n",
       "              22    23    24    25    26    27    28    29    30    31    32  \\\n",
       "Slot        R1Y7  R1Y8  R1Z1  R1Z2  R1Z3  R1Z4  R1Z5  R1Z6  R1Z7  R1Z8  R2W1   \n",
       "Team         Y10   Y09   Z01   Z02   Z03   Z04   Z05   Z06   Z10   Z09   W01   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     W     W     W     W     W     W     W     W     W     W     W   \n",
       "\n",
       "              33    34    35    36    37    38    39    40    41    42    43  \\\n",
       "Slot        R2W2  R2W3  R2W4  R2X1  R2X2  R2X3  R2X4  R2Y1  R2Y2  R2Y3  R2Y4   \n",
       "Team         W02   W03   W05   X01   X02   X06   X04   Y01   Y02   Y03   Y04   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     W     W     W     W     W     W     W     W     W     W     W   \n",
       "\n",
       "              44    45    46    47    48    49    50    51    52    53    54  \\\n",
       "Slot        R2Z1  R2Z2  R2Z3  R2Z4  R3W1  R3W2  R3X1  R3X2  R3Y1  R3Y2  R3Z1   \n",
       "Team         Z01   Z10   Z03   Z05   W01   W02   X04   X02   Y01   Y02   Z01   \n",
       "Bracket        1     1     1     1     1     1     1     1     1     1     1   \n",
       "Tournament     W     W     W     W     W     W     W     W     W     W     W   \n",
       "\n",
       "              55    56    57    58    59    60    61    62  \n",
       "Slot        R3Z2  R4W1  R4X1  R4Y1  R4Z1  R5WX  R5YZ  R6CH  \n",
       "Team         Z03   W01   X02   Y01   Z03   W01   Y01   Y01  \n",
       "Bracket        1     1     1     1     1     1     1     1  \n",
       "Tournament     W     W     W     W     W     W     W     W  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_brackets.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num brackets\n",
    "n_brackets = 100000\n",
    "\n",
    "# combine results\n",
    "submission = pd.concat([result_m_class, result_w_class])\n",
    "submission = submission.reset_index(drop=True)\n",
    "submission.index.names = ['RowId']\n",
    "\n",
    "# reorder\n",
    "submission = submission[['Tournament', 'Bracket', 'Slot', 'Team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "submission.to_csv('submission.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_1_score_game_missing\n",
      "A_1_score_game_ratio\n",
      "A_1_score_win_ratio\n",
      "A_PlayIn\n",
      "A_Seed\n",
      "A_avg_Ast_against\n",
      "A_avg_Ast_for\n",
      "A_avg_Blk_against\n",
      "A_avg_Blk_for\n",
      "A_avg_DR_against\n",
      "A_avg_DR_for\n",
      "A_avg_FGA3_against\n",
      "A_avg_FGA3_for\n",
      "A_avg_FGA_against\n",
      "A_avg_FGA_for\n",
      "A_avg_FGM3_against\n",
      "A_avg_FGM3_for\n",
      "A_avg_FGM_against\n",
      "A_avg_FGM_for\n",
      "A_avg_FTA_against\n",
      "A_avg_FTA_for\n",
      "A_avg_FTM_against\n",
      "A_avg_FTM_for\n",
      "A_avg_OR_against\n",
      "A_avg_OR_for\n",
      "A_avg_PF_against\n",
      "A_avg_PF_for\n",
      "A_avg_Score_against\n",
      "A_avg_Score_for\n",
      "A_avg_Stl_against\n",
      "A_avg_Stl_for\n",
      "A_avg_TO_against\n",
      "A_avg_TO_for\n",
      "A_avg_diff\n",
      "A_away_missing\n",
      "A_away_win_ratio\n",
      "A_conf_champs\n",
      "A_home_missing\n",
      "A_home_win_ratio\n",
      "A_neutral_missing\n",
      "A_neutral_win_ratio\n",
      "A_num_games\n",
      "A_ot_game_missing\n",
      "A_ot_ratio\n",
      "A_ot_win_ratio\n",
      "A_recent_avg_Ast_against\n",
      "A_recent_avg_Ast_for\n",
      "A_recent_avg_Blk_against\n",
      "A_recent_avg_Blk_for\n",
      "A_recent_avg_DR_against\n",
      "A_recent_avg_DR_for\n",
      "A_recent_avg_FGA3_against\n",
      "A_recent_avg_FGA3_for\n",
      "A_recent_avg_FGA_against\n",
      "A_recent_avg_FGA_for\n",
      "A_recent_avg_FGM3_against\n",
      "A_recent_avg_FGM3_for\n",
      "A_recent_avg_FGM_against\n",
      "A_recent_avg_FGM_for\n",
      "A_recent_avg_FTA_against\n",
      "A_recent_avg_FTA_for\n",
      "A_recent_avg_FTM_against\n",
      "A_recent_avg_FTM_for\n",
      "A_recent_avg_OR_against\n",
      "A_recent_avg_OR_for\n",
      "A_recent_avg_PF_against\n",
      "A_recent_avg_PF_for\n",
      "A_recent_avg_Score_against\n",
      "A_recent_avg_Score_for\n",
      "A_recent_avg_Stl_against\n",
      "A_recent_avg_Stl_for\n",
      "A_recent_avg_TO_against\n",
      "A_recent_avg_TO_for\n",
      "A_recent_avg_diff\n",
      "A_recent_std_Ast_against\n",
      "A_recent_std_Ast_for\n",
      "A_recent_std_Blk_against\n",
      "A_recent_std_Blk_for\n",
      "A_recent_std_DR_against\n",
      "A_recent_std_DR_for\n",
      "A_recent_std_FGA3_against\n",
      "A_recent_std_FGA3_for\n",
      "A_recent_std_FGA_against\n",
      "A_recent_std_FGA_for\n",
      "A_recent_std_FGM3_against\n",
      "A_recent_std_FGM3_for\n",
      "A_recent_std_FGM_against\n",
      "A_recent_std_FGM_for\n",
      "A_recent_std_FTA_against\n",
      "A_recent_std_FTA_for\n",
      "A_recent_std_FTM_against\n",
      "A_recent_std_FTM_for\n",
      "A_recent_std_OR_against\n",
      "A_recent_std_OR_for\n",
      "A_recent_std_PF_against\n",
      "A_recent_std_PF_for\n",
      "A_recent_std_Score_against\n",
      "A_recent_std_Score_for\n",
      "A_recent_std_Stl_against\n",
      "A_recent_std_Stl_for\n",
      "A_recent_std_TO_against\n",
      "A_recent_std_TO_for\n",
      "A_recent_win_ratio\n",
      "A_std_Ast_against\n",
      "A_std_Ast_for\n",
      "A_std_Blk_against\n",
      "A_std_Blk_for\n",
      "A_std_DR_against\n",
      "A_std_DR_for\n",
      "A_std_FGA3_against\n",
      "A_std_FGA3_for\n",
      "A_std_FGA_against\n",
      "A_std_FGA_for\n",
      "A_std_FGM3_against\n",
      "A_std_FGM3_for\n",
      "A_std_FGM_against\n",
      "A_std_FGM_for\n",
      "A_std_FTA_against\n",
      "A_std_FTA_for\n",
      "A_std_FTM_against\n",
      "A_std_FTM_for\n",
      "A_std_OR_against\n",
      "A_std_OR_for\n",
      "A_std_PF_against\n",
      "A_std_PF_for\n",
      "A_std_Score_against\n",
      "A_std_Score_for\n",
      "A_std_Stl_against\n",
      "A_std_Stl_for\n",
      "A_std_TO_against\n",
      "A_std_TO_for\n",
      "A_std_diff\n",
      "A_win_ratio\n",
      "B_1_score_game_missing\n",
      "B_1_score_game_ratio\n",
      "B_1_score_win_ratio\n",
      "B_PlayIn\n",
      "B_Seed\n",
      "B_avg_Ast_against\n",
      "B_avg_Ast_for\n",
      "B_avg_Blk_against\n",
      "B_avg_Blk_for\n",
      "B_avg_DR_against\n",
      "B_avg_DR_for\n",
      "B_avg_FGA3_against\n",
      "B_avg_FGA3_for\n",
      "B_avg_FGA_against\n",
      "B_avg_FGA_for\n",
      "B_avg_FGM3_against\n",
      "B_avg_FGM3_for\n",
      "B_avg_FGM_against\n",
      "B_avg_FGM_for\n",
      "B_avg_FTA_against\n",
      "B_avg_FTA_for\n",
      "B_avg_FTM_against\n",
      "B_avg_FTM_for\n",
      "B_avg_OR_against\n",
      "B_avg_OR_for\n",
      "B_avg_PF_against\n",
      "B_avg_PF_for\n",
      "B_avg_Score_against\n",
      "B_avg_Score_for\n",
      "B_avg_Stl_against\n",
      "B_avg_Stl_for\n",
      "B_avg_TO_against\n",
      "B_avg_TO_for\n",
      "B_avg_diff\n",
      "B_away_missing\n",
      "B_away_win_ratio\n",
      "B_conf_champs\n",
      "B_home_missing\n",
      "B_home_win_ratio\n",
      "B_neutral_missing\n",
      "B_neutral_win_ratio\n",
      "B_num_games\n",
      "B_ot_game_missing\n",
      "B_ot_ratio\n",
      "B_ot_win_ratio\n",
      "B_recent_avg_Ast_against\n",
      "B_recent_avg_Ast_for\n",
      "B_recent_avg_Blk_against\n",
      "B_recent_avg_Blk_for\n",
      "B_recent_avg_DR_against\n",
      "B_recent_avg_DR_for\n",
      "B_recent_avg_FGA3_against\n",
      "B_recent_avg_FGA3_for\n",
      "B_recent_avg_FGA_against\n",
      "B_recent_avg_FGA_for\n",
      "B_recent_avg_FGM3_against\n",
      "B_recent_avg_FGM3_for\n",
      "B_recent_avg_FGM_against\n",
      "B_recent_avg_FGM_for\n",
      "B_recent_avg_FTA_against\n",
      "B_recent_avg_FTA_for\n",
      "B_recent_avg_FTM_against\n",
      "B_recent_avg_FTM_for\n",
      "B_recent_avg_OR_against\n",
      "B_recent_avg_OR_for\n",
      "B_recent_avg_PF_against\n",
      "B_recent_avg_PF_for\n",
      "B_recent_avg_Score_against\n",
      "B_recent_avg_Score_for\n",
      "B_recent_avg_Stl_against\n",
      "B_recent_avg_Stl_for\n",
      "B_recent_avg_TO_against\n",
      "B_recent_avg_TO_for\n",
      "B_recent_avg_diff\n",
      "B_recent_std_Ast_against\n",
      "B_recent_std_Ast_for\n",
      "B_recent_std_Blk_against\n",
      "B_recent_std_Blk_for\n",
      "B_recent_std_DR_against\n",
      "B_recent_std_DR_for\n",
      "B_recent_std_FGA3_against\n",
      "B_recent_std_FGA3_for\n",
      "B_recent_std_FGA_against\n",
      "B_recent_std_FGA_for\n",
      "B_recent_std_FGM3_against\n",
      "B_recent_std_FGM3_for\n",
      "B_recent_std_FGM_against\n",
      "B_recent_std_FGM_for\n",
      "B_recent_std_FTA_against\n",
      "B_recent_std_FTA_for\n",
      "B_recent_std_FTM_against\n",
      "B_recent_std_FTM_for\n",
      "B_recent_std_OR_against\n",
      "B_recent_std_OR_for\n",
      "B_recent_std_PF_against\n",
      "B_recent_std_PF_for\n",
      "B_recent_std_Score_against\n",
      "B_recent_std_Score_for\n",
      "B_recent_std_Stl_against\n",
      "B_recent_std_Stl_for\n",
      "B_recent_std_TO_against\n",
      "B_recent_std_TO_for\n",
      "B_recent_win_ratio\n",
      "B_std_Ast_against\n",
      "B_std_Ast_for\n",
      "B_std_Blk_against\n",
      "B_std_Blk_for\n",
      "B_std_DR_against\n",
      "B_std_DR_for\n",
      "B_std_FGA3_against\n",
      "B_std_FGA3_for\n",
      "B_std_FGA_against\n",
      "B_std_FGA_for\n",
      "B_std_FGM3_against\n",
      "B_std_FGM3_for\n",
      "B_std_FGM_against\n",
      "B_std_FGM_for\n",
      "B_std_FTA_against\n",
      "B_std_FTA_for\n",
      "B_std_FTM_against\n",
      "B_std_FTM_for\n",
      "B_std_OR_against\n",
      "B_std_OR_for\n",
      "B_std_PF_against\n",
      "B_std_PF_for\n",
      "B_std_Score_against\n",
      "B_std_Score_for\n",
      "B_std_Stl_against\n",
      "B_std_Stl_for\n",
      "B_std_TO_against\n",
      "B_std_TO_for\n",
      "B_std_diff\n",
      "B_win_ratio\n",
      "A_Loc_A\n",
      "A_Loc_H\n",
      "round_2\n",
      "round_3\n",
      "round_4\n",
      "round_5\n",
      "round_6\n",
      "A_score_diff_adj\n",
      "A_win\n"
     ]
    }
   ],
   "source": [
    "for col in mfeatures.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
