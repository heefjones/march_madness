{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling (Detailed)\n",
    "In this notebook, we:\n",
    "- Try different models on the [detailed features](./eda_detailed.ipynb) and assess their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2204, 90)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG2_pct_x</th>\n",
       "      <th>FG2_pct_y</th>\n",
       "      <th>FG3_pct_x</th>\n",
       "      <th>FG3_pct_y</th>\n",
       "      <th>FT_pct_x</th>\n",
       "      <th>FT_pct_y</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>PlayIn_x</th>\n",
       "      <th>PlayIn_y</th>\n",
       "      <th>Score_x</th>\n",
       "      <th>Score_y</th>\n",
       "      <th>Season</th>\n",
       "      <th>Seed_num_x</th>\n",
       "      <th>Seed_num_y</th>\n",
       "      <th>TeamID_x</th>\n",
       "      <th>TeamID_y</th>\n",
       "      <th>close_games_pct_x</th>\n",
       "      <th>close_games_pct_y</th>\n",
       "      <th>close_games_win_pct_x</th>\n",
       "      <th>close_games_win_pct_y</th>\n",
       "      <th>mean_FG2A_x</th>\n",
       "      <th>mean_FG2A_y</th>\n",
       "      <th>mean_FG3A_x</th>\n",
       "      <th>mean_FG3A_y</th>\n",
       "      <th>mean_FTA_x</th>\n",
       "      <th>mean_FTA_y</th>\n",
       "      <th>mean_TO_against_x</th>\n",
       "      <th>mean_TO_against_y</th>\n",
       "      <th>mean_TO_x</th>\n",
       "      <th>mean_TO_y</th>\n",
       "      <th>mean_ast_against_x</th>\n",
       "      <th>mean_ast_against_y</th>\n",
       "      <th>mean_ast_x</th>\n",
       "      <th>mean_ast_y</th>\n",
       "      <th>mean_blk_against_x</th>\n",
       "      <th>mean_blk_against_y</th>\n",
       "      <th>mean_blk_x</th>\n",
       "      <th>mean_blk_y</th>\n",
       "      <th>mean_fouls_against_x</th>\n",
       "      <th>mean_fouls_against_y</th>\n",
       "      <th>mean_fouls_x</th>\n",
       "      <th>mean_fouls_y</th>\n",
       "      <th>mean_pts_against_x</th>\n",
       "      <th>mean_pts_against_y</th>\n",
       "      <th>mean_pts_x</th>\n",
       "      <th>mean_pts_y</th>\n",
       "      <th>mean_reb_against_x</th>\n",
       "      <th>mean_reb_against_y</th>\n",
       "      <th>mean_reb_x</th>\n",
       "      <th>mean_reb_y</th>\n",
       "      <th>mean_score_diff_x</th>\n",
       "      <th>mean_score_diff_y</th>\n",
       "      <th>mean_stl_against_x</th>\n",
       "      <th>mean_stl_against_y</th>\n",
       "      <th>mean_stl_x</th>\n",
       "      <th>mean_stl_y</th>\n",
       "      <th>num_games_x</th>\n",
       "      <th>num_games_y</th>\n",
       "      <th>std_TO_against_x</th>\n",
       "      <th>std_TO_against_y</th>\n",
       "      <th>std_TO_x</th>\n",
       "      <th>std_TO_y</th>\n",
       "      <th>std_ast_against_x</th>\n",
       "      <th>std_ast_against_y</th>\n",
       "      <th>std_ast_x</th>\n",
       "      <th>std_ast_y</th>\n",
       "      <th>std_blk_against_x</th>\n",
       "      <th>std_blk_against_y</th>\n",
       "      <th>std_blk_x</th>\n",
       "      <th>std_blk_y</th>\n",
       "      <th>std_fouls_against_x</th>\n",
       "      <th>std_fouls_against_y</th>\n",
       "      <th>std_fouls_x</th>\n",
       "      <th>std_fouls_y</th>\n",
       "      <th>std_pts_against_x</th>\n",
       "      <th>std_pts_against_y</th>\n",
       "      <th>std_pts_x</th>\n",
       "      <th>std_pts_y</th>\n",
       "      <th>std_reb_against_x</th>\n",
       "      <th>std_reb_against_y</th>\n",
       "      <th>std_reb_x</th>\n",
       "      <th>std_reb_y</th>\n",
       "      <th>std_score_diff_x</th>\n",
       "      <th>std_score_diff_y</th>\n",
       "      <th>std_stl_against_x</th>\n",
       "      <th>std_stl_against_y</th>\n",
       "      <th>std_stl_x</th>\n",
       "      <th>std_stl_y</th>\n",
       "      <th>win_pct_x</th>\n",
       "      <th>win_pct_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>0.517343</td>\n",
       "      <td>0.55913</td>\n",
       "      <td>0.348509</td>\n",
       "      <td>0.36236</td>\n",
       "      <td>0.696801</td>\n",
       "      <td>0.715613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>39.852941</td>\n",
       "      <td>34.096774</td>\n",
       "      <td>18.735294</td>\n",
       "      <td>22.967742</td>\n",
       "      <td>21.147059</td>\n",
       "      <td>17.354839</td>\n",
       "      <td>16.764706</td>\n",
       "      <td>10.709677</td>\n",
       "      <td>10.205882</td>\n",
       "      <td>10.935484</td>\n",
       "      <td>12.117647</td>\n",
       "      <td>11.870968</td>\n",
       "      <td>15.735294</td>\n",
       "      <td>12.709677</td>\n",
       "      <td>3.470588</td>\n",
       "      <td>2.967742</td>\n",
       "      <td>3.058824</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18.441176</td>\n",
       "      <td>17.354839</td>\n",
       "      <td>16.470588</td>\n",
       "      <td>14.83871</td>\n",
       "      <td>61.294118</td>\n",
       "      <td>71.645161</td>\n",
       "      <td>75.558824</td>\n",
       "      <td>75.516129</td>\n",
       "      <td>29.794118</td>\n",
       "      <td>31.290323</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.483871</td>\n",
       "      <td>14.264706</td>\n",
       "      <td>3.870968</td>\n",
       "      <td>6.147059</td>\n",
       "      <td>6.709677</td>\n",
       "      <td>10.441176</td>\n",
       "      <td>5.935484</td>\n",
       "      <td>34.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.911698</td>\n",
       "      <td>3.475568</td>\n",
       "      <td>4.318946</td>\n",
       "      <td>3.054346</td>\n",
       "      <td>3.539943</td>\n",
       "      <td>3.730433</td>\n",
       "      <td>4.876094</td>\n",
       "      <td>3.100468</td>\n",
       "      <td>1.98838</td>\n",
       "      <td>2.105293</td>\n",
       "      <td>1.739752</td>\n",
       "      <td>1.770122</td>\n",
       "      <td>3.893983</td>\n",
       "      <td>3.911934</td>\n",
       "      <td>3.543969</td>\n",
       "      <td>3.916604</td>\n",
       "      <td>12.013955</td>\n",
       "      <td>12.210783</td>\n",
       "      <td>13.089879</td>\n",
       "      <td>8.559093</td>\n",
       "      <td>6.104044</td>\n",
       "      <td>4.845572</td>\n",
       "      <td>5.331439</td>\n",
       "      <td>5.993724</td>\n",
       "      <td>18.475715</td>\n",
       "      <td>12.711522</td>\n",
       "      <td>3.211083</td>\n",
       "      <td>2.355045</td>\n",
       "      <td>3.783466</td>\n",
       "      <td>2.24997</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.612903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FG2_pct_x  FG2_pct_y  FG3_pct_x  FG3_pct_y  FT_pct_x  FT_pct_y  NumOT  \\\n",
       "1265   0.517343    0.55913   0.348509    0.36236  0.696801  0.715613    0.0   \n",
       "\n",
       "      PlayIn_x  PlayIn_y  Score_x  Score_y  Season  Seed_num_x  Seed_num_y  \\\n",
       "1265       0.0       0.0     82.0     65.0  2024.0         2.0        15.0   \n",
       "\n",
       "      TeamID_x  TeamID_y  close_games_pct_x  close_games_pct_y  \\\n",
       "1265    1235.0    1355.0           0.058824           0.258065   \n",
       "\n",
       "      close_games_win_pct_x  close_games_win_pct_y  mean_FG2A_x  mean_FG2A_y  \\\n",
       "1265                    0.5                    0.5    39.852941    34.096774   \n",
       "\n",
       "      mean_FG3A_x  mean_FG3A_y  mean_FTA_x  mean_FTA_y  mean_TO_against_x  \\\n",
       "1265    18.735294    22.967742   21.147059   17.354839          16.764706   \n",
       "\n",
       "      mean_TO_against_y  mean_TO_x  mean_TO_y  mean_ast_against_x  \\\n",
       "1265          10.709677  10.205882  10.935484           12.117647   \n",
       "\n",
       "      mean_ast_against_y  mean_ast_x  mean_ast_y  mean_blk_against_x  \\\n",
       "1265           11.870968   15.735294   12.709677            3.470588   \n",
       "\n",
       "      mean_blk_against_y  mean_blk_x  mean_blk_y  mean_fouls_against_x  \\\n",
       "1265            2.967742    3.058824         3.0             18.441176   \n",
       "\n",
       "      mean_fouls_against_y  mean_fouls_x  mean_fouls_y  mean_pts_against_x  \\\n",
       "1265             17.354839     16.470588      14.83871           61.294118   \n",
       "\n",
       "      mean_pts_against_y  mean_pts_x  mean_pts_y  mean_reb_against_x  \\\n",
       "1265           71.645161   75.558824   75.516129           29.794118   \n",
       "\n",
       "      mean_reb_against_y  mean_reb_x  mean_reb_y  mean_score_diff_x  \\\n",
       "1265           31.290323        31.0   31.483871          14.264706   \n",
       "\n",
       "      mean_score_diff_y  mean_stl_against_x  mean_stl_against_y  mean_stl_x  \\\n",
       "1265           3.870968            6.147059            6.709677   10.441176   \n",
       "\n",
       "      mean_stl_y  num_games_x  num_games_y  std_TO_against_x  \\\n",
       "1265    5.935484         34.0         31.0          4.911698   \n",
       "\n",
       "      std_TO_against_y  std_TO_x  std_TO_y  std_ast_against_x  \\\n",
       "1265          3.475568  4.318946  3.054346           3.539943   \n",
       "\n",
       "      std_ast_against_y  std_ast_x  std_ast_y  std_blk_against_x  \\\n",
       "1265           3.730433   4.876094   3.100468            1.98838   \n",
       "\n",
       "      std_blk_against_y  std_blk_x  std_blk_y  std_fouls_against_x  \\\n",
       "1265           2.105293   1.739752   1.770122             3.893983   \n",
       "\n",
       "      std_fouls_against_y  std_fouls_x  std_fouls_y  std_pts_against_x  \\\n",
       "1265             3.911934     3.543969     3.916604          12.013955   \n",
       "\n",
       "      std_pts_against_y  std_pts_x  std_pts_y  std_reb_against_x  \\\n",
       "1265          12.210783  13.089879   8.559093           6.104044   \n",
       "\n",
       "      std_reb_against_y  std_reb_x  std_reb_y  std_score_diff_x  \\\n",
       "1265           4.845572   5.331439   5.993724         18.475715   \n",
       "\n",
       "      std_score_diff_y  std_stl_against_x  std_stl_against_y  std_stl_x  \\\n",
       "1265         12.711522           3.211083           2.355045   3.783466   \n",
       "\n",
       "      std_stl_y  win_pct_x  win_pct_y  \n",
       "1265    2.24997   0.794118   0.612903  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper import *\n",
    "\n",
    "# load in features compact\n",
    "fdet = pd.read_csv('data/cleaned/features_detailed.csv')\n",
    "\n",
    "# sort cols\n",
    "fdet = fdet.reindex(sorted(fdet.columns), axis=1)\n",
    "\n",
    "# view\n",
    "print(fdet.shape)\n",
    "fdet.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flip Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2204, 90)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure team_x always wins\n",
    "fdet.query('Score_x > Score_y').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the winner is always on the left side of the row (team 'x'). To prevent models from learning this positional encoding, we will duplicate rows similar to how we duplicated the regular season data. This will also double our pool of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG2_pct_x</th>\n",
       "      <th>FG2_pct_y</th>\n",
       "      <th>FG3_pct_x</th>\n",
       "      <th>FG3_pct_y</th>\n",
       "      <th>FT_pct_x</th>\n",
       "      <th>FT_pct_y</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>PlayIn_x</th>\n",
       "      <th>PlayIn_y</th>\n",
       "      <th>Score_x</th>\n",
       "      <th>Score_y</th>\n",
       "      <th>Season</th>\n",
       "      <th>Seed_num_x</th>\n",
       "      <th>Seed_num_y</th>\n",
       "      <th>TeamID_x</th>\n",
       "      <th>TeamID_y</th>\n",
       "      <th>close_games_pct_x</th>\n",
       "      <th>close_games_pct_y</th>\n",
       "      <th>close_games_win_pct_x</th>\n",
       "      <th>close_games_win_pct_y</th>\n",
       "      <th>mean_FG2A_x</th>\n",
       "      <th>mean_FG2A_y</th>\n",
       "      <th>mean_FG3A_x</th>\n",
       "      <th>mean_FG3A_y</th>\n",
       "      <th>mean_FTA_x</th>\n",
       "      <th>mean_FTA_y</th>\n",
       "      <th>mean_TO_against_x</th>\n",
       "      <th>mean_TO_against_y</th>\n",
       "      <th>mean_TO_x</th>\n",
       "      <th>mean_TO_y</th>\n",
       "      <th>mean_ast_against_x</th>\n",
       "      <th>mean_ast_against_y</th>\n",
       "      <th>mean_ast_x</th>\n",
       "      <th>mean_ast_y</th>\n",
       "      <th>mean_blk_against_x</th>\n",
       "      <th>mean_blk_against_y</th>\n",
       "      <th>mean_blk_x</th>\n",
       "      <th>mean_blk_y</th>\n",
       "      <th>mean_fouls_against_x</th>\n",
       "      <th>mean_fouls_against_y</th>\n",
       "      <th>mean_fouls_x</th>\n",
       "      <th>mean_fouls_y</th>\n",
       "      <th>mean_pts_against_x</th>\n",
       "      <th>mean_pts_against_y</th>\n",
       "      <th>mean_pts_x</th>\n",
       "      <th>mean_pts_y</th>\n",
       "      <th>mean_reb_against_x</th>\n",
       "      <th>mean_reb_against_y</th>\n",
       "      <th>mean_reb_x</th>\n",
       "      <th>mean_reb_y</th>\n",
       "      <th>mean_score_diff_x</th>\n",
       "      <th>mean_score_diff_y</th>\n",
       "      <th>mean_stl_against_x</th>\n",
       "      <th>mean_stl_against_y</th>\n",
       "      <th>mean_stl_x</th>\n",
       "      <th>mean_stl_y</th>\n",
       "      <th>num_games_x</th>\n",
       "      <th>num_games_y</th>\n",
       "      <th>std_TO_against_x</th>\n",
       "      <th>std_TO_against_y</th>\n",
       "      <th>std_TO_x</th>\n",
       "      <th>std_TO_y</th>\n",
       "      <th>std_ast_against_x</th>\n",
       "      <th>std_ast_against_y</th>\n",
       "      <th>std_ast_x</th>\n",
       "      <th>std_ast_y</th>\n",
       "      <th>std_blk_against_x</th>\n",
       "      <th>std_blk_against_y</th>\n",
       "      <th>std_blk_x</th>\n",
       "      <th>std_blk_y</th>\n",
       "      <th>std_fouls_against_x</th>\n",
       "      <th>std_fouls_against_y</th>\n",
       "      <th>std_fouls_x</th>\n",
       "      <th>std_fouls_y</th>\n",
       "      <th>std_pts_against_x</th>\n",
       "      <th>std_pts_against_y</th>\n",
       "      <th>std_pts_x</th>\n",
       "      <th>std_pts_y</th>\n",
       "      <th>std_reb_against_x</th>\n",
       "      <th>std_reb_against_y</th>\n",
       "      <th>std_reb_x</th>\n",
       "      <th>std_reb_y</th>\n",
       "      <th>std_score_diff_x</th>\n",
       "      <th>std_score_diff_y</th>\n",
       "      <th>std_stl_against_x</th>\n",
       "      <th>std_stl_against_y</th>\n",
       "      <th>std_stl_x</th>\n",
       "      <th>std_stl_y</th>\n",
       "      <th>win_pct_x</th>\n",
       "      <th>win_pct_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.510172</td>\n",
       "      <td>0.484202</td>\n",
       "      <td>0.350534</td>\n",
       "      <td>0.340757</td>\n",
       "      <td>0.701429</td>\n",
       "      <td>0.657848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>45.642857</td>\n",
       "      <td>40.37931</td>\n",
       "      <td>20.071429</td>\n",
       "      <td>15.482759</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.551724</td>\n",
       "      <td>16.857143</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.785714</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>15.464286</td>\n",
       "      <td>13.275862</td>\n",
       "      <td>17.642857</td>\n",
       "      <td>14.206897</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>3.655172</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>2.965517</td>\n",
       "      <td>22.071429</td>\n",
       "      <td>17.931034</td>\n",
       "      <td>17.75</td>\n",
       "      <td>15.896552</td>\n",
       "      <td>70.25</td>\n",
       "      <td>63.137931</td>\n",
       "      <td>85.214286</td>\n",
       "      <td>67.793103</td>\n",
       "      <td>36.392857</td>\n",
       "      <td>31.448276</td>\n",
       "      <td>42.821429</td>\n",
       "      <td>38.689655</td>\n",
       "      <td>14.964286</td>\n",
       "      <td>4.655172</td>\n",
       "      <td>5.964286</td>\n",
       "      <td>7.103448</td>\n",
       "      <td>8.464286</td>\n",
       "      <td>6.862069</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.147558</td>\n",
       "      <td>4.234214</td>\n",
       "      <td>4.458509</td>\n",
       "      <td>3.483953</td>\n",
       "      <td>3.156207</td>\n",
       "      <td>4.096135</td>\n",
       "      <td>4.339434</td>\n",
       "      <td>4.857831</td>\n",
       "      <td>1.663091</td>\n",
       "      <td>3.003282</td>\n",
       "      <td>2.006603</td>\n",
       "      <td>1.721352</td>\n",
       "      <td>4.438289</td>\n",
       "      <td>4.333681</td>\n",
       "      <td>2.989178</td>\n",
       "      <td>4.04744</td>\n",
       "      <td>9.359586</td>\n",
       "      <td>10.384481</td>\n",
       "      <td>10.379314</td>\n",
       "      <td>11.995176</td>\n",
       "      <td>5.349554</td>\n",
       "      <td>7.15435</td>\n",
       "      <td>4.546497</td>\n",
       "      <td>5.352592</td>\n",
       "      <td>12.556486</td>\n",
       "      <td>12.601802</td>\n",
       "      <td>3.06089</td>\n",
       "      <td>2.968223</td>\n",
       "      <td>3.646227</td>\n",
       "      <td>3.502286</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.655172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FG2_pct_x  FG2_pct_y  FG3_pct_x  FG3_pct_y  FT_pct_x  FT_pct_y  NumOT  \\\n",
       "0   0.510172   0.484202   0.350534   0.340757  0.701429  0.657848    0.0   \n",
       "\n",
       "   PlayIn_x  PlayIn_y  Score_x  Score_y  Season  Seed_num_x  Seed_num_y  \\\n",
       "0       0.0       0.0     80.0     51.0  2003.0         1.0        16.0   \n",
       "\n",
       "   TeamID_x  TeamID_y  close_games_pct_x  close_games_pct_y  \\\n",
       "0    1112.0    1436.0           0.178571           0.241379   \n",
       "\n",
       "   close_games_win_pct_x  close_games_win_pct_y  mean_FG2A_x  mean_FG2A_y  \\\n",
       "0                    0.6               0.428571    45.642857     40.37931   \n",
       "\n",
       "   mean_FG3A_x  mean_FG3A_y  mean_FTA_x  mean_FTA_y  mean_TO_against_x  \\\n",
       "0    20.071429    15.482759        25.0   19.551724          16.857143   \n",
       "\n",
       "   mean_TO_against_y  mean_TO_x  mean_TO_y  mean_ast_against_x  \\\n",
       "0               13.0  14.785714  14.068966           15.464286   \n",
       "\n",
       "   mean_ast_against_y  mean_ast_x  mean_ast_y  mean_blk_against_x  \\\n",
       "0           13.275862   17.642857   14.206897            2.392857   \n",
       "\n",
       "   mean_blk_against_y  mean_blk_x  mean_blk_y  mean_fouls_against_x  \\\n",
       "0            3.655172    4.214286    2.965517             22.071429   \n",
       "\n",
       "   mean_fouls_against_y  mean_fouls_x  mean_fouls_y  mean_pts_against_x  \\\n",
       "0             17.931034         17.75     15.896552               70.25   \n",
       "\n",
       "   mean_pts_against_y  mean_pts_x  mean_pts_y  mean_reb_against_x  \\\n",
       "0           63.137931   85.214286   67.793103           36.392857   \n",
       "\n",
       "   mean_reb_against_y  mean_reb_x  mean_reb_y  mean_score_diff_x  \\\n",
       "0           31.448276   42.821429   38.689655          14.964286   \n",
       "\n",
       "   mean_score_diff_y  mean_stl_against_x  mean_stl_against_y  mean_stl_x  \\\n",
       "0           4.655172            5.964286            7.103448    8.464286   \n",
       "\n",
       "   mean_stl_y  num_games_x  num_games_y  std_TO_against_x  std_TO_against_y  \\\n",
       "0    6.862069         28.0         29.0          5.147558          4.234214   \n",
       "\n",
       "   std_TO_x  std_TO_y  std_ast_against_x  std_ast_against_y  std_ast_x  \\\n",
       "0  4.458509  3.483953           3.156207           4.096135   4.339434   \n",
       "\n",
       "   std_ast_y  std_blk_against_x  std_blk_against_y  std_blk_x  std_blk_y  \\\n",
       "0   4.857831           1.663091           3.003282   2.006603   1.721352   \n",
       "\n",
       "   std_fouls_against_x  std_fouls_against_y  std_fouls_x  std_fouls_y  \\\n",
       "0             4.438289             4.333681     2.989178      4.04744   \n",
       "\n",
       "   std_pts_against_x  std_pts_against_y  std_pts_x  std_pts_y  \\\n",
       "0           9.359586          10.384481  10.379314  11.995176   \n",
       "\n",
       "   std_reb_against_x  std_reb_against_y  std_reb_x  std_reb_y  \\\n",
       "0           5.349554            7.15435   4.546497   5.352592   \n",
       "\n",
       "   std_score_diff_x  std_score_diff_y  std_stl_against_x  std_stl_against_y  \\\n",
       "0         12.556486         12.601802            3.06089           2.968223   \n",
       "\n",
       "   std_stl_x  std_stl_y  win_pct_x  win_pct_y  \n",
       "0   3.646227   3.502286   0.892857   0.655172  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG2_pct_x</th>\n",
       "      <th>FG2_pct_y</th>\n",
       "      <th>FG3_pct_x</th>\n",
       "      <th>FG3_pct_y</th>\n",
       "      <th>FT_pct_x</th>\n",
       "      <th>FT_pct_y</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>PlayIn_x</th>\n",
       "      <th>PlayIn_y</th>\n",
       "      <th>Score_x</th>\n",
       "      <th>Score_y</th>\n",
       "      <th>Season</th>\n",
       "      <th>Seed_num_x</th>\n",
       "      <th>Seed_num_y</th>\n",
       "      <th>TeamID_x</th>\n",
       "      <th>TeamID_y</th>\n",
       "      <th>close_games_pct_x</th>\n",
       "      <th>close_games_pct_y</th>\n",
       "      <th>close_games_win_pct_x</th>\n",
       "      <th>close_games_win_pct_y</th>\n",
       "      <th>mean_FG2A_x</th>\n",
       "      <th>mean_FG2A_y</th>\n",
       "      <th>mean_FG3A_x</th>\n",
       "      <th>mean_FG3A_y</th>\n",
       "      <th>mean_FTA_x</th>\n",
       "      <th>mean_FTA_y</th>\n",
       "      <th>mean_TO_against_x</th>\n",
       "      <th>mean_TO_against_y</th>\n",
       "      <th>mean_TO_x</th>\n",
       "      <th>mean_TO_y</th>\n",
       "      <th>mean_ast_against_x</th>\n",
       "      <th>mean_ast_against_y</th>\n",
       "      <th>mean_ast_x</th>\n",
       "      <th>mean_ast_y</th>\n",
       "      <th>mean_blk_against_x</th>\n",
       "      <th>mean_blk_against_y</th>\n",
       "      <th>mean_blk_x</th>\n",
       "      <th>mean_blk_y</th>\n",
       "      <th>mean_fouls_against_x</th>\n",
       "      <th>mean_fouls_against_y</th>\n",
       "      <th>mean_fouls_x</th>\n",
       "      <th>mean_fouls_y</th>\n",
       "      <th>mean_pts_against_x</th>\n",
       "      <th>mean_pts_against_y</th>\n",
       "      <th>mean_pts_x</th>\n",
       "      <th>mean_pts_y</th>\n",
       "      <th>mean_reb_against_x</th>\n",
       "      <th>mean_reb_against_y</th>\n",
       "      <th>mean_reb_x</th>\n",
       "      <th>mean_reb_y</th>\n",
       "      <th>mean_score_diff_x</th>\n",
       "      <th>mean_score_diff_y</th>\n",
       "      <th>mean_stl_against_x</th>\n",
       "      <th>mean_stl_against_y</th>\n",
       "      <th>mean_stl_x</th>\n",
       "      <th>mean_stl_y</th>\n",
       "      <th>num_games_x</th>\n",
       "      <th>num_games_y</th>\n",
       "      <th>std_TO_against_x</th>\n",
       "      <th>std_TO_against_y</th>\n",
       "      <th>std_TO_x</th>\n",
       "      <th>std_TO_y</th>\n",
       "      <th>std_ast_against_x</th>\n",
       "      <th>std_ast_against_y</th>\n",
       "      <th>std_ast_x</th>\n",
       "      <th>std_ast_y</th>\n",
       "      <th>std_blk_against_x</th>\n",
       "      <th>std_blk_against_y</th>\n",
       "      <th>std_blk_x</th>\n",
       "      <th>std_blk_y</th>\n",
       "      <th>std_fouls_against_x</th>\n",
       "      <th>std_fouls_against_y</th>\n",
       "      <th>std_fouls_x</th>\n",
       "      <th>std_fouls_y</th>\n",
       "      <th>std_pts_against_x</th>\n",
       "      <th>std_pts_against_y</th>\n",
       "      <th>std_pts_x</th>\n",
       "      <th>std_pts_y</th>\n",
       "      <th>std_reb_against_x</th>\n",
       "      <th>std_reb_against_y</th>\n",
       "      <th>std_reb_x</th>\n",
       "      <th>std_reb_y</th>\n",
       "      <th>std_score_diff_x</th>\n",
       "      <th>std_score_diff_y</th>\n",
       "      <th>std_stl_against_x</th>\n",
       "      <th>std_stl_against_y</th>\n",
       "      <th>std_stl_x</th>\n",
       "      <th>std_stl_y</th>\n",
       "      <th>win_pct_x</th>\n",
       "      <th>win_pct_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.484202</td>\n",
       "      <td>0.510172</td>\n",
       "      <td>0.340757</td>\n",
       "      <td>0.350534</td>\n",
       "      <td>0.657848</td>\n",
       "      <td>0.701429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.6</td>\n",
       "      <td>40.37931</td>\n",
       "      <td>45.642857</td>\n",
       "      <td>15.482759</td>\n",
       "      <td>20.071429</td>\n",
       "      <td>19.551724</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.857143</td>\n",
       "      <td>14.068966</td>\n",
       "      <td>14.785714</td>\n",
       "      <td>13.275862</td>\n",
       "      <td>15.464286</td>\n",
       "      <td>14.206897</td>\n",
       "      <td>17.642857</td>\n",
       "      <td>3.655172</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>2.965517</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>17.931034</td>\n",
       "      <td>22.071429</td>\n",
       "      <td>15.896552</td>\n",
       "      <td>17.75</td>\n",
       "      <td>63.137931</td>\n",
       "      <td>70.25</td>\n",
       "      <td>67.793103</td>\n",
       "      <td>85.214286</td>\n",
       "      <td>31.448276</td>\n",
       "      <td>36.392857</td>\n",
       "      <td>38.689655</td>\n",
       "      <td>42.821429</td>\n",
       "      <td>4.655172</td>\n",
       "      <td>14.964286</td>\n",
       "      <td>7.103448</td>\n",
       "      <td>5.964286</td>\n",
       "      <td>6.862069</td>\n",
       "      <td>8.464286</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.234214</td>\n",
       "      <td>5.147558</td>\n",
       "      <td>3.483953</td>\n",
       "      <td>4.458509</td>\n",
       "      <td>4.096135</td>\n",
       "      <td>3.156207</td>\n",
       "      <td>4.857831</td>\n",
       "      <td>4.339434</td>\n",
       "      <td>3.003282</td>\n",
       "      <td>1.663091</td>\n",
       "      <td>1.721352</td>\n",
       "      <td>2.006603</td>\n",
       "      <td>4.333681</td>\n",
       "      <td>4.438289</td>\n",
       "      <td>4.04744</td>\n",
       "      <td>2.989178</td>\n",
       "      <td>10.384481</td>\n",
       "      <td>9.359586</td>\n",
       "      <td>11.995176</td>\n",
       "      <td>10.379314</td>\n",
       "      <td>7.15435</td>\n",
       "      <td>5.349554</td>\n",
       "      <td>5.352592</td>\n",
       "      <td>4.546497</td>\n",
       "      <td>12.601802</td>\n",
       "      <td>12.556486</td>\n",
       "      <td>2.968223</td>\n",
       "      <td>3.06089</td>\n",
       "      <td>3.502286</td>\n",
       "      <td>3.646227</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FG2_pct_x  FG2_pct_y  FG3_pct_x  FG3_pct_y  FT_pct_x  FT_pct_y  NumOT  \\\n",
       "0   0.484202   0.510172   0.340757   0.350534  0.657848  0.701429    0.0   \n",
       "\n",
       "   PlayIn_x  PlayIn_y  Score_x  Score_y  Season  Seed_num_x  Seed_num_y  \\\n",
       "0       0.0       0.0     51.0     80.0  2003.0        16.0         1.0   \n",
       "\n",
       "   TeamID_x  TeamID_y  close_games_pct_x  close_games_pct_y  \\\n",
       "0    1436.0    1112.0           0.241379           0.178571   \n",
       "\n",
       "   close_games_win_pct_x  close_games_win_pct_y  mean_FG2A_x  mean_FG2A_y  \\\n",
       "0               0.428571                    0.6     40.37931    45.642857   \n",
       "\n",
       "   mean_FG3A_x  mean_FG3A_y  mean_FTA_x  mean_FTA_y  mean_TO_against_x  \\\n",
       "0    15.482759    20.071429   19.551724        25.0               13.0   \n",
       "\n",
       "   mean_TO_against_y  mean_TO_x  mean_TO_y  mean_ast_against_x  \\\n",
       "0          16.857143  14.068966  14.785714           13.275862   \n",
       "\n",
       "   mean_ast_against_y  mean_ast_x  mean_ast_y  mean_blk_against_x  \\\n",
       "0           15.464286   14.206897   17.642857            3.655172   \n",
       "\n",
       "   mean_blk_against_y  mean_blk_x  mean_blk_y  mean_fouls_against_x  \\\n",
       "0            2.392857    2.965517    4.214286             17.931034   \n",
       "\n",
       "   mean_fouls_against_y  mean_fouls_x  mean_fouls_y  mean_pts_against_x  \\\n",
       "0             22.071429     15.896552         17.75           63.137931   \n",
       "\n",
       "   mean_pts_against_y  mean_pts_x  mean_pts_y  mean_reb_against_x  \\\n",
       "0               70.25   67.793103   85.214286           31.448276   \n",
       "\n",
       "   mean_reb_against_y  mean_reb_x  mean_reb_y  mean_score_diff_x  \\\n",
       "0           36.392857   38.689655   42.821429           4.655172   \n",
       "\n",
       "   mean_score_diff_y  mean_stl_against_x  mean_stl_against_y  mean_stl_x  \\\n",
       "0          14.964286            7.103448            5.964286    6.862069   \n",
       "\n",
       "   mean_stl_y  num_games_x  num_games_y  std_TO_against_x  std_TO_against_y  \\\n",
       "0    8.464286         29.0         28.0          4.234214          5.147558   \n",
       "\n",
       "   std_TO_x  std_TO_y  std_ast_against_x  std_ast_against_y  std_ast_x  \\\n",
       "0  3.483953  4.458509           4.096135           3.156207   4.857831   \n",
       "\n",
       "   std_ast_y  std_blk_against_x  std_blk_against_y  std_blk_x  std_blk_y  \\\n",
       "0   4.339434           3.003282           1.663091   1.721352   2.006603   \n",
       "\n",
       "   std_fouls_against_x  std_fouls_against_y  std_fouls_x  std_fouls_y  \\\n",
       "0             4.333681             4.438289      4.04744     2.989178   \n",
       "\n",
       "   std_pts_against_x  std_pts_against_y  std_pts_x  std_pts_y  \\\n",
       "0          10.384481           9.359586  11.995176  10.379314   \n",
       "\n",
       "   std_reb_against_x  std_reb_against_y  std_reb_x  std_reb_y  \\\n",
       "0            7.15435           5.349554   5.352592   4.546497   \n",
       "\n",
       "   std_score_diff_x  std_score_diff_y  std_stl_against_x  std_stl_against_y  \\\n",
       "0         12.601802         12.556486           2.968223            3.06089   \n",
       "\n",
       "   std_stl_x  std_stl_y  win_pct_x  win_pct_y  \n",
       "0   3.502286   3.646227   0.655172   0.892857  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# flip x and y col values\n",
    "flipped = fdet.copy()\n",
    "for col in flipped.columns:\n",
    "    if col[-2:] == '_x':\n",
    "        col_y = col.replace('_x', '_y')\n",
    "        flipped.loc[:, col], flipped.loc[:, col_y] = flipped.loc[:, col_y].copy(), flipped.loc[:, col].copy()\n",
    "\n",
    "# rearrange cols\n",
    "flipped = flipped.reindex(sorted(flipped.columns), axis=1)\n",
    "\n",
    "# check\n",
    "display(fdet.head(1))\n",
    "display(flipped.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a flipped version of our features, we can combine the 2 dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4408, 90)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FG2_pct_x</th>\n",
       "      <th>FG2_pct_y</th>\n",
       "      <th>FG3_pct_x</th>\n",
       "      <th>FG3_pct_y</th>\n",
       "      <th>FT_pct_x</th>\n",
       "      <th>FT_pct_y</th>\n",
       "      <th>NumOT</th>\n",
       "      <th>PlayIn_x</th>\n",
       "      <th>PlayIn_y</th>\n",
       "      <th>Score_x</th>\n",
       "      <th>Score_y</th>\n",
       "      <th>Season</th>\n",
       "      <th>Seed_num_x</th>\n",
       "      <th>Seed_num_y</th>\n",
       "      <th>TeamID_x</th>\n",
       "      <th>TeamID_y</th>\n",
       "      <th>close_games_pct_x</th>\n",
       "      <th>close_games_pct_y</th>\n",
       "      <th>close_games_win_pct_x</th>\n",
       "      <th>close_games_win_pct_y</th>\n",
       "      <th>mean_FG2A_x</th>\n",
       "      <th>mean_FG2A_y</th>\n",
       "      <th>mean_FG3A_x</th>\n",
       "      <th>mean_FG3A_y</th>\n",
       "      <th>mean_FTA_x</th>\n",
       "      <th>mean_FTA_y</th>\n",
       "      <th>mean_TO_against_x</th>\n",
       "      <th>mean_TO_against_y</th>\n",
       "      <th>mean_TO_x</th>\n",
       "      <th>mean_TO_y</th>\n",
       "      <th>mean_ast_against_x</th>\n",
       "      <th>mean_ast_against_y</th>\n",
       "      <th>mean_ast_x</th>\n",
       "      <th>mean_ast_y</th>\n",
       "      <th>mean_blk_against_x</th>\n",
       "      <th>mean_blk_against_y</th>\n",
       "      <th>mean_blk_x</th>\n",
       "      <th>mean_blk_y</th>\n",
       "      <th>mean_fouls_against_x</th>\n",
       "      <th>mean_fouls_against_y</th>\n",
       "      <th>mean_fouls_x</th>\n",
       "      <th>mean_fouls_y</th>\n",
       "      <th>mean_pts_against_x</th>\n",
       "      <th>mean_pts_against_y</th>\n",
       "      <th>mean_pts_x</th>\n",
       "      <th>mean_pts_y</th>\n",
       "      <th>mean_reb_against_x</th>\n",
       "      <th>mean_reb_against_y</th>\n",
       "      <th>mean_reb_x</th>\n",
       "      <th>mean_reb_y</th>\n",
       "      <th>mean_score_diff_x</th>\n",
       "      <th>mean_score_diff_y</th>\n",
       "      <th>mean_stl_against_x</th>\n",
       "      <th>mean_stl_against_y</th>\n",
       "      <th>mean_stl_x</th>\n",
       "      <th>mean_stl_y</th>\n",
       "      <th>num_games_x</th>\n",
       "      <th>num_games_y</th>\n",
       "      <th>std_TO_against_x</th>\n",
       "      <th>std_TO_against_y</th>\n",
       "      <th>std_TO_x</th>\n",
       "      <th>std_TO_y</th>\n",
       "      <th>std_ast_against_x</th>\n",
       "      <th>std_ast_against_y</th>\n",
       "      <th>std_ast_x</th>\n",
       "      <th>std_ast_y</th>\n",
       "      <th>std_blk_against_x</th>\n",
       "      <th>std_blk_against_y</th>\n",
       "      <th>std_blk_x</th>\n",
       "      <th>std_blk_y</th>\n",
       "      <th>std_fouls_against_x</th>\n",
       "      <th>std_fouls_against_y</th>\n",
       "      <th>std_fouls_x</th>\n",
       "      <th>std_fouls_y</th>\n",
       "      <th>std_pts_against_x</th>\n",
       "      <th>std_pts_against_y</th>\n",
       "      <th>std_pts_x</th>\n",
       "      <th>std_pts_y</th>\n",
       "      <th>std_reb_against_x</th>\n",
       "      <th>std_reb_against_y</th>\n",
       "      <th>std_reb_x</th>\n",
       "      <th>std_reb_y</th>\n",
       "      <th>std_score_diff_x</th>\n",
       "      <th>std_score_diff_y</th>\n",
       "      <th>std_stl_against_x</th>\n",
       "      <th>std_stl_against_y</th>\n",
       "      <th>std_stl_x</th>\n",
       "      <th>std_stl_y</th>\n",
       "      <th>win_pct_x</th>\n",
       "      <th>win_pct_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.5434</td>\n",
       "      <td>0.545982</td>\n",
       "      <td>0.363095</td>\n",
       "      <td>0.351275</td>\n",
       "      <td>0.657046</td>\n",
       "      <td>0.684515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1276.0</td>\n",
       "      <td>1199.0</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.16129</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>32.529412</td>\n",
       "      <td>38.935484</td>\n",
       "      <td>24.705882</td>\n",
       "      <td>22.774194</td>\n",
       "      <td>17.323529</td>\n",
       "      <td>22.290323</td>\n",
       "      <td>12.852941</td>\n",
       "      <td>14.096774</td>\n",
       "      <td>9.176471</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.470588</td>\n",
       "      <td>12.83871</td>\n",
       "      <td>14.588235</td>\n",
       "      <td>15.258065</td>\n",
       "      <td>3.235294</td>\n",
       "      <td>2.967742</td>\n",
       "      <td>3.088235</td>\n",
       "      <td>5.16129</td>\n",
       "      <td>17.705882</td>\n",
       "      <td>18.806452</td>\n",
       "      <td>15.705882</td>\n",
       "      <td>18.290323</td>\n",
       "      <td>63.529412</td>\n",
       "      <td>74.483871</td>\n",
       "      <td>73.647059</td>\n",
       "      <td>81.774194</td>\n",
       "      <td>32.617647</td>\n",
       "      <td>34.935484</td>\n",
       "      <td>33.323529</td>\n",
       "      <td>38.032258</td>\n",
       "      <td>10.117647</td>\n",
       "      <td>7.290323</td>\n",
       "      <td>4.088235</td>\n",
       "      <td>5.870968</td>\n",
       "      <td>6.176471</td>\n",
       "      <td>6.709677</td>\n",
       "      <td>34.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.806135</td>\n",
       "      <td>4.901393</td>\n",
       "      <td>2.3928</td>\n",
       "      <td>3.172801</td>\n",
       "      <td>4.69839</td>\n",
       "      <td>4.359639</td>\n",
       "      <td>3.985714</td>\n",
       "      <td>4.732637</td>\n",
       "      <td>1.985689</td>\n",
       "      <td>2.575317</td>\n",
       "      <td>1.928536</td>\n",
       "      <td>2.59611</td>\n",
       "      <td>3.888829</td>\n",
       "      <td>4.784833</td>\n",
       "      <td>4.034168</td>\n",
       "      <td>4.345063</td>\n",
       "      <td>11.133448</td>\n",
       "      <td>12.842302</td>\n",
       "      <td>10.548251</td>\n",
       "      <td>13.300901</td>\n",
       "      <td>5.039328</td>\n",
       "      <td>5.887475</td>\n",
       "      <td>5.34688</td>\n",
       "      <td>6.954537</td>\n",
       "      <td>14.446903</td>\n",
       "      <td>15.423345</td>\n",
       "      <td>1.747165</td>\n",
       "      <td>2.093513</td>\n",
       "      <td>3.069603</td>\n",
       "      <td>2.946111</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.645161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FG2_pct_x  FG2_pct_y  FG3_pct_x  FG3_pct_y  FT_pct_x  FT_pct_y  NumOT  \\\n",
       "1002     0.5434   0.545982   0.363095   0.351275  0.657046  0.684515    0.0   \n",
       "\n",
       "      PlayIn_x  PlayIn_y  Score_x  Score_y  Season  Seed_num_x  Seed_num_y  \\\n",
       "1002       0.0       0.0     58.0     54.0  2018.0         3.0         9.0   \n",
       "\n",
       "      TeamID_x  TeamID_y  close_games_pct_x  close_games_pct_y  \\\n",
       "1002    1276.0    1199.0           0.147059            0.16129   \n",
       "\n",
       "      close_games_win_pct_x  close_games_win_pct_y  mean_FG2A_x  mean_FG2A_y  \\\n",
       "1002                    0.6                    0.8    32.529412    38.935484   \n",
       "\n",
       "      mean_FG3A_x  mean_FG3A_y  mean_FTA_x  mean_FTA_y  mean_TO_against_x  \\\n",
       "1002    24.705882    22.774194   17.323529   22.290323          12.852941   \n",
       "\n",
       "      mean_TO_against_y  mean_TO_x  mean_TO_y  mean_ast_against_x  \\\n",
       "1002          14.096774   9.176471       13.0           10.470588   \n",
       "\n",
       "      mean_ast_against_y  mean_ast_x  mean_ast_y  mean_blk_against_x  \\\n",
       "1002            12.83871   14.588235   15.258065            3.235294   \n",
       "\n",
       "      mean_blk_against_y  mean_blk_x  mean_blk_y  mean_fouls_against_x  \\\n",
       "1002            2.967742    3.088235     5.16129             17.705882   \n",
       "\n",
       "      mean_fouls_against_y  mean_fouls_x  mean_fouls_y  mean_pts_against_x  \\\n",
       "1002             18.806452     15.705882     18.290323           63.529412   \n",
       "\n",
       "      mean_pts_against_y  mean_pts_x  mean_pts_y  mean_reb_against_x  \\\n",
       "1002           74.483871   73.647059   81.774194           32.617647   \n",
       "\n",
       "      mean_reb_against_y  mean_reb_x  mean_reb_y  mean_score_diff_x  \\\n",
       "1002           34.935484   33.323529   38.032258          10.117647   \n",
       "\n",
       "      mean_score_diff_y  mean_stl_against_x  mean_stl_against_y  mean_stl_x  \\\n",
       "1002           7.290323            4.088235            5.870968    6.176471   \n",
       "\n",
       "      mean_stl_y  num_games_x  num_games_y  std_TO_against_x  \\\n",
       "1002    6.709677         34.0         31.0          4.806135   \n",
       "\n",
       "      std_TO_against_y  std_TO_x  std_TO_y  std_ast_against_x  \\\n",
       "1002          4.901393    2.3928  3.172801            4.69839   \n",
       "\n",
       "      std_ast_against_y  std_ast_x  std_ast_y  std_blk_against_x  \\\n",
       "1002           4.359639   3.985714   4.732637           1.985689   \n",
       "\n",
       "      std_blk_against_y  std_blk_x  std_blk_y  std_fouls_against_x  \\\n",
       "1002           2.575317   1.928536    2.59611             3.888829   \n",
       "\n",
       "      std_fouls_against_y  std_fouls_x  std_fouls_y  std_pts_against_x  \\\n",
       "1002             4.784833     4.034168     4.345063          11.133448   \n",
       "\n",
       "      std_pts_against_y  std_pts_x  std_pts_y  std_reb_against_x  \\\n",
       "1002          12.842302  10.548251  13.300901           5.039328   \n",
       "\n",
       "      std_reb_against_y  std_reb_x  std_reb_y  std_score_diff_x  \\\n",
       "1002           5.887475    5.34688   6.954537         14.446903   \n",
       "\n",
       "      std_score_diff_y  std_stl_against_x  std_stl_against_y  std_stl_x  \\\n",
       "1002         15.423345           1.747165           2.093513   3.069603   \n",
       "\n",
       "      std_stl_y  win_pct_x  win_pct_y  \n",
       "1002   2.946111   0.794118   0.645161  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine\n",
    "fdet = pd.concat([fdet, flipped], axis=0)\n",
    "\n",
    "# check\n",
    "print(fdet.shape)\n",
    "display(fdet.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score_x</th>\n",
       "      <th>Score_y</th>\n",
       "      <th>score_diff_x</th>\n",
       "      <th>win_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>64.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Score_x  Score_y  score_diff_x  win_x\n",
       "783     64.0     76.0         -12.0      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create regression label, drop cols\n",
    "fdet['score_diff_x'] = fdet['Score_x'] - fdet['Score_y']\n",
    "\n",
    "# create adjusted score diff col (score diff is inversely scaled by NumOT periods)\n",
    "fdet['score_diff_adj_x'] = fdet['score_diff_x'] / (2 ** fdet['NumOT'])\n",
    "\n",
    "# create binary label\n",
    "fdet['win_x'] = fdet['score_diff_x'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# create upset win label (Seed_num_x > Seed_num_y & win_x == 1) | (Seed_num_x < Seed_num_y & win_x == 0)\n",
    "# fdet['win_upset'] = ((fdet['Seed_num_x'] > fdet['Seed_num_y']) & (fdet['win_x'] == 1)) | ((fdet['Seed_num_x'] < fdet['Seed_num_y']) & (fdet['win_x'] == 0))\n",
    "\n",
    "# check\n",
    "fdet.sample()[['Score_x', 'Score_y', 'score_diff_x', 'win_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of upset wins for men and women\n",
    "# fdet.query('TeamID_x < 3000')['win_upset'].value_counts(normalize=True), fdet.query('TeamID_x > 3000')['win_upset'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Men's upsets since 2003: __29%__\n",
    "- Women's upsets since 2010: __21%__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chalk Bracket\n",
    "Here, we will simply predict the better seed to win each game. If seeds are equal (in rounds 5 and 6), we will predict the team with the better regular season winning percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of dummy predictions in men's tournaments (39 brackets): 70.57%.\n",
      "Accuracy of dummy predictions in women's tournaments (26 brackets): 78.46%\n"
     ]
    }
   ],
   "source": [
    "# split genders for winning rows\n",
    "fdet_mens, fdet_womens = split_genders(fdet[fdet['win_x'] == 1], id_col='TeamID_x')\n",
    "\n",
    "# get dummy preds for men and calculate accuracy\n",
    "mchalk_preds = get_dummy_preds(fdet_mens)\n",
    "mchalk_acc = accuracy_score(fdet_mens['win_x'], mchalk_preds)\n",
    "\n",
    "# women\n",
    "wchalk_preds = get_dummy_preds(fdet_womens)\n",
    "wchalk_acc = accuracy_score(fdet_womens['win_x'], wchalk_preds)\n",
    "\n",
    "# show\n",
    "print(f\"Accuracy of dummy predictions in men's tournaments (39 brackets): {mchalk_acc*100:.2f}%.\")\n",
    "print(f\"Accuracy of dummy predictions in women's tournaments (26 brackets): {wchalk_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that the better seed wins about 8% more often in women's tournaments (post 2000). This is slightly more often than the chalk bracket in the full compact data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalize Columns\n",
    "We will one-hot encode categorical columns, then drop non-feature columns. We will also create seperate feature subsets that only contain general scoring and win data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2644, 86), (1764, 86))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get copy\n",
    "X = fdet.copy()\n",
    "\n",
    "# non-feature columns\n",
    "dropped_cols = ['Score_x', 'Score_y', 'score_diff_x', 'NumOT']\n",
    "\n",
    "# split genders\n",
    "X_mens, X_womens = split_genders(X, id_col='TeamID_x')\n",
    "\n",
    "# drop non-feature columns\n",
    "X_mens = X_mens.drop(columns=dropped_cols + ['Season', 'TeamID_x', 'TeamID_y'])\n",
    "X_womens = X_womens.drop(columns=dropped_cols + ['Season', 'TeamID_x', 'TeamID_y'])\n",
    "\n",
    "# number of features\n",
    "X_mens.shape, X_womens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have roughly 2000 rows of training data for both men and women.\n",
    "- 3 of these columns are labels. Thus, we have 83 features.\n",
    "- Each gender will be trained seperately, as we have already found multiple statistically-significant differences in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models\n",
    "We will experiment with 5 regression models, 5 classification models, and 3 scalers. We will also try each of the 2 different feature subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Target</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_Params</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_LogLoss</th>\n",
       "      <th>Val_LogLoss</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Gender, Target, Model, Model_Params, Scaler, Num_Features, Train_R2, Val_R2, Train_RMSE, Val_RMSE, Train_LogLoss, Val_LogLoss, Train_Acc, Val_Acc]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define models and scalers\n",
    "models_class = [LogisticRegression(n_jobs=-1), DecisionTreeClassifier(max_depth=10), RandomForestClassifier(n_jobs=-1, max_depth=10), XGBClassifier(n_jobs=-1, max_depth=5), SVC()]\n",
    "\n",
    "# create a df to hold model performance\n",
    "models_df = pd.DataFrame(columns=['Gender', 'Target', 'Model', 'Model_Params', 'Scaler', 'Num_Features', 'Train_R2', 'Val_R2', 'Train_RMSE', 'Val_RMSE', 'Train_LogLoss', 'Val_LogLoss', 'Train_Acc', 'Val_Acc']).astype({\n",
    "    'Gender': 'object', 'Target': 'object', 'Model': 'object', 'Model_Params': 'object', 'Scaler': 'object', 'Num_Features': 'int', 'Train_R2': 'float', 'Val_R2': 'float', 'Train_RMSE': 'float', 'Val_RMSE': 'float', \n",
    "    'Train_LogLoss': 'float', 'Val_LogLoss': 'float', 'Train_Acc': 'float', 'Val_Acc': 'float'})\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Men's\n",
    "We're trying to beat the chalk accuracy of __70.57%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Models: 100%|██████████| 5/5 [00:26<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# run models\n",
    "for model in tqdm(models_class, desc=\"Running Models\"):\n",
    "    cross_val_model(estimator=model, df=X_mens, target_col='win_x', gender='M', scaler=RobustScaler(), models_df=models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Target</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_Params</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_LogLoss</th>\n",
       "      <th>Val_LogLoss</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>win_x</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': -1, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.009239e+01</td>\n",
       "      <td>10.932829</td>\n",
       "      <td>0.719995</td>\n",
       "      <td>0.696678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>win_x</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.801866e-01</td>\n",
       "      <td>11.287186</td>\n",
       "      <td>0.989452</td>\n",
       "      <td>0.686847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>win_x</td>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.648603e+00</td>\n",
       "      <td>11.600791</td>\n",
       "      <td>0.871029</td>\n",
       "      <td>0.678146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>win_x</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 5, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>12.119189</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>win_x</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.623402e+00</td>\n",
       "      <td>13.482264</td>\n",
       "      <td>0.927216</td>\n",
       "      <td>0.625946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender Target                   Model  \\\n",
       "0      M  win_x      LogisticRegression   \n",
       "1      M  win_x  RandomForestClassifier   \n",
       "2      M  win_x                     SVC   \n",
       "3      M  win_x           XGBClassifier   \n",
       "4      M  win_x  DecisionTreeClassifier   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Model_Params  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': -1, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}   \n",
       "3  {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 5, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}   \n",
       "\n",
       "         Scaler  Num_Features  Train_R2  Val_R2  Train_RMSE  Val_RMSE  \\\n",
       "0  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "1  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "2  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "3  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "4  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "\n",
       "   Train_LogLoss  Val_LogLoss  Train_Acc   Val_Acc  \n",
       "0   1.009239e+01    10.932829   0.719995  0.696678  \n",
       "1   3.801866e-01    11.287186   0.989452  0.686847  \n",
       "2   4.648603e+00    11.600791   0.871029  0.678146  \n",
       "3   2.220446e-16    12.119189   1.000000  0.663764  \n",
       "4   2.623402e+00    13.482264   0.927216  0.625946  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect\n",
    "models_df.query(\"Gender == 'M'\").sort_values(by='Val_LogLoss').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best performance was __69.67%__ accuracy using a __Logistic Regression__, but this is worse than our results on the compact data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Women's\n",
    "We're trying to beat the chalk accuracy of __78.46%__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Models: 100%|██████████| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# run models\n",
    "for model in tqdm(models_class, desc=\"Running Models\"):\n",
    "    cross_val_model(estimator=model, df=X_womens, target_col='win_x', gender='W', scaler=RobustScaler(), models_df=models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Target</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_Params</th>\n",
       "      <th>Scaler</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "      <th>Train_LogLoss</th>\n",
       "      <th>Val_LogLoss</th>\n",
       "      <th>Train_Acc</th>\n",
       "      <th>Val_Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>W</td>\n",
       "      <td>win_x</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': -1, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.754188e+00</td>\n",
       "      <td>7.723491</td>\n",
       "      <td>0.812611</td>\n",
       "      <td>0.785718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>W</td>\n",
       "      <td>win_x</td>\n",
       "      <td>SVC</td>\n",
       "      <td>{'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.795963e+00</td>\n",
       "      <td>8.132268</td>\n",
       "      <td>0.894684</td>\n",
       "      <td>0.774377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>W</td>\n",
       "      <td>win_x</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.945417e-02</td>\n",
       "      <td>8.398616</td>\n",
       "      <td>0.997796</td>\n",
       "      <td>0.766988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>W</td>\n",
       "      <td>win_x</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>{'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 5, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>8.683938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.759072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>W</td>\n",
       "      <td>win_x</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}</td>\n",
       "      <td>RobustScaler</td>\n",
       "      <td>84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.023862e+00</td>\n",
       "      <td>10.421327</td>\n",
       "      <td>0.971594</td>\n",
       "      <td>0.710869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender Target                   Model  \\\n",
       "5      W  win_x      LogisticRegression   \n",
       "7      W  win_x                     SVC   \n",
       "6      W  win_x  RandomForestClassifier   \n",
       "8      W  win_x           XGBClassifier   \n",
       "9      W  win_x  DecisionTreeClassifier   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Model_Params  \\\n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': -1, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         {'C': 1.0, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}   \n",
       "8  {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 5, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}   \n",
       "\n",
       "         Scaler  Num_Features  Train_R2  Val_R2  Train_RMSE  Val_RMSE  \\\n",
       "5  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "7  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "6  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "8  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "9  RobustScaler            84       0.0     0.0         0.0       0.0   \n",
       "\n",
       "   Train_LogLoss  Val_LogLoss  Train_Acc   Val_Acc  \n",
       "5   6.754188e+00     7.723491   0.812611  0.785718  \n",
       "7   3.795963e+00     8.132268   0.894684  0.774377  \n",
       "6   7.945417e-02     8.398616   0.997796  0.766988  \n",
       "8   2.220446e-16     8.683938   1.000000  0.759072  \n",
       "9   1.023862e+00    10.421327   0.971594  0.710869  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect\n",
    "models_df.query(\"Gender == 'W'\").sort_values(by='Val_LogLoss').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best performance was __78.57%__ accuracy using a __Logistic Regression__, but this is worse than our results on the compact data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models_df\n",
    "models_df.to_csv('models/models_detailed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "Here we will try out a PyTorch neural network with a two-tower approach, first training a dense NN on the 42 team_x features, and another NN on the 42 team_y features, then, we will pass the two outputs into a final NN that outputs the probability that team_x wins. Maybe this model will be strong enough to compete with our 2 best models from the [compact predictions](./preds_compact.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "|   iter    |  target   | combin... | dropou... | hidden... | hidden... |    lr     | weight... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m4.635    \u001b[0m | \u001b[0m0.5015   \u001b[0m | \u001b[0m127.9    \u001b[0m | \u001b[0m35.99    \u001b[0m | \u001b[0m0.01422  \u001b[0m | \u001b[0m0.02186  \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m-0.6932  \u001b[0m | \u001b[95m108.3    \u001b[0m | \u001b[95m0.2985   \u001b[0m | \u001b[95m23.35    \u001b[0m | \u001b[95m89.76    \u001b[0m | \u001b[95m0.01669  \u001b[0m | \u001b[95m0.08786  \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m243.5    \u001b[0m | \u001b[0m0.131    \u001b[0m | \u001b[0m179.6    \u001b[0m | \u001b[0m147.5    \u001b[0m | \u001b[0m0.0898   \u001b[0m | \u001b[0m0.06669  \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m141.2    \u001b[0m | \u001b[0m0.6619   \u001b[0m | \u001b[0m100.2    \u001b[0m | \u001b[0m178.4    \u001b[0m | \u001b[0m0.08249  \u001b[0m | \u001b[0m0.04657  \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m240.9    \u001b[0m | \u001b[0m0.743    \u001b[0m | \u001b[0m251.8    \u001b[0m | \u001b[0m45.09    \u001b[0m | \u001b[0m0.05211  \u001b[0m | \u001b[0m0.09455  \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m167.0    \u001b[0m | \u001b[0m0.7887   \u001b[0m | \u001b[0m248.6    \u001b[0m | \u001b[0m49.13    \u001b[0m | \u001b[0m0.02432  \u001b[0m | \u001b[0m0.08734  \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m232.2    \u001b[0m | \u001b[0m0.7066   \u001b[0m | \u001b[0m90.11    \u001b[0m | \u001b[0m119.7    \u001b[0m | \u001b[0m0.06106  \u001b[0m | \u001b[0m0.04963  \u001b[0m |\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m-0.6932  \u001b[0m | \u001b[95m123.4    \u001b[0m | \u001b[95m0.4484   \u001b[0m | \u001b[95m223.1    \u001b[0m | \u001b[95m49.59    \u001b[0m | \u001b[95m0.06877  \u001b[0m | \u001b[95m0.07036  \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.9608  \u001b[0m | \u001b[0m237.0    \u001b[0m | \u001b[0m0.1316   \u001b[0m | \u001b[0m166.4    \u001b[0m | \u001b[0m164.1    \u001b[0m | \u001b[0m0.04618  \u001b[0m | \u001b[0m0.005315 \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m11.8     \u001b[0m | \u001b[0m0.2356   \u001b[0m | \u001b[0m203.8    \u001b[0m | \u001b[0m43.3     \u001b[0m | \u001b[0m0.01388  \u001b[0m | \u001b[0m0.0353   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m238.2    \u001b[0m | \u001b[0m0.2827   \u001b[0m | \u001b[0m252.7    \u001b[0m | \u001b[0m40.54    \u001b[0m | \u001b[0m0.07447  \u001b[0m | \u001b[0m0.03903  \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m248.2    \u001b[0m | \u001b[0m0.873    \u001b[0m | \u001b[0m194.1    \u001b[0m | \u001b[0m137.6    \u001b[0m | \u001b[0m0.01298  \u001b[0m | \u001b[0m0.02712  \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m249.8    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m175.2    \u001b[0m | \u001b[0m128.9    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m0.1      \u001b[0m |\n",
      "| \u001b[95m14       \u001b[0m | \u001b[95m-0.6932  \u001b[0m | \u001b[95m148.9    \u001b[0m | \u001b[95m0.392    \u001b[0m | \u001b[95m234.4    \u001b[0m | \u001b[95m48.37    \u001b[0m | \u001b[95m0.06416  \u001b[0m | \u001b[95m0.08606  \u001b[0m |\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m-0.5956  \u001b[0m | \u001b[95m226.1    \u001b[0m | \u001b[95m0.745    \u001b[0m | \u001b[95m182.0    \u001b[0m | \u001b[95m127.9    \u001b[0m | \u001b[95m0.07255  \u001b[0m | \u001b[95m0.01243  \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m10.03    \u001b[0m | \u001b[0m0.5016   \u001b[0m | \u001b[0m202.5    \u001b[0m | \u001b[0m38.21    \u001b[0m | \u001b[0m0.02429  \u001b[0m | \u001b[0m0.01506  \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m222.7    \u001b[0m | \u001b[0m0.4689   \u001b[0m | \u001b[0m179.2    \u001b[0m | \u001b[0m117.8    \u001b[0m | \u001b[0m0.02028  \u001b[0m | \u001b[0m0.0809   \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m227.8    \u001b[0m | \u001b[0m0.3979   \u001b[0m | \u001b[0m191.9    \u001b[0m | \u001b[0m124.3    \u001b[0m | \u001b[0m0.01591  \u001b[0m | \u001b[0m0.06913  \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m223.2    \u001b[0m | \u001b[0m0.9      \u001b[0m | \u001b[0m178.7    \u001b[0m | \u001b[0m136.1    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m1e-05    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m237.9    \u001b[0m | \u001b[0m0.536    \u001b[0m | \u001b[0m251.7    \u001b[0m | \u001b[0m45.73    \u001b[0m | \u001b[0m0.08432  \u001b[0m | \u001b[0m0.04166  \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m236.2    \u001b[0m | \u001b[0m0.5337   \u001b[0m | \u001b[0m182.2    \u001b[0m | \u001b[0m123.5    \u001b[0m | \u001b[0m0.01499  \u001b[0m | \u001b[0m0.07118  \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m229.7    \u001b[0m | \u001b[0m0.6081   \u001b[0m | \u001b[0m166.6    \u001b[0m | \u001b[0m128.9    \u001b[0m | \u001b[0m0.03827  \u001b[0m | \u001b[0m0.09698  \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m203.9    \u001b[0m | \u001b[0m0.329    \u001b[0m | \u001b[0m187.4    \u001b[0m | \u001b[0m124.5    \u001b[0m | \u001b[0m0.02567  \u001b[0m | \u001b[0m0.08654  \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m229.2    \u001b[0m | \u001b[0m0.4551   \u001b[0m | \u001b[0m190.6    \u001b[0m | \u001b[0m140.7    \u001b[0m | \u001b[0m0.03819  \u001b[0m | \u001b[0m0.09257  \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m218.8    \u001b[0m | \u001b[0m0.3558   \u001b[0m | \u001b[0m181.7    \u001b[0m | \u001b[0m127.2    \u001b[0m | \u001b[0m0.09613  \u001b[0m | \u001b[0m0.08764  \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.6281  \u001b[0m | \u001b[0m166.4    \u001b[0m | \u001b[0m0.4313   \u001b[0m | \u001b[0m236.3    \u001b[0m | \u001b[0m34.85    \u001b[0m | \u001b[0m0.02852  \u001b[0m | \u001b[0m0.01054  \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m172.8    \u001b[0m | \u001b[0m0.5013   \u001b[0m | \u001b[0m232.8    \u001b[0m | \u001b[0m32.8     \u001b[0m | \u001b[0m0.04777  \u001b[0m | \u001b[0m0.08405  \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m168.1    \u001b[0m | \u001b[0m0.2299   \u001b[0m | \u001b[0m232.0    \u001b[0m | \u001b[0m45.15    \u001b[0m | \u001b[0m0.0695   \u001b[0m | \u001b[0m0.06399  \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.6225  \u001b[0m | \u001b[0m160.5    \u001b[0m | \u001b[0m0.7514   \u001b[0m | \u001b[0m241.7    \u001b[0m | \u001b[0m39.15    \u001b[0m | \u001b[0m0.06188  \u001b[0m | \u001b[0m0.01908  \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m152.7    \u001b[0m | \u001b[0m0.8301   \u001b[0m | \u001b[0m244.4    \u001b[0m | \u001b[0m31.9     \u001b[0m | \u001b[0m0.07685  \u001b[0m | \u001b[0m0.09966  \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m159.9    \u001b[0m | \u001b[0m0.884    \u001b[0m | \u001b[0m227.6    \u001b[0m | \u001b[0m33.67    \u001b[0m | \u001b[0m0.03408  \u001b[0m | \u001b[0m0.06282  \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m232.8    \u001b[0m | \u001b[0m0.7153   \u001b[0m | \u001b[0m178.5    \u001b[0m | \u001b[0m131.0    \u001b[0m | \u001b[0m0.08936  \u001b[0m | \u001b[0m0.03398  \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m166.1    \u001b[0m | \u001b[0m0.5176   \u001b[0m | \u001b[0m243.0    \u001b[0m | \u001b[0m34.17    \u001b[0m | \u001b[0m0.05292  \u001b[0m | \u001b[0m0.03489  \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m162.0    \u001b[0m | \u001b[0m0.2213   \u001b[0m | \u001b[0m235.2    \u001b[0m | \u001b[0m35.73    \u001b[0m | \u001b[0m0.06898  \u001b[0m | \u001b[0m0.05138  \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m228.6    \u001b[0m | \u001b[0m0.1018   \u001b[0m | \u001b[0m183.0    \u001b[0m | \u001b[0m131.1    \u001b[0m | \u001b[0m0.09147  \u001b[0m | \u001b[0m0.08425  \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m160.9    \u001b[0m | \u001b[0m0.6341   \u001b[0m | \u001b[0m241.1    \u001b[0m | \u001b[0m40.92    \u001b[0m | \u001b[0m0.09649  \u001b[0m | \u001b[0m0.03065  \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m245.0    \u001b[0m | \u001b[0m0.4766   \u001b[0m | \u001b[0m179.5    \u001b[0m | \u001b[0m145.3    \u001b[0m | \u001b[0m0.07452  \u001b[0m | \u001b[0m0.06363  \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m169.3    \u001b[0m | \u001b[0m0.127    \u001b[0m | \u001b[0m238.9    \u001b[0m | \u001b[0m34.33    \u001b[0m | \u001b[0m0.09444  \u001b[0m | \u001b[0m0.06208  \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m251.0    \u001b[0m | \u001b[0m0.3842   \u001b[0m | \u001b[0m192.8    \u001b[0m | \u001b[0m136.3    \u001b[0m | \u001b[0m0.09146  \u001b[0m | \u001b[0m0.04962  \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m165.1    \u001b[0m | \u001b[0m0.6126   \u001b[0m | \u001b[0m236.0    \u001b[0m | \u001b[0m37.43    \u001b[0m | \u001b[0m0.02506  \u001b[0m | \u001b[0m0.07474  \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-0.6391  \u001b[0m | \u001b[0m227.4    \u001b[0m | \u001b[0m0.8706   \u001b[0m | \u001b[0m185.0    \u001b[0m | \u001b[0m126.5    \u001b[0m | \u001b[0m0.09629  \u001b[0m | \u001b[0m0.02552  \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m225.5    \u001b[0m | \u001b[0m0.2711   \u001b[0m | \u001b[0m178.7    \u001b[0m | \u001b[0m126.6    \u001b[0m | \u001b[0m0.0295   \u001b[0m | \u001b[0m0.08224  \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m-0.6264  \u001b[0m | \u001b[0m163.5    \u001b[0m | \u001b[0m0.1489   \u001b[0m | \u001b[0m232.5    \u001b[0m | \u001b[0m32.8     \u001b[0m | \u001b[0m0.08081  \u001b[0m | \u001b[0m0.01796  \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m229.8    \u001b[0m | \u001b[0m0.108    \u001b[0m | \u001b[0m180.1    \u001b[0m | \u001b[0m126.7    \u001b[0m | \u001b[0m0.05331  \u001b[0m | \u001b[0m0.05868  \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m162.5    \u001b[0m | \u001b[0m0.7149   \u001b[0m | \u001b[0m235.1    \u001b[0m | \u001b[0m30.94    \u001b[0m | \u001b[0m0.09992  \u001b[0m | \u001b[0m0.04305  \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m157.5    \u001b[0m | \u001b[0m0.4267   \u001b[0m | \u001b[0m238.4    \u001b[0m | \u001b[0m39.8     \u001b[0m | \u001b[0m0.02109  \u001b[0m | \u001b[0m0.04154  \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m164.1    \u001b[0m | \u001b[0m0.6241   \u001b[0m | \u001b[0m241.0    \u001b[0m | \u001b[0m36.56    \u001b[0m | \u001b[0m0.09575  \u001b[0m | \u001b[0m0.03447  \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m229.5    \u001b[0m | \u001b[0m0.7041   \u001b[0m | \u001b[0m182.6    \u001b[0m | \u001b[0m127.8    \u001b[0m | \u001b[0m0.05968  \u001b[0m | \u001b[0m0.07983  \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m158.4    \u001b[0m | \u001b[0m0.4264   \u001b[0m | \u001b[0m231.9    \u001b[0m | \u001b[0m30.98    \u001b[0m | \u001b[0m0.01979  \u001b[0m | \u001b[0m0.05656  \u001b[0m |\n",
      "| \u001b[95m50       \u001b[0m | \u001b[95m-0.5882  \u001b[0m | \u001b[95m158.0    \u001b[0m | \u001b[95m0.8639   \u001b[0m | \u001b[95m248.5    \u001b[0m | \u001b[95m40.22    \u001b[0m | \u001b[95m0.001788 \u001b[0m | \u001b[95m0.01126  \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m-0.596   \u001b[0m | \u001b[0m101.5    \u001b[0m | \u001b[0m0.494    \u001b[0m | \u001b[0m36.95    \u001b[0m | \u001b[0m227.7    \u001b[0m | \u001b[0m0.09894  \u001b[0m | \u001b[0m0.007293 \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m244.3    \u001b[0m | \u001b[0m0.2891   \u001b[0m | \u001b[0m133.2    \u001b[0m | \u001b[0m152.3    \u001b[0m | \u001b[0m0.06804  \u001b[0m | \u001b[0m0.03502  \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m254.5    \u001b[0m | \u001b[0m0.1048   \u001b[0m | \u001b[0m69.71    \u001b[0m | \u001b[0m187.3    \u001b[0m | \u001b[0m0.09206  \u001b[0m | \u001b[0m0.06765  \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m180.0    \u001b[0m | \u001b[0m0.2234   \u001b[0m | \u001b[0m207.4    \u001b[0m | \u001b[0m254.4    \u001b[0m | \u001b[0m0.0369   \u001b[0m | \u001b[0m0.03029  \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m159.8    \u001b[0m | \u001b[0m0.4625   \u001b[0m | \u001b[0m87.84    \u001b[0m | \u001b[0m86.53    \u001b[0m | \u001b[0m0.05005  \u001b[0m | \u001b[0m0.09975  \u001b[0m |\n",
      "| \u001b[0m56       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m6.073    \u001b[0m | \u001b[0m0.7587   \u001b[0m | \u001b[0m71.33    \u001b[0m | \u001b[0m29.27    \u001b[0m | \u001b[0m0.08934  \u001b[0m | \u001b[0m0.05464  \u001b[0m |\n",
      "| \u001b[0m57       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m177.0    \u001b[0m | \u001b[0m0.7802   \u001b[0m | \u001b[0m172.9    \u001b[0m | \u001b[0m231.2    \u001b[0m | \u001b[0m0.04219  \u001b[0m | \u001b[0m0.03461  \u001b[0m |\n",
      "| \u001b[0m58       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m208.6    \u001b[0m | \u001b[0m0.7294   \u001b[0m | \u001b[0m207.0    \u001b[0m | \u001b[0m78.44    \u001b[0m | \u001b[0m0.03032  \u001b[0m | \u001b[0m0.0883   \u001b[0m |\n",
      "| \u001b[0m59       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m97.43    \u001b[0m | \u001b[0m0.5681   \u001b[0m | \u001b[0m61.6     \u001b[0m | \u001b[0m169.6    \u001b[0m | \u001b[0m0.04069  \u001b[0m | \u001b[0m0.0325   \u001b[0m |\n",
      "| \u001b[0m60       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m138.0    \u001b[0m | \u001b[0m0.3904   \u001b[0m | \u001b[0m215.3    \u001b[0m | \u001b[0m245.5    \u001b[0m | \u001b[0m0.03547  \u001b[0m | \u001b[0m0.07265  \u001b[0m |\n",
      "| \u001b[0m61       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m169.5    \u001b[0m | \u001b[0m0.1387   \u001b[0m | \u001b[0m61.65    \u001b[0m | \u001b[0m135.1    \u001b[0m | \u001b[0m0.06071  \u001b[0m | \u001b[0m0.03677  \u001b[0m |\n",
      "| \u001b[0m62       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m233.5    \u001b[0m | \u001b[0m0.2158   \u001b[0m | \u001b[0m108.5    \u001b[0m | \u001b[0m55.74    \u001b[0m | \u001b[0m0.008608 \u001b[0m | \u001b[0m0.04198  \u001b[0m |\n",
      "| \u001b[0m63       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m125.5    \u001b[0m | \u001b[0m0.2065   \u001b[0m | \u001b[0m106.4    \u001b[0m | \u001b[0m143.4    \u001b[0m | \u001b[0m0.07633  \u001b[0m | \u001b[0m0.05074  \u001b[0m |\n",
      "| \u001b[0m64       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m143.5    \u001b[0m | \u001b[0m0.1654   \u001b[0m | \u001b[0m4.92     \u001b[0m | \u001b[0m236.5    \u001b[0m | \u001b[0m0.09156  \u001b[0m | \u001b[0m0.08011  \u001b[0m |\n",
      "| \u001b[0m65       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m118.2    \u001b[0m | \u001b[0m0.4792   \u001b[0m | \u001b[0m122.8    \u001b[0m | \u001b[0m216.5    \u001b[0m | \u001b[0m0.06138  \u001b[0m | \u001b[0m0.09765  \u001b[0m |\n",
      "| \u001b[0m66       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m61.14    \u001b[0m | \u001b[0m0.82     \u001b[0m | \u001b[0m219.6    \u001b[0m | \u001b[0m23.47    \u001b[0m | \u001b[0m0.01851  \u001b[0m | \u001b[0m0.06797  \u001b[0m |\n",
      "| \u001b[0m67       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m27.12    \u001b[0m | \u001b[0m0.7842   \u001b[0m | \u001b[0m56.89    \u001b[0m | \u001b[0m183.0    \u001b[0m | \u001b[0m0.05841  \u001b[0m | \u001b[0m0.02995  \u001b[0m |\n",
      "| \u001b[0m68       \u001b[0m | \u001b[0m-0.6328  \u001b[0m | \u001b[0m127.2    \u001b[0m | \u001b[0m0.31     \u001b[0m | \u001b[0m237.6    \u001b[0m | \u001b[0m220.5    \u001b[0m | \u001b[0m0.06245  \u001b[0m | \u001b[0m0.01607  \u001b[0m |\n",
      "| \u001b[0m69       \u001b[0m | \u001b[0m-1.217   \u001b[0m | \u001b[0m104.5    \u001b[0m | \u001b[0m0.1752   \u001b[0m | \u001b[0m139.1    \u001b[0m | \u001b[0m8.435    \u001b[0m | \u001b[0m0.003909 \u001b[0m | \u001b[0m0.002003 \u001b[0m |\n",
      "| \u001b[0m70       \u001b[0m | \u001b[0m-1.447   \u001b[0m | \u001b[0m100.9    \u001b[0m | \u001b[0m0.2124   \u001b[0m | \u001b[0m44.08    \u001b[0m | \u001b[0m232.3    \u001b[0m | \u001b[0m0.009438 \u001b[0m | \u001b[0m0.001327 \u001b[0m |\n",
      "| \u001b[0m71       \u001b[0m | \u001b[0m-0.6338  \u001b[0m | \u001b[0m102.6    \u001b[0m | \u001b[0m0.4255   \u001b[0m | \u001b[0m31.41    \u001b[0m | \u001b[0m220.9    \u001b[0m | \u001b[0m0.03118  \u001b[0m | \u001b[0m0.01677  \u001b[0m |\n",
      "| \u001b[0m72       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m104.4    \u001b[0m | \u001b[0m0.7371   \u001b[0m | \u001b[0m29.78    \u001b[0m | \u001b[0m228.7    \u001b[0m | \u001b[0m0.05807  \u001b[0m | \u001b[0m0.0382   \u001b[0m |\n",
      "| \u001b[0m73       \u001b[0m | \u001b[0m-0.662   \u001b[0m | \u001b[0m110.2    \u001b[0m | \u001b[0m0.1296   \u001b[0m | \u001b[0m35.41    \u001b[0m | \u001b[0m222.5    \u001b[0m | \u001b[0m0.09442  \u001b[0m | \u001b[0m0.00522  \u001b[0m |\n",
      "| \u001b[0m74       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m102.5    \u001b[0m | \u001b[0m0.4773   \u001b[0m | \u001b[0m36.66    \u001b[0m | \u001b[0m223.3    \u001b[0m | \u001b[0m0.001257 \u001b[0m | \u001b[0m0.06818  \u001b[0m |\n",
      "| \u001b[0m75       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m99.87    \u001b[0m | \u001b[0m0.8836   \u001b[0m | \u001b[0m26.94    \u001b[0m | \u001b[0m215.7    \u001b[0m | \u001b[0m0.04389  \u001b[0m | \u001b[0m0.05913  \u001b[0m |\n",
      "| \u001b[0m76       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m131.8    \u001b[0m | \u001b[0m0.579    \u001b[0m | \u001b[0m233.0    \u001b[0m | \u001b[0m222.8    \u001b[0m | \u001b[0m0.08489  \u001b[0m | \u001b[0m0.07247  \u001b[0m |\n",
      "| \u001b[0m77       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m95.56    \u001b[0m | \u001b[0m0.4231   \u001b[0m | \u001b[0m34.91    \u001b[0m | \u001b[0m224.2    \u001b[0m | \u001b[0m0.07124  \u001b[0m | \u001b[0m0.07649  \u001b[0m |\n",
      "| \u001b[0m78       \u001b[0m | \u001b[0m-0.6876  \u001b[0m | \u001b[0m159.9    \u001b[0m | \u001b[0m0.1702   \u001b[0m | \u001b[0m252.3    \u001b[0m | \u001b[0m33.02    \u001b[0m | \u001b[0m0.03131  \u001b[0m | \u001b[0m0.01318  \u001b[0m |\n",
      "| \u001b[0m79       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m152.6    \u001b[0m | \u001b[0m0.8555   \u001b[0m | \u001b[0m250.4    \u001b[0m | \u001b[0m46.35    \u001b[0m | \u001b[0m0.003988 \u001b[0m | \u001b[0m0.08823  \u001b[0m |\n",
      "| \u001b[0m80       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m164.4    \u001b[0m | \u001b[0m0.7341   \u001b[0m | \u001b[0m251.5    \u001b[0m | \u001b[0m39.25    \u001b[0m | \u001b[0m0.09065  \u001b[0m | \u001b[0m0.06746  \u001b[0m |\n",
      "| \u001b[0m81       \u001b[0m | \u001b[0m-0.6931  \u001b[0m | \u001b[0m128.4    \u001b[0m | \u001b[0m0.7025   \u001b[0m | \u001b[0m239.3    \u001b[0m | \u001b[0m213.6    \u001b[0m | \u001b[0m0.07385  \u001b[0m | \u001b[0m0.05317  \u001b[0m |\n",
      "| \u001b[0m82       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m134.5    \u001b[0m | \u001b[0m0.4449   \u001b[0m | \u001b[0m243.5    \u001b[0m | \u001b[0m222.6    \u001b[0m | \u001b[0m0.01442  \u001b[0m | \u001b[0m0.05884  \u001b[0m |\n",
      "| \u001b[0m83       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m110.0    \u001b[0m | \u001b[0m0.7566   \u001b[0m | \u001b[0m26.82    \u001b[0m | \u001b[0m215.4    \u001b[0m | \u001b[0m0.03882  \u001b[0m | \u001b[0m0.07114  \u001b[0m |\n",
      "| \u001b[0m84       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m117.4    \u001b[0m | \u001b[0m0.2786   \u001b[0m | \u001b[0m237.6    \u001b[0m | \u001b[0m222.4    \u001b[0m | \u001b[0m0.03985  \u001b[0m | \u001b[0m0.03682  \u001b[0m |\n",
      "| \u001b[0m85       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m152.4    \u001b[0m | \u001b[0m0.6765   \u001b[0m | \u001b[0m245.1    \u001b[0m | \u001b[0m39.57    \u001b[0m | \u001b[0m0.08367  \u001b[0m | \u001b[0m0.05681  \u001b[0m |\n",
      "| \u001b[0m86       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m125.9    \u001b[0m | \u001b[0m0.3937   \u001b[0m | \u001b[0m247.1    \u001b[0m | \u001b[0m228.2    \u001b[0m | \u001b[0m0.005209 \u001b[0m | \u001b[0m0.03364  \u001b[0m |\n",
      "| \u001b[0m87       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m159.5    \u001b[0m | \u001b[0m0.7791   \u001b[0m | \u001b[0m253.5    \u001b[0m | \u001b[0m44.76    \u001b[0m | \u001b[0m0.08521  \u001b[0m | \u001b[0m0.06945  \u001b[0m |\n",
      "| \u001b[0m88       \u001b[0m | \u001b[0m-0.6107  \u001b[0m | \u001b[0m217.6    \u001b[0m | \u001b[0m0.4415   \u001b[0m | \u001b[0m189.3    \u001b[0m | \u001b[0m136.5    \u001b[0m | \u001b[0m0.06053  \u001b[0m | \u001b[0m0.005841 \u001b[0m |\n",
      "| \u001b[0m89       \u001b[0m | \u001b[0m-0.6931  \u001b[0m | \u001b[0m210.9    \u001b[0m | \u001b[0m0.8967   \u001b[0m | \u001b[0m188.6    \u001b[0m | \u001b[0m141.1    \u001b[0m | \u001b[0m0.03653  \u001b[0m | \u001b[0m0.02904  \u001b[0m |\n",
      "| \u001b[0m90       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m216.1    \u001b[0m | \u001b[0m0.2828   \u001b[0m | \u001b[0m196.2    \u001b[0m | \u001b[0m144.9    \u001b[0m | \u001b[0m0.001195 \u001b[0m | \u001b[0m0.08138  \u001b[0m |\n",
      "| \u001b[0m91       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m222.0    \u001b[0m | \u001b[0m0.8145   \u001b[0m | \u001b[0m185.5    \u001b[0m | \u001b[0m138.3    \u001b[0m | \u001b[0m0.05706  \u001b[0m | \u001b[0m0.0779   \u001b[0m |\n",
      "| \u001b[95m92       \u001b[0m | \u001b[95m-0.5774  \u001b[0m | \u001b[95m220.8    \u001b[0m | \u001b[95m0.6922   \u001b[0m | \u001b[95m197.7    \u001b[0m | \u001b[95m132.3    \u001b[0m | \u001b[95m0.02771  \u001b[0m | \u001b[95m0.02001  \u001b[0m |\n",
      "| \u001b[0m93       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m219.1    \u001b[0m | \u001b[0m0.2828   \u001b[0m | \u001b[0m203.2    \u001b[0m | \u001b[0m135.7    \u001b[0m | \u001b[0m0.04195  \u001b[0m | \u001b[0m0.09581  \u001b[0m |\n",
      "| \u001b[0m94       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m215.2    \u001b[0m | \u001b[0m0.5402   \u001b[0m | \u001b[0m193.7    \u001b[0m | \u001b[0m130.1    \u001b[0m | \u001b[0m0.08334  \u001b[0m | \u001b[0m0.07958  \u001b[0m |\n",
      "| \u001b[0m95       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m226.0    \u001b[0m | \u001b[0m0.2801   \u001b[0m | \u001b[0m203.1    \u001b[0m | \u001b[0m125.6    \u001b[0m | \u001b[0m0.07276  \u001b[0m | \u001b[0m0.05435  \u001b[0m |\n",
      "| \u001b[0m96       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m166.7    \u001b[0m | \u001b[0m0.132    \u001b[0m | \u001b[0m228.2    \u001b[0m | \u001b[0m26.83    \u001b[0m | \u001b[0m0.09948  \u001b[0m | \u001b[0m0.09172  \u001b[0m |\n",
      "| \u001b[0m97       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m222.9    \u001b[0m | \u001b[0m0.4857   \u001b[0m | \u001b[0m186.6    \u001b[0m | \u001b[0m129.1    \u001b[0m | \u001b[0m0.05854  \u001b[0m | \u001b[0m0.07708  \u001b[0m |\n",
      "| \u001b[0m98       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m230.7    \u001b[0m | \u001b[0m0.2878   \u001b[0m | \u001b[0m199.3    \u001b[0m | \u001b[0m139.6    \u001b[0m | \u001b[0m0.05954  \u001b[0m | \u001b[0m0.07721  \u001b[0m |\n",
      "| \u001b[0m99       \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m117.9    \u001b[0m | \u001b[0m0.1412   \u001b[0m | \u001b[0m33.27    \u001b[0m | \u001b[0m224.8    \u001b[0m | \u001b[0m0.05722  \u001b[0m | \u001b[0m0.02348  \u001b[0m |\n",
      "| \u001b[0m100      \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m160.1    \u001b[0m | \u001b[0m0.2783   \u001b[0m | \u001b[0m243.6    \u001b[0m | \u001b[0m33.43    \u001b[0m | \u001b[0m0.05468  \u001b[0m | \u001b[0m0.05576  \u001b[0m |\n",
      "| \u001b[0m101      \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m226.4    \u001b[0m | \u001b[0m0.7857   \u001b[0m | \u001b[0m202.2    \u001b[0m | \u001b[0m134.4    \u001b[0m | \u001b[0m0.09672  \u001b[0m | \u001b[0m0.02489  \u001b[0m |\n",
      "| \u001b[0m102      \u001b[0m | \u001b[0m-0.6931  \u001b[0m | \u001b[0m223.5    \u001b[0m | \u001b[0m0.7788   \u001b[0m | \u001b[0m194.6    \u001b[0m | \u001b[0m132.5    \u001b[0m | \u001b[0m0.0862   \u001b[0m | \u001b[0m0.06009  \u001b[0m |\n",
      "| \u001b[0m103      \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m214.8    \u001b[0m | \u001b[0m0.2231   \u001b[0m | \u001b[0m200.1    \u001b[0m | \u001b[0m129.2    \u001b[0m | \u001b[0m0.01709  \u001b[0m | \u001b[0m0.09974  \u001b[0m |\n",
      "| \u001b[0m104      \u001b[0m | \u001b[0m-0.6247  \u001b[0m | \u001b[0m123.6    \u001b[0m | \u001b[0m0.6432   \u001b[0m | \u001b[0m233.5    \u001b[0m | \u001b[0m233.1    \u001b[0m | \u001b[0m0.09049  \u001b[0m | \u001b[0m0.009754 \u001b[0m |\n",
      "| \u001b[0m105      \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m124.5    \u001b[0m | \u001b[0m0.836    \u001b[0m | \u001b[0m241.9    \u001b[0m | \u001b[0m236.7    \u001b[0m | \u001b[0m0.04223  \u001b[0m | \u001b[0m0.04348  \u001b[0m |\n",
      "| \u001b[0m106      \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m117.3    \u001b[0m | \u001b[0m0.4435   \u001b[0m | \u001b[0m234.5    \u001b[0m | \u001b[0m232.0    \u001b[0m | \u001b[0m0.0902   \u001b[0m | \u001b[0m0.06361  \u001b[0m |\n",
      "| \u001b[0m107      \u001b[0m | \u001b[0m-0.6634  \u001b[0m | \u001b[0m213.9    \u001b[0m | \u001b[0m0.2796   \u001b[0m | \u001b[0m193.9    \u001b[0m | \u001b[0m137.0    \u001b[0m | \u001b[0m0.09588  \u001b[0m | \u001b[0m0.02012  \u001b[0m |\n",
      "| \u001b[0m108      \u001b[0m | \u001b[0m-0.6707  \u001b[0m | \u001b[0m124.1    \u001b[0m | \u001b[0m0.732    \u001b[0m | \u001b[0m242.5    \u001b[0m | \u001b[0m219.7    \u001b[0m | \u001b[0m0.08781  \u001b[0m | \u001b[0m0.008213 \u001b[0m |\n",
      "| \u001b[0m109      \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m220.7    \u001b[0m | \u001b[0m0.8393   \u001b[0m | \u001b[0m196.7    \u001b[0m | \u001b[0m126.0    \u001b[0m | \u001b[0m0.0447   \u001b[0m | \u001b[0m0.06448  \u001b[0m |\n",
      "| \u001b[0m110      \u001b[0m | \u001b[0m-0.6932  \u001b[0m | \u001b[0m123.2    \u001b[0m | \u001b[0m0.3717   \u001b[0m | \u001b[0m228.7    \u001b[0m | \u001b[0m232.9    \u001b[0m | \u001b[0m0.04868  \u001b[0m | \u001b[0m0.09496  \u001b[0m |\n",
      "=================================================================================================\n",
      "Final log loss of best model: 0.5807237139227318\n",
      "Final accuracy of best model: 0.6792452830188679\n"
     ]
    }
   ],
   "source": [
    "# run bayes opt nn (mens)\n",
    "bayes_opt_nn(df=X_mens, init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This accuracy is still worse than our results using the compact data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "|   iter    |  target   | combin... | dropou... | hidden... | hidden... |    lr     | weight... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-0.4951  \u001b[0m | \u001b[0m4.635    \u001b[0m | \u001b[0m0.5015   \u001b[0m | \u001b[0m127.9    \u001b[0m | \u001b[0m35.99    \u001b[0m | \u001b[0m0.01422  \u001b[0m | \u001b[0m0.02186  \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m108.3    \u001b[0m | \u001b[0m0.2985   \u001b[0m | \u001b[0m23.35    \u001b[0m | \u001b[0m89.76    \u001b[0m | \u001b[0m0.01669  \u001b[0m | \u001b[0m0.08786  \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m243.5    \u001b[0m | \u001b[0m0.131    \u001b[0m | \u001b[0m179.6    \u001b[0m | \u001b[0m147.5    \u001b[0m | \u001b[0m0.0898   \u001b[0m | \u001b[0m0.06669  \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m141.2    \u001b[0m | \u001b[0m0.6619   \u001b[0m | \u001b[0m100.2    \u001b[0m | \u001b[0m178.4    \u001b[0m | \u001b[0m0.08249  \u001b[0m | \u001b[0m0.04657  \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m240.9    \u001b[0m | \u001b[0m0.743    \u001b[0m | \u001b[0m251.8    \u001b[0m | \u001b[0m45.09    \u001b[0m | \u001b[0m0.05211  \u001b[0m | \u001b[0m0.09455  \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m167.0    \u001b[0m | \u001b[0m0.7887   \u001b[0m | \u001b[0m248.6    \u001b[0m | \u001b[0m49.13    \u001b[0m | \u001b[0m0.02432  \u001b[0m | \u001b[0m0.08734  \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m232.2    \u001b[0m | \u001b[0m0.7066   \u001b[0m | \u001b[0m90.11    \u001b[0m | \u001b[0m119.7    \u001b[0m | \u001b[0m0.06106  \u001b[0m | \u001b[0m0.04963  \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m123.4    \u001b[0m | \u001b[0m0.4484   \u001b[0m | \u001b[0m223.1    \u001b[0m | \u001b[0m49.59    \u001b[0m | \u001b[0m0.06877  \u001b[0m | \u001b[0m0.07036  \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-0.5889  \u001b[0m | \u001b[0m237.0    \u001b[0m | \u001b[0m0.1316   \u001b[0m | \u001b[0m166.4    \u001b[0m | \u001b[0m164.1    \u001b[0m | \u001b[0m0.04618  \u001b[0m | \u001b[0m0.005315 \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-0.6279  \u001b[0m | \u001b[0m11.8     \u001b[0m | \u001b[0m0.2356   \u001b[0m | \u001b[0m203.8    \u001b[0m | \u001b[0m43.3     \u001b[0m | \u001b[0m0.01388  \u001b[0m | \u001b[0m0.0353   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-0.509   \u001b[0m | \u001b[0m231.5    \u001b[0m | \u001b[0m0.3425   \u001b[0m | \u001b[0m162.0    \u001b[0m | \u001b[0m161.1    \u001b[0m | \u001b[0m0.07178  \u001b[0m | \u001b[0m0.02844  \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m3.528    \u001b[0m | \u001b[0m0.2887   \u001b[0m | \u001b[0m126.7    \u001b[0m | \u001b[0m36.52    \u001b[0m | \u001b[0m0.06708  \u001b[0m | \u001b[0m0.0437   \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m19.49    \u001b[0m | \u001b[0m0.2669   \u001b[0m | \u001b[0m154.2    \u001b[0m | \u001b[0m174.4    \u001b[0m | \u001b[0m0.02375  \u001b[0m | \u001b[0m0.09049  \u001b[0m |\n",
      "| \u001b[95m14       \u001b[0m | \u001b[95m-0.4854  \u001b[0m | \u001b[95m40.44    \u001b[0m | \u001b[95m0.4709   \u001b[0m | \u001b[95m165.7    \u001b[0m | \u001b[95m55.69    \u001b[0m | \u001b[95m0.03046  \u001b[0m | \u001b[95m0.02064  \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-0.6936  \u001b[0m | \u001b[0m12.78    \u001b[0m | \u001b[0m0.1632   \u001b[0m | \u001b[0m53.72    \u001b[0m | \u001b[0m133.2    \u001b[0m | \u001b[0m0.001054 \u001b[0m | \u001b[0m0.07576  \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m54.54    \u001b[0m | \u001b[0m0.5277   \u001b[0m | \u001b[0m68.94    \u001b[0m | \u001b[0m255.8    \u001b[0m | \u001b[0m0.06406  \u001b[0m | \u001b[0m0.05314  \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m18.89    \u001b[0m | \u001b[0m0.8983   \u001b[0m | \u001b[0m225.4    \u001b[0m | \u001b[0m119.7    \u001b[0m | \u001b[0m0.06193  \u001b[0m | \u001b[0m0.04666  \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m191.0    \u001b[0m | \u001b[0m0.3161   \u001b[0m | \u001b[0m168.5    \u001b[0m | \u001b[0m66.58    \u001b[0m | \u001b[0m0.05989  \u001b[0m | \u001b[0m0.08351  \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m18.26    \u001b[0m | \u001b[0m0.5572   \u001b[0m | \u001b[0m204.7    \u001b[0m | \u001b[0m34.72    \u001b[0m | \u001b[0m0.03989  \u001b[0m | \u001b[0m0.04946  \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m48.2     \u001b[0m | \u001b[0m0.639    \u001b[0m | \u001b[0m222.0    \u001b[0m | \u001b[0m148.4    \u001b[0m | \u001b[0m0.08671  \u001b[0m | \u001b[0m0.08897  \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m219.8    \u001b[0m | \u001b[0m0.3962   \u001b[0m | \u001b[0m48.61    \u001b[0m | \u001b[0m173.6    \u001b[0m | \u001b[0m0.08214  \u001b[0m | \u001b[0m0.09623  \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m94.9     \u001b[0m | \u001b[0m0.5972   \u001b[0m | \u001b[0m245.3    \u001b[0m | \u001b[0m13.99    \u001b[0m | \u001b[0m0.06679  \u001b[0m | \u001b[0m0.09414  \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m-0.505   \u001b[0m | \u001b[0m174.9    \u001b[0m | \u001b[0m0.1675   \u001b[0m | \u001b[0m198.1    \u001b[0m | \u001b[0m223.4    \u001b[0m | \u001b[0m0.01609  \u001b[0m | \u001b[0m0.05277  \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m33.11    \u001b[0m | \u001b[0m0.474    \u001b[0m | \u001b[0m63.42    \u001b[0m | \u001b[0m57.25    \u001b[0m | \u001b[0m0.08919  \u001b[0m | \u001b[0m0.06221  \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m5.22     \u001b[0m | \u001b[0m0.7843   \u001b[0m | \u001b[0m172.3    \u001b[0m | \u001b[0m172.0    \u001b[0m | \u001b[0m0.03705  \u001b[0m | \u001b[0m0.05896  \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m-0.6362  \u001b[0m | \u001b[0m68.5     \u001b[0m | \u001b[0m0.4153   \u001b[0m | \u001b[0m219.4    \u001b[0m | \u001b[0m49.4     \u001b[0m | \u001b[0m0.07576  \u001b[0m | \u001b[0m0.02193  \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m123.5    \u001b[0m | \u001b[0m0.895    \u001b[0m | \u001b[0m15.25    \u001b[0m | \u001b[0m118.2    \u001b[0m | \u001b[0m0.03472  \u001b[0m | \u001b[0m0.09231  \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m195.9    \u001b[0m | \u001b[0m0.7765   \u001b[0m | \u001b[0m127.6    \u001b[0m | \u001b[0m214.9    \u001b[0m | \u001b[0m0.08784  \u001b[0m | \u001b[0m0.07928  \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m-0.5679  \u001b[0m | \u001b[0m10.49    \u001b[0m | \u001b[0m0.2297   \u001b[0m | \u001b[0m45.11    \u001b[0m | \u001b[0m49.72    \u001b[0m | \u001b[0m0.09334  \u001b[0m | \u001b[0m0.009571 \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m3.495    \u001b[0m | \u001b[0m0.4073   \u001b[0m | \u001b[0m230.2    \u001b[0m | \u001b[0m241.4    \u001b[0m | \u001b[0m0.09144  \u001b[0m | \u001b[0m0.06453  \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m43.85    \u001b[0m | \u001b[0m0.2836   \u001b[0m | \u001b[0m126.1    \u001b[0m | \u001b[0m172.6    \u001b[0m | \u001b[0m0.008025 \u001b[0m | \u001b[0m0.07893  \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m92.14    \u001b[0m | \u001b[0m0.2128   \u001b[0m | \u001b[0m25.58    \u001b[0m | \u001b[0m154.4    \u001b[0m | \u001b[0m0.05056  \u001b[0m | \u001b[0m0.05466  \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m-0.4991  \u001b[0m | \u001b[0m14.39    \u001b[0m | \u001b[0m0.7422   \u001b[0m | \u001b[0m250.4    \u001b[0m | \u001b[0m43.41    \u001b[0m | \u001b[0m0.002482 \u001b[0m | \u001b[0m0.01701  \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m3.468    \u001b[0m | \u001b[0m0.403    \u001b[0m | \u001b[0m135.2    \u001b[0m | \u001b[0m136.7    \u001b[0m | \u001b[0m0.05321  \u001b[0m | \u001b[0m0.09907  \u001b[0m |\n",
      "| \u001b[95m35       \u001b[0m | \u001b[95m-0.4785  \u001b[0m | \u001b[95m252.5    \u001b[0m | \u001b[95m0.4673   \u001b[0m | \u001b[95m20.94    \u001b[0m | \u001b[95m243.8    \u001b[0m | \u001b[95m0.01052  \u001b[0m | \u001b[95m0.02361  \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m-0.6374  \u001b[0m | \u001b[0m126.3    \u001b[0m | \u001b[0m0.7119   \u001b[0m | \u001b[0m177.1    \u001b[0m | \u001b[0m3.154    \u001b[0m | \u001b[0m0.06182  \u001b[0m | \u001b[0m0.03038  \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m18.6     \u001b[0m | \u001b[0m0.3337   \u001b[0m | \u001b[0m118.9    \u001b[0m | \u001b[0m116.3    \u001b[0m | \u001b[0m0.05849  \u001b[0m | \u001b[0m0.09292  \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m132.3    \u001b[0m | \u001b[0m0.5119   \u001b[0m | \u001b[0m30.15    \u001b[0m | \u001b[0m49.74    \u001b[0m | \u001b[0m0.05241  \u001b[0m | \u001b[0m0.08131  \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m109.7    \u001b[0m | \u001b[0m0.6341   \u001b[0m | \u001b[0m13.55    \u001b[0m | \u001b[0m6.817    \u001b[0m | \u001b[0m0.02557  \u001b[0m | \u001b[0m0.08961  \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m112.4    \u001b[0m | \u001b[0m0.3815   \u001b[0m | \u001b[0m175.4    \u001b[0m | \u001b[0m205.6    \u001b[0m | \u001b[0m0.05887  \u001b[0m | \u001b[0m0.06294  \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m24.07    \u001b[0m | \u001b[0m0.3703   \u001b[0m | \u001b[0m113.0    \u001b[0m | \u001b[0m150.8    \u001b[0m | \u001b[0m0.03198  \u001b[0m | \u001b[0m0.09309  \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m-0.5556  \u001b[0m | \u001b[0m128.5    \u001b[0m | \u001b[0m0.2781   \u001b[0m | \u001b[0m35.18    \u001b[0m | \u001b[0m209.0    \u001b[0m | \u001b[0m0.05926  \u001b[0m | \u001b[0m0.007307 \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m175.0    \u001b[0m | \u001b[0m0.2025   \u001b[0m | \u001b[0m81.97    \u001b[0m | \u001b[0m184.8    \u001b[0m | \u001b[0m0.0701   \u001b[0m | \u001b[0m0.06801  \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m55.89    \u001b[0m | \u001b[0m0.1087   \u001b[0m | \u001b[0m205.9    \u001b[0m | \u001b[0m196.9    \u001b[0m | \u001b[0m0.022    \u001b[0m | \u001b[0m0.05443  \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m197.7    \u001b[0m | \u001b[0m0.8883   \u001b[0m | \u001b[0m81.03    \u001b[0m | \u001b[0m39.06    \u001b[0m | \u001b[0m0.09819  \u001b[0m | \u001b[0m0.09611  \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m53.03    \u001b[0m | \u001b[0m0.5772   \u001b[0m | \u001b[0m121.7    \u001b[0m | \u001b[0m24.33    \u001b[0m | \u001b[0m0.01775  \u001b[0m | \u001b[0m0.04279  \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m-0.5514  \u001b[0m | \u001b[0m161.1    \u001b[0m | \u001b[0m0.3371   \u001b[0m | \u001b[0m33.09    \u001b[0m | \u001b[0m2.427    \u001b[0m | \u001b[0m0.03598  \u001b[0m | \u001b[0m0.01086  \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m104.4    \u001b[0m | \u001b[0m0.59     \u001b[0m | \u001b[0m161.7    \u001b[0m | \u001b[0m82.09    \u001b[0m | \u001b[0m0.09258  \u001b[0m | \u001b[0m0.05391  \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m-0.5849  \u001b[0m | \u001b[0m43.42    \u001b[0m | \u001b[0m0.3897   \u001b[0m | \u001b[0m51.84    \u001b[0m | \u001b[0m230.7    \u001b[0m | \u001b[0m0.02356  \u001b[0m | \u001b[0m0.007131 \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m-0.4984  \u001b[0m | \u001b[0m113.0    \u001b[0m | \u001b[0m0.235    \u001b[0m | \u001b[0m125.9    \u001b[0m | \u001b[0m79.59    \u001b[0m | \u001b[0m0.07387  \u001b[0m | \u001b[0m0.03078  \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m-0.5134  \u001b[0m | \u001b[0m101.5    \u001b[0m | \u001b[0m0.494    \u001b[0m | \u001b[0m36.95    \u001b[0m | \u001b[0m227.7    \u001b[0m | \u001b[0m0.09894  \u001b[0m | \u001b[0m0.007293 \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m-0.4799  \u001b[0m | \u001b[0m244.3    \u001b[0m | \u001b[0m0.2891   \u001b[0m | \u001b[0m133.2    \u001b[0m | \u001b[0m152.3    \u001b[0m | \u001b[0m0.06804  \u001b[0m | \u001b[0m0.03502  \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m-0.602   \u001b[0m | \u001b[0m254.5    \u001b[0m | \u001b[0m0.1048   \u001b[0m | \u001b[0m69.71    \u001b[0m | \u001b[0m187.3    \u001b[0m | \u001b[0m0.09206  \u001b[0m | \u001b[0m0.06765  \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m-0.4802  \u001b[0m | \u001b[0m180.0    \u001b[0m | \u001b[0m0.2234   \u001b[0m | \u001b[0m207.4    \u001b[0m | \u001b[0m254.4    \u001b[0m | \u001b[0m0.0369   \u001b[0m | \u001b[0m0.03029  \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m159.8    \u001b[0m | \u001b[0m0.4625   \u001b[0m | \u001b[0m87.84    \u001b[0m | \u001b[0m86.53    \u001b[0m | \u001b[0m0.05005  \u001b[0m | \u001b[0m0.09975  \u001b[0m |\n",
      "| \u001b[0m56       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m6.073    \u001b[0m | \u001b[0m0.7587   \u001b[0m | \u001b[0m71.33    \u001b[0m | \u001b[0m29.27    \u001b[0m | \u001b[0m0.08934  \u001b[0m | \u001b[0m0.05464  \u001b[0m |\n",
      "| \u001b[0m57       \u001b[0m | \u001b[0m-0.4797  \u001b[0m | \u001b[0m177.0    \u001b[0m | \u001b[0m0.7802   \u001b[0m | \u001b[0m172.9    \u001b[0m | \u001b[0m231.2    \u001b[0m | \u001b[0m0.04219  \u001b[0m | \u001b[0m0.03461  \u001b[0m |\n",
      "| \u001b[0m58       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m208.6    \u001b[0m | \u001b[0m0.7294   \u001b[0m | \u001b[0m207.0    \u001b[0m | \u001b[0m78.44    \u001b[0m | \u001b[0m0.03032  \u001b[0m | \u001b[0m0.0883   \u001b[0m |\n",
      "| \u001b[95m59       \u001b[0m | \u001b[95m-0.4737  \u001b[0m | \u001b[95m97.43    \u001b[0m | \u001b[95m0.5681   \u001b[0m | \u001b[95m61.6     \u001b[0m | \u001b[95m169.6    \u001b[0m | \u001b[95m0.04069  \u001b[0m | \u001b[95m0.0325   \u001b[0m |\n",
      "| \u001b[0m60       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m138.0    \u001b[0m | \u001b[0m0.3904   \u001b[0m | \u001b[0m215.3    \u001b[0m | \u001b[0m245.5    \u001b[0m | \u001b[0m0.03547  \u001b[0m | \u001b[0m0.07265  \u001b[0m |\n",
      "| \u001b[0m61       \u001b[0m | \u001b[0m-0.4885  \u001b[0m | \u001b[0m169.5    \u001b[0m | \u001b[0m0.1387   \u001b[0m | \u001b[0m61.65    \u001b[0m | \u001b[0m135.1    \u001b[0m | \u001b[0m0.06071  \u001b[0m | \u001b[0m0.03677  \u001b[0m |\n",
      "| \u001b[0m62       \u001b[0m | \u001b[0m-0.4861  \u001b[0m | \u001b[0m233.5    \u001b[0m | \u001b[0m0.2158   \u001b[0m | \u001b[0m108.5    \u001b[0m | \u001b[0m55.74    \u001b[0m | \u001b[0m0.008608 \u001b[0m | \u001b[0m0.04198  \u001b[0m |\n",
      "| \u001b[0m63       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m125.5    \u001b[0m | \u001b[0m0.2065   \u001b[0m | \u001b[0m106.4    \u001b[0m | \u001b[0m143.4    \u001b[0m | \u001b[0m0.07633  \u001b[0m | \u001b[0m0.05074  \u001b[0m |\n",
      "| \u001b[0m64       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m143.5    \u001b[0m | \u001b[0m0.1654   \u001b[0m | \u001b[0m4.92     \u001b[0m | \u001b[0m236.5    \u001b[0m | \u001b[0m0.09156  \u001b[0m | \u001b[0m0.08011  \u001b[0m |\n",
      "| \u001b[0m65       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m118.2    \u001b[0m | \u001b[0m0.4792   \u001b[0m | \u001b[0m122.8    \u001b[0m | \u001b[0m216.5    \u001b[0m | \u001b[0m0.06138  \u001b[0m | \u001b[0m0.09765  \u001b[0m |\n",
      "| \u001b[0m66       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m61.14    \u001b[0m | \u001b[0m0.82     \u001b[0m | \u001b[0m219.6    \u001b[0m | \u001b[0m23.47    \u001b[0m | \u001b[0m0.01851  \u001b[0m | \u001b[0m0.06797  \u001b[0m |\n",
      "| \u001b[0m67       \u001b[0m | \u001b[0m-0.5843  \u001b[0m | \u001b[0m27.12    \u001b[0m | \u001b[0m0.7842   \u001b[0m | \u001b[0m56.89    \u001b[0m | \u001b[0m183.0    \u001b[0m | \u001b[0m0.05841  \u001b[0m | \u001b[0m0.02995  \u001b[0m |\n",
      "| \u001b[95m68       \u001b[0m | \u001b[95m-0.4701  \u001b[0m | \u001b[95m127.2    \u001b[0m | \u001b[95m0.31     \u001b[0m | \u001b[95m237.6    \u001b[0m | \u001b[95m220.5    \u001b[0m | \u001b[95m0.06245  \u001b[0m | \u001b[95m0.01607  \u001b[0m |\n",
      "| \u001b[0m69       \u001b[0m | \u001b[0m-1.068   \u001b[0m | \u001b[0m104.5    \u001b[0m | \u001b[0m0.1752   \u001b[0m | \u001b[0m139.1    \u001b[0m | \u001b[0m8.435    \u001b[0m | \u001b[0m0.003909 \u001b[0m | \u001b[0m0.002003 \u001b[0m |\n",
      "| \u001b[0m70       \u001b[0m | \u001b[0m-0.4855  \u001b[0m | \u001b[0m130.5    \u001b[0m | \u001b[0m0.2269   \u001b[0m | \u001b[0m253.8    \u001b[0m | \u001b[0m169.6    \u001b[0m | \u001b[0m0.05765  \u001b[0m | \u001b[0m0.02476  \u001b[0m |\n",
      "| \u001b[0m71       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m88.8     \u001b[0m | \u001b[0m0.4251   \u001b[0m | \u001b[0m34.97    \u001b[0m | \u001b[0m199.7    \u001b[0m | \u001b[0m0.0516   \u001b[0m | \u001b[0m0.06417  \u001b[0m |\n",
      "| \u001b[0m72       \u001b[0m | \u001b[0m-0.4979  \u001b[0m | \u001b[0m247.0    \u001b[0m | \u001b[0m0.8414   \u001b[0m | \u001b[0m53.84    \u001b[0m | \u001b[0m47.37    \u001b[0m | \u001b[0m0.08172  \u001b[0m | \u001b[0m0.01134  \u001b[0m |\n",
      "| \u001b[0m73       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m111.6    \u001b[0m | \u001b[0m0.5726   \u001b[0m | \u001b[0m114.4    \u001b[0m | \u001b[0m65.23    \u001b[0m | \u001b[0m0.0758   \u001b[0m | \u001b[0m0.06667  \u001b[0m |\n",
      "| \u001b[0m74       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m101.1    \u001b[0m | \u001b[0m0.3458   \u001b[0m | \u001b[0m245.2    \u001b[0m | \u001b[0m21.83    \u001b[0m | \u001b[0m0.02279  \u001b[0m | \u001b[0m0.06807  \u001b[0m |\n",
      "| \u001b[0m75       \u001b[0m | \u001b[0m-0.4877  \u001b[0m | \u001b[0m85.52    \u001b[0m | \u001b[0m0.1061   \u001b[0m | \u001b[0m104.2    \u001b[0m | \u001b[0m85.92    \u001b[0m | \u001b[0m0.01439  \u001b[0m | \u001b[0m0.03807  \u001b[0m |\n",
      "| \u001b[0m76       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m5.555    \u001b[0m | \u001b[0m0.8699   \u001b[0m | \u001b[0m204.7    \u001b[0m | \u001b[0m138.5    \u001b[0m | \u001b[0m0.06649  \u001b[0m | \u001b[0m0.09391  \u001b[0m |\n",
      "| \u001b[0m77       \u001b[0m | \u001b[0m-0.5471  \u001b[0m | \u001b[0m31.03    \u001b[0m | \u001b[0m0.8913   \u001b[0m | \u001b[0m236.8    \u001b[0m | \u001b[0m57.38    \u001b[0m | \u001b[0m0.06019  \u001b[0m | \u001b[0m0.01311  \u001b[0m |\n",
      "| \u001b[0m78       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m90.2     \u001b[0m | \u001b[0m0.8719   \u001b[0m | \u001b[0m190.7    \u001b[0m | \u001b[0m241.1    \u001b[0m | \u001b[0m0.01076  \u001b[0m | \u001b[0m0.08619  \u001b[0m |\n",
      "| \u001b[0m79       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m208.3    \u001b[0m | \u001b[0m0.6785   \u001b[0m | \u001b[0m217.8    \u001b[0m | \u001b[0m77.65    \u001b[0m | \u001b[0m0.03371  \u001b[0m | \u001b[0m0.08191  \u001b[0m |\n",
      "| \u001b[0m80       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m21.5     \u001b[0m | \u001b[0m0.363    \u001b[0m | \u001b[0m51.45    \u001b[0m | \u001b[0m208.3    \u001b[0m | \u001b[0m0.09272  \u001b[0m | \u001b[0m0.0924   \u001b[0m |\n",
      "| \u001b[0m81       \u001b[0m | \u001b[0m-0.4768  \u001b[0m | \u001b[0m33.11    \u001b[0m | \u001b[0m0.2833   \u001b[0m | \u001b[0m52.16    \u001b[0m | \u001b[0m46.2     \u001b[0m | \u001b[0m0.09873  \u001b[0m | \u001b[0m0.03158  \u001b[0m |\n",
      "| \u001b[0m82       \u001b[0m | \u001b[0m-0.5101  \u001b[0m | \u001b[0m189.5    \u001b[0m | \u001b[0m0.8699   \u001b[0m | \u001b[0m41.22    \u001b[0m | \u001b[0m250.3    \u001b[0m | \u001b[0m0.06752  \u001b[0m | \u001b[0m0.03429  \u001b[0m |\n",
      "| \u001b[0m83       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m108.3    \u001b[0m | \u001b[0m0.2072   \u001b[0m | \u001b[0m23.24    \u001b[0m | \u001b[0m203.4    \u001b[0m | \u001b[0m0.09186  \u001b[0m | \u001b[0m0.06419  \u001b[0m |\n",
      "| \u001b[0m84       \u001b[0m | \u001b[0m-0.4741  \u001b[0m | \u001b[0m187.6    \u001b[0m | \u001b[0m0.747    \u001b[0m | \u001b[0m36.29    \u001b[0m | \u001b[0m224.1    \u001b[0m | \u001b[0m0.08794  \u001b[0m | \u001b[0m0.02069  \u001b[0m |\n",
      "| \u001b[0m85       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m233.5    \u001b[0m | \u001b[0m0.6137   \u001b[0m | \u001b[0m220.4    \u001b[0m | \u001b[0m37.51    \u001b[0m | \u001b[0m0.02629  \u001b[0m | \u001b[0m0.06406  \u001b[0m |\n",
      "| \u001b[0m86       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m165.1    \u001b[0m | \u001b[0m0.7666   \u001b[0m | \u001b[0m184.0    \u001b[0m | \u001b[0m6.577    \u001b[0m | \u001b[0m0.01326  \u001b[0m | \u001b[0m0.08467  \u001b[0m |\n",
      "| \u001b[0m87       \u001b[0m | \u001b[0m-0.4755  \u001b[0m | \u001b[0m92.5     \u001b[0m | \u001b[0m0.7266   \u001b[0m | \u001b[0m94.35    \u001b[0m | \u001b[0m226.9    \u001b[0m | \u001b[0m0.01764  \u001b[0m | \u001b[0m0.02576  \u001b[0m |\n",
      "| \u001b[0m88       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m22.27    \u001b[0m | \u001b[0m0.2238   \u001b[0m | \u001b[0m249.8    \u001b[0m | \u001b[0m156.5    \u001b[0m | \u001b[0m0.06344  \u001b[0m | \u001b[0m0.06176  \u001b[0m |\n",
      "| \u001b[95m89       \u001b[0m | \u001b[95m-0.4651  \u001b[0m | \u001b[95m44.86    \u001b[0m | \u001b[95m0.7109   \u001b[0m | \u001b[95m14.9     \u001b[0m | \u001b[95m240.1    \u001b[0m | \u001b[95m0.0608   \u001b[0m | \u001b[95m0.008333 \u001b[0m |\n",
      "| \u001b[0m90       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m6.757    \u001b[0m | \u001b[0m0.2441   \u001b[0m | \u001b[0m93.11    \u001b[0m | \u001b[0m23.42    \u001b[0m | \u001b[0m0.0175   \u001b[0m | \u001b[0m0.05404  \u001b[0m |\n",
      "| \u001b[0m91       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m147.7    \u001b[0m | \u001b[0m0.4609   \u001b[0m | \u001b[0m178.1    \u001b[0m | \u001b[0m48.1     \u001b[0m | \u001b[0m0.001604 \u001b[0m | \u001b[0m0.05749  \u001b[0m |\n",
      "| \u001b[0m92       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m91.68    \u001b[0m | \u001b[0m0.1545   \u001b[0m | \u001b[0m94.52    \u001b[0m | \u001b[0m158.8    \u001b[0m | \u001b[0m0.07516  \u001b[0m | \u001b[0m0.07482  \u001b[0m |\n",
      "| \u001b[0m93       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m171.2    \u001b[0m | \u001b[0m0.7186   \u001b[0m | \u001b[0m79.46    \u001b[0m | \u001b[0m68.59    \u001b[0m | \u001b[0m0.03208  \u001b[0m | \u001b[0m0.09961  \u001b[0m |\n",
      "| \u001b[0m94       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m116.7    \u001b[0m | \u001b[0m0.1346   \u001b[0m | \u001b[0m82.78    \u001b[0m | \u001b[0m208.6    \u001b[0m | \u001b[0m0.08944  \u001b[0m | \u001b[0m0.0915   \u001b[0m |\n",
      "| \u001b[0m95       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m15.35    \u001b[0m | \u001b[0m0.7197   \u001b[0m | \u001b[0m8.611    \u001b[0m | \u001b[0m251.4    \u001b[0m | \u001b[0m0.06738  \u001b[0m | \u001b[0m0.05568  \u001b[0m |\n",
      "| \u001b[0m96       \u001b[0m | \u001b[0m-0.485   \u001b[0m | \u001b[0m131.7    \u001b[0m | \u001b[0m0.2161   \u001b[0m | \u001b[0m249.8    \u001b[0m | \u001b[0m54.31    \u001b[0m | \u001b[0m0.04087  \u001b[0m | \u001b[0m0.02983  \u001b[0m |\n",
      "| \u001b[0m97       \u001b[0m | \u001b[0m-0.5842  \u001b[0m | \u001b[0m176.6    \u001b[0m | \u001b[0m0.431    \u001b[0m | \u001b[0m95.82    \u001b[0m | \u001b[0m42.41    \u001b[0m | \u001b[0m0.03715  \u001b[0m | \u001b[0m0.005835 \u001b[0m |\n",
      "| \u001b[0m98       \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m151.1    \u001b[0m | \u001b[0m0.6961   \u001b[0m | \u001b[0m124.1    \u001b[0m | \u001b[0m16.53    \u001b[0m | \u001b[0m0.01626  \u001b[0m | \u001b[0m0.04598  \u001b[0m |\n",
      "| \u001b[0m99       \u001b[0m | \u001b[0m-0.6839  \u001b[0m | \u001b[0m24.69    \u001b[0m | \u001b[0m0.6804   \u001b[0m | \u001b[0m191.3    \u001b[0m | \u001b[0m100.6    \u001b[0m | \u001b[0m0.0002476\u001b[0m | \u001b[0m0.03815  \u001b[0m |\n",
      "| \u001b[0m100      \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m213.5    \u001b[0m | \u001b[0m0.1178   \u001b[0m | \u001b[0m86.91    \u001b[0m | \u001b[0m209.7    \u001b[0m | \u001b[0m0.03224  \u001b[0m | \u001b[0m0.08948  \u001b[0m |\n",
      "| \u001b[0m101      \u001b[0m | \u001b[0m-0.5481  \u001b[0m | \u001b[0m89.7     \u001b[0m | \u001b[0m0.4712   \u001b[0m | \u001b[0m51.76    \u001b[0m | \u001b[0m150.6    \u001b[0m | \u001b[0m0.02     \u001b[0m | \u001b[0m0.0112   \u001b[0m |\n",
      "| \u001b[0m102      \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m230.1    \u001b[0m | \u001b[0m0.2303   \u001b[0m | \u001b[0m110.6    \u001b[0m | \u001b[0m79.68    \u001b[0m | \u001b[0m0.03841  \u001b[0m | \u001b[0m0.06084  \u001b[0m |\n",
      "| \u001b[0m103      \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m117.9    \u001b[0m | \u001b[0m0.8839   \u001b[0m | \u001b[0m63.6     \u001b[0m | \u001b[0m103.4    \u001b[0m | \u001b[0m0.07215  \u001b[0m | \u001b[0m0.04774  \u001b[0m |\n",
      "| \u001b[0m104      \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m113.6    \u001b[0m | \u001b[0m0.3838   \u001b[0m | \u001b[0m254.8    \u001b[0m | \u001b[0m229.1    \u001b[0m | \u001b[0m0.04249  \u001b[0m | \u001b[0m0.05056  \u001b[0m |\n",
      "| \u001b[0m105      \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m17.26    \u001b[0m | \u001b[0m0.1899   \u001b[0m | \u001b[0m253.5    \u001b[0m | \u001b[0m57.64    \u001b[0m | \u001b[0m0.02949  \u001b[0m | \u001b[0m0.09099  \u001b[0m |\n",
      "| \u001b[0m106      \u001b[0m | \u001b[0m-0.4835  \u001b[0m | \u001b[0m46.44    \u001b[0m | \u001b[0m0.653    \u001b[0m | \u001b[0m13.45    \u001b[0m | \u001b[0m175.8    \u001b[0m | \u001b[0m0.04304  \u001b[0m | \u001b[0m0.003767 \u001b[0m |\n",
      "| \u001b[0m107      \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m139.2    \u001b[0m | \u001b[0m0.1809   \u001b[0m | \u001b[0m60.13    \u001b[0m | \u001b[0m111.9    \u001b[0m | \u001b[0m0.02303  \u001b[0m | \u001b[0m0.05109  \u001b[0m |\n",
      "| \u001b[0m108      \u001b[0m | \u001b[0m-0.51    \u001b[0m | \u001b[0m5.225    \u001b[0m | \u001b[0m0.5955   \u001b[0m | \u001b[0m129.2    \u001b[0m | \u001b[0m35.68    \u001b[0m | \u001b[0m0.03511  \u001b[0m | \u001b[0m0.002939 \u001b[0m |\n",
      "| \u001b[0m109      \u001b[0m | \u001b[0m-0.6934  \u001b[0m | \u001b[0m252.4    \u001b[0m | \u001b[0m0.8145   \u001b[0m | \u001b[0m18.77    \u001b[0m | \u001b[0m242.8    \u001b[0m | \u001b[0m0.06999  \u001b[0m | \u001b[0m0.07765  \u001b[0m |\n",
      "| \u001b[0m110      \u001b[0m | \u001b[0m-0.4861  \u001b[0m | \u001b[0m128.2    \u001b[0m | \u001b[0m0.7344   \u001b[0m | \u001b[0m239.6    \u001b[0m | \u001b[0m219.5    \u001b[0m | \u001b[0m0.0007628\u001b[0m | \u001b[0m0.0052   \u001b[0m |\n",
      "=================================================================================================\n",
      "Final log loss of best model: 0.46787541625695744\n",
      "Final accuracy of best model: 0.7627118644067796\n"
     ]
    }
   ],
   "source": [
    "# run bayes opt nn (womens)\n",
    "bayes_opt_nn(df=X_womens, init_points=10, n_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This accuracy is still worse than our results using the compact data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
